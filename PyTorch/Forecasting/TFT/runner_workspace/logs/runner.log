+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ [[ onnx == \t\s\-\t\r\a\c\e ]]
+ [[ onnx == \t\s\-\s\c\r\i\p\t ]]
+ export FORMAT_SUFFIX=onnx
+ FORMAT_SUFFIX=onnx
+ python3 triton/export_model.py --input-path triton/model.py --input-type pyt --output-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx --output-type onnx --ignore-unknown-parameters --onnx-opset 13 --checkpoint /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/electricity_bin/ --precision fp32 --dataloader triton/dataloader.py --dataset datasets/electricity_bin --batch-size 1
Got additional args []
2023-03-04 04:00:32,777 INFO export_model args:
2023-03-04 04:00:32,777 INFO export_model     input_path = triton/model.py
2023-03-04 04:00:32,777 INFO export_model     input_type = pyt
2023-03-04 04:00:32,777 INFO export_model     output_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx
2023-03-04 04:00:32,777 INFO export_model     output_type = onnx
2023-03-04 04:00:32,777 INFO export_model     dataloader = triton/dataloader.py
2023-03-04 04:00:32,777 INFO export_model     verbose = False
2023-03-04 04:00:32,777 INFO export_model     ignore_unknown_parameters = True
2023-03-04 04:00:32,777 INFO export_model     precision = fp32
2023-03-04 04:00:32,777 INFO export_model     onnx_opset = 13
2023-03-04 04:00:32,777 INFO export_model     dataset = datasets/electricity_bin
2023-03-04 04:00:32,777 INFO export_model     checkpoint = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/electricity_bin/
2023-03-04 04:00:32,777 INFO export_model     batch_size = 1
2023-03-04 04:00:32,778 INFO triton.deployment_toolkit.args Initializing get_dataloader_fn({'dataset': 'datasets/electricity_bin', 'checkpoint': '/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/electricity_bin/', 'batch_size': '1'})
2023-03-04 04:00:34,252 INFO triton.deployment_toolkit.args Initializing PyTorchModelLoader({'checkpoint': '/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/electricity_bin/', 'precision': 'fp32'})
2023-03-04 04:00:35,609 INFO export_model inputs: {'s_cat__0': TensorSpec(name='s_cat__0', shape=(-1, 192, 1), dtype=dtype('int64')), 's_cont__1': TensorSpec(name='s_cont__1', shape=(-1, 1), dtype=dtype('float32')), 'k_cat__2': TensorSpec(name='k_cat__2', shape=(-1, 1), dtype=dtype('float32')), 'k_cont__3': TensorSpec(name='k_cont__3', shape=(-1, 192, 3), dtype=dtype('float32')), 'o_cat__4': TensorSpec(name='o_cat__4', shape=(-1, 1), dtype=dtype('float32')), 'o_cont__5': TensorSpec(name='o_cont__5', shape=(-1, 1), dtype=dtype('float32')), 'target__6': TensorSpec(name='target__6', shape=(-1, 192, 1), dtype=dtype('float32')), 'id__7': TensorSpec(name='id__7', shape=(-1, 192, 1), dtype=dtype('int64'))}
2023-03-04 04:00:35,610 INFO export_model outputs: {'target__0': TensorSpec(name='target__0', shape=(-1, 24, 3), dtype=dtype('float32'))}
2023-03-04 04:00:35,610 INFO triton.deployment_toolkit.args Initializing PYT2ONNXSaver({'onnx_opset': 13})
triton/model.py:33: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['s_cat'] = s_cat if s_cat.shape[1] != 1 else None
triton/model.py:34: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['s_cont'] = s_cont if s_cont.shape[1] != 1 else None
triton/model.py:35: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['k_cat'] = k_cat if k_cat.shape[1] != 1 else None
triton/model.py:36: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['k_cont'] = k_cont if k_cont.shape[1] != 1 else None
triton/model.py:37: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['o_cat'] = o_cat if o_cat.shape[1] != 1 else None
triton/model.py:38: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['o_cont'] = o_cont if o_cont.shape[1] != 1 else None
/opt/conda/lib/python3.8/site-packages/torch/onnx/symbolic_opset9.py:2161: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. 
  warnings.warn("Exporting a model to ONNX with a batch_size other than 1, " +
WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ [[ onnx == \t\s\-\t\r\a\c\e ]]
+ [[ onnx == \t\s\-\s\c\r\i\p\t ]]
+ export FORMAT_SUFFIX=onnx
+ FORMAT_SUFFIX=onnx
+ model-navigator convert --model-name TFT --model-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx --output-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model --target-formats trt --target-precisions fp16 --launch-mode local --override-workspace --verbose --onnx-opsets 13 --max-batch-size 1024 --container-version 21.08 --max-workspace-size 10000000000 --atol target__0=100 --rtol target__0=100
2023-03-04 04:00:39 - DEBUG - model_navigator.log: initialized logger verbose=True
2023-03-04 04:00:39 - DEBUG - convert: Running 'model-navigator convert' with config_path: None
2023-03-04 04:00:39 - INFO - model_navigator.log: Environment state
2023-03-04 04:00:39 - INFO - model_navigator.log: 	docker_container_id = 98c4b3fc07
2023-03-04 04:00:39 - INFO - model_navigator.log: 	docker_container_ip_address = None
2023-03-04 04:00:39 - INFO - model_navigator.log: 	docker_container_mounts = [{'host_path': PosixPath('/var/run/docker.sock'), 'container_path': PosixPath('/var/run/docker.sock'), 'mount_type': 'bind'}, {'host_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/dataset/processed'), 'container_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/datasets'), 'mount_type': 'bind'}, {'host_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT'), 'container_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT'), 'mount_type': 'bind'}]
2023-03-04 04:00:39 - DEBUG - model_navigator.validators: Running command validator: <model_navigator.validators.ModelNavigatorDockerContainerShouldHaveMountedWorkspaceDir object at 0x7f814c5fd190>
2023-03-04 04:00:39 - DEBUG - model_navigator.validators: ModelNavigatorDockerContainerShouldHaveMountedWorkspaceDir is_running_in_container=True has_mounted_workspace_path=True is_running_converter_in_docker=False
2023-03-04 04:00:39 - DEBUG - model_navigator.utils.workspace: Workspace path /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:00:39 - INFO - model_navigator.log: convert args:
2023-03-04 04:00:39 - INFO - model_navigator.log: 	model_name = TFT
2023-03-04 04:00:39 - INFO - model_navigator.log: 	model_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx
2023-03-04 04:00:39 - INFO - model_navigator.log: 	model_format = None
2023-03-04 04:00:39 - INFO - model_navigator.log: 	model_version = 1
2023-03-04 04:00:39 - INFO - model_navigator.log: 	target_formats = [<Format.TENSORRT: 'trt'>]
2023-03-04 04:00:39 - INFO - model_navigator.log: 	onnx_opsets = [13]
2023-03-04 04:00:39 - INFO - model_navigator.log: 	tensorrt_precisions = [<TensorRTPrecision.FP16: 'fp16'>]
2023-03-04 04:00:39 - INFO - model_navigator.log: 	tensorrt_precisions_mode = TensorRTPrecisionMode.HIERARCHY
2023-03-04 04:00:39 - INFO - model_navigator.log: 	tensorrt_explicit_precision = False
2023-03-04 04:00:39 - INFO - model_navigator.log: 	tensorrt_strict_types = False
2023-03-04 04:00:39 - INFO - model_navigator.log: 	tensorrt_sparse_weights = False
2023-03-04 04:00:39 - INFO - model_navigator.log: 	tensorrt_max_workspace_size = 10000000000
2023-03-04 04:00:39 - INFO - model_navigator.log: 	atol = {'target__0': 100.0}
2023-03-04 04:00:39 - INFO - model_navigator.log: 	rtol = {'target__0': 100.0}
2023-03-04 04:00:39 - INFO - model_navigator.log: 	max_batch_size = 1024
2023-03-04 04:00:39 - INFO - model_navigator.log: 	inputs = None
2023-03-04 04:00:39 - INFO - model_navigator.log: 	outputs = None
2023-03-04 04:00:39 - INFO - model_navigator.log: 	min_shapes = None
2023-03-04 04:00:39 - INFO - model_navigator.log: 	opt_shapes = None
2023-03-04 04:00:39 - INFO - model_navigator.log: 	max_shapes = None
2023-03-04 04:00:39 - INFO - model_navigator.log: 	value_ranges = None
2023-03-04 04:00:39 - INFO - model_navigator.log: 	dtypes = None
2023-03-04 04:00:39 - INFO - model_navigator.log: 	workspace_path = navigator_workspace
2023-03-04 04:00:39 - INFO - model_navigator.log: 	override_workspace = True
2023-03-04 04:00:39 - INFO - model_navigator.log: 	output_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model
2023-03-04 04:00:39 - INFO - model_navigator.log: 	container_version = 21.08
2023-03-04 04:00:39 - INFO - model_navigator.log: 	framework_docker_image = nvcr.io/nvidia/pytorch:21.08-py3
2023-03-04 04:00:39 - INFO - model_navigator.log: 	gpus = ('all',)
2023-03-04 04:00:39 - DEBUG - model_navigator.utils.workspace: Cleaning workspace dir /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:00:39 - DEBUG - model_navigator.converter.convert: Converter created; workspace=/home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:00:39 - DEBUG - model_navigator.converter.convert: Convert Commands Executor created; workspace=/home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:00:39 - DEBUG - model_navigator.converter.transformers: Running command polygraphyonnx2trt_fp16_mh on /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx
2023-03-04 04:00:39 - INFO - polygraphy.transformers: Polygraphy onnx2trt started.
2023-03-04 04:00:39 - WARNING - polygraphy.transformers: This conversion should be done on target GPU platform
2023-03-04 04:00:39 - DEBUG - polygraphy.transformers: Use min_shapes: {'s_cat__0': (1, 192, 1), 'k_cont__3': (1, 192, 3), 'target__6': (1, 192, 1)} as runtime shape
================================
/opt/conda/bin/polygraphy run --onnxrt --trt /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx --model-type onnx --inputs s_cat__0:[1,192,1] k_cont__3:[1,192,3] target__6:[1,192,1] --onnx-outputs target__0 --shape-inference --tf32 --fp16 --trt-min-shapes s_cat__0:[1,192,1] k_cont__3:[1,192,3] target__6:[1,192,1] --trt-opt-shapes s_cat__0:[1024,192,1] k_cont__3:[1024,192,3] target__6:[1024,192,1] --trt-max-shapes s_cat__0:[1024,192,1] k_cont__3:[1024,192,3] target__6:[1024,192,1] --save-inputs /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_inputs.json --save-outputs /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_outputs.json --validate --rtol target__0:100.0 1e-05 --atol target__0:100.0 1e-05 --workspace 10000000000 --save-engine /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan -v
================================


2023-03-04 04:00:39 - DEBUG - asyncio: Using selector: EpollSelector
	[38;5;13m[V] Model: /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx[0m
	[38;5;11m[W] --workspace is deprecated and will be removed in Polygraphy 0.45.0. Use --pool-limit workspace:10000000000 instead.[0m
	[38;5;14m[I] RUNNING | Command: /opt/conda/bin/polygraphy run --onnxrt --trt /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx --model-type onnx --inputs s_cat__0:[1,192,1] k_cont__3:[1,192,3] target__6:[1,192,1] --onnx-outputs target__0 --shape-inference --tf32 --fp16 --trt-min-shapes s_cat__0:[1,192,1] k_cont__3:[1,192,3] target__6:[1,192,1] --trt-opt-shapes s_cat__0:[1024,192,1] k_cont__3:[1024,192,3] target__6:[1024,192,1] --trt-max-shapes s_cat__0:[1024,192,1] k_cont__3:[1024,192,3] target__6:[1024,192,1] --save-inputs /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_inputs.json --save-outputs /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_outputs.json --validate --rtol target__0:100.0 1e-05 --atol target__0:100.0 1e-05 --workspace 10000000000 --save-engine /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan -v[0m
	[38;5;13m[V] Loaded Module: polygraphy | Version: 0.44.2 | Path: ['/opt/conda/lib/python3.8/site-packages/polygraphy'][0m
	[38;5;13m[V] Loaded extension modules: [][0m
	[38;5;13m[V] Will save runner results to: /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_outputs.json[0m
	[38;5;13m[V] Loaded Module: tensorrt | Version: 8.2.1.8 | Path: ['/opt/conda/lib/python3.8/site-packages/tensorrt'][0m
	[I] Will generate inference input data according to provided TensorMetadata: {s_cat__0 [shape=(1, 192, 1)],
	     k_cont__3 [shape=(1, 192, 3)],
	     target__6 [shape=(1, 192, 1)]}
	[38;5;14m[I] onnxrt-runner-N0-03/04/23-04:00:39  | Activating and starting inference[0m
	[38;5;13m[V] Starting shape inference[0m
	[38;5;11m[W] Falling back to `onnx.shape_inference` because `onnxruntime.tools.symbolic_shape_infer` could not be loaded.
	    Note that using ONNX-Runtime for shape inference may be faster and require less memory.
	    Consider installing ONNX-Runtime or settting POLYGRAPHY_AUTOINSTALL_DEPS=1 in your environment variables to allow Polygraphy to do so automatically.[0m
	[38;5;13m[V] Loaded Module: onnx | Version: 1.8.1 | Path: ['/opt/conda/lib/python3.8/site-packages/onnx'][0m
	[38;5;13m[V] Writing shape-inferred model to: /tmp/tmp_polygraphy_105bf1617b21cad9fe28c63b4ae5df2ded7f37d9e0e3f85c.onnx[0m
	[38;5;13m[V] Loaded Module: onnx.shape_inference[0m
	[I] Loading model: /tmp/tmp_polygraphy_105bf1617b21cad9fe28c63b4ae5df2ded7f37d9e0e3f85c.onnx
	[38;5;13m[V] Loading external data from: /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared[0m
	[38;5;13m[V] Loaded Module: onnx.external_data_helper[0m
	[38;5;13m[V] Shape inference completed successfully[0m
	[38;5;13m[V] Loaded Module: onnxruntime | Version: 1.8.1 | Path: ['/opt/conda/lib/python3.8/site-packages/onnxruntime'][0m
	[38;5;14m[I] Creating ONNX-Runtime Inference Session with providers: ['CPUExecutionProvider'][0m
	[38;5;13m[V] Loaded Module: numpy | Version: 1.21.4 | Path: ['/opt/conda/lib/python3.8/site-packages/numpy'][0m
	[38;5;13m[V] Loading inputs from data loader[0m
	[38;5;13m[V] Generating data using numpy seed: 1[0m
	[38;5;13m[V] Input tensor: s_cat__0 | Generating input data in range: [0, 1][0m
	[38;5;13m[V] Input tensor: k_cont__3 | Generating input data in range: [0.0, 1.0][0m
	[38;5;13m[V] Input tensor: target__6 | Generating input data in range: [0.0, 1.0][0m
	[I] Saving inference input data to /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_inputs.json
	[I] onnxrt-runner-N0-03/04/23-04:00:39
	    ---- Inference Input(s) ----
	    {s_cat__0 [dtype=int64, shape=(1, 192, 1)],
	     k_cont__3 [dtype=float32, shape=(1, 192, 3)],
	     target__6 [dtype=float32, shape=(1, 192, 1)]}
	[38;5;13m[V] Runner input metadata is: {s_cat__0 [dtype=int64, shape=('batch_size_0', 192, 1)],
	     k_cont__3 [dtype=float32, shape=('batch_size_0', 192, 3)],
	     target__6 [dtype=float32, shape=('batch_size_0', 192, 1)]}[0m
	[I] onnxrt-runner-N0-03/04/23-04:00:39
	    ---- Inference Output(s) ----
	    {target__0 [dtype=float32, shape=(1, 24, 3)]}
	[38;5;10m[I] onnxrt-runner-N0-03/04/23-04:00:39  | Completed 1 iteration(s) in 7.754 ms | Average inference time: 7.754 ms.[0m
	[38;5;14m[I] trt-runner-N0-03/04/23-04:00:39     | Activating and starting inference[0m
	[38;5;13m[V] [MemUsageChange] Init CUDA: CPU +721, GPU +0, now: CPU 745, GPU 1170 (MiB)[0m
	[38;5;13m[V] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 745 MiB, GPU 1170 MiB[0m
	[38;5;13m[V] [MemUsageSnapshot] End constructing builder kernel library: CPU 1013 MiB, GPU 1242 MiB[0m
	[38;5;13m[V] Starting shape inference[0m
	[38;5;13m[V] Writing shape-inferred model to: /tmp/tmp_polygraphy_e19a68a1a7ca98bdb6b06f7a953f240e72d88268acc46886.onnx[0m
	[I] Loading model: /tmp/tmp_polygraphy_e19a68a1a7ca98bdb6b06f7a953f240e72d88268acc46886.onnx
	[38;5;13m[V] Loading external data from: /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared[0m
	[38;5;13m[V] Shape inference completed successfully[0m
	[38;5;11m[W] parsers/onnx/onnx2trt_utils.cpp:364: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;11m[W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;11m[W] parsers/onnx/ShapedWeights.cpp:171: Weights 1007 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;11m[W] parsers/onnx/ShapedWeights.cpp:171: Weights 1045 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;11m[W] parsers/onnx/ShapedWeights.cpp:171: Weights 1080 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_529: broadcasting input1 to make tensors conform, dims(input0)=[-1,4,192,192][NONE] dims(input1)=[-1,1,192,32][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_529: broadcasting input1 to make tensors conform, dims(input0)=[-1,4,192,192][NONE] dims(input1)=[-1,1,192,32][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_529: broadcasting input1 to make tensors conform, dims(input0)=[-1,4,192,192][NONE] dims(input1)=[-1,1,192,32][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_529: broadcasting input1 to make tensors conform, dims(input0)=[-1,4,192,192][NONE] dims(input1)=[-1,1,192,32][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_529: broadcasting input1 to make tensors conform, dims(input0)=[-1,4,192,192][NONE] dims(input1)=[-1,1,192,32][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_529: broadcasting input1 to make tensors conform, dims(input0)=[-1,4,192,192][NONE] dims(input1)=[-1,1,192,32][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_529: broadcasting input1 to make tensors conform, dims(input0)=[-1,4,192,192][NONE] dims(input1)=[-1,1,192,32][NONE].[0m
	[38;5;11m[W] Output type must be INT32 for shape outputs[0m
	[38;5;13m[V]     Setting TensorRT Optimization Profiles[0m
	[38;5;13m[V]     Input tensor: s_cat__0 (dtype=DataType.INT32, shape=(-1, 192, 1)) | Setting input tensor shapes to: (min=[1, 192, 1], opt=[1024, 192, 1], max=[1024, 192, 1])[0m
	[38;5;13m[V]     Input tensor: k_cont__3 (dtype=DataType.FLOAT, shape=(-1, 192, 3)) | Setting input tensor shapes to: (min=[1, 192, 3], opt=[1024, 192, 3], max=[1024, 192, 3])[0m
	[38;5;13m[V]     Input tensor: target__6 (dtype=DataType.FLOAT, shape=(-1, 192, 1)) | Setting input tensor shapes to: (min=[1, 192, 1], opt=[1024, 192, 1], max=[1024, 192, 1])[0m
	[I]     Configuring with profiles: [Profile().add('s_cat__0', min=[1, 192, 1], opt=[1024, 192, 1], max=[1024, 192, 1]).add('k_cont__3', min=[1, 192, 3], opt=[1024, 192, 3], max=[1024, 192, 3]).add('target__6', min=[1, 192, 1], opt=[1024, 192, 1], max=[1024, 192, 1])]
	[38;5;14m[I] Building engine with configuration:
	    Flags                  | [FP16, TF32]
	    Engine Capability      | EngineCapability.DEFAULT
	    Tactic Sources         | [CUBLAS, CUBLAS_LT, CUDNN]
	    DLA                    | Default Device Type: DeviceType.GPU, Core: 0
	    Profiling Verbosity    | ProfilingVerbosity.DETAILED[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_529: broadcasting input1 to make tensors conform, dims(input0)=[-1,4,192,192][NONE] dims(input1)=[-1,1,192,32][NONE].[0m
	[38;5;13m[V] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1240, GPU +352, now: CPU 2264, GPU 1594 (MiB)[0m
	[38;5;13m[V] [MemUsageChange] Init cuDNN: CPU +252, GPU +58, now: CPU 2516, GPU 1652 (MiB)[0m
	[38;5;13m[V] Global timing cache in use. Profiling results in this builder pass will be stored.[0m
	[38;5;11m[W] Weights [name=model.attention._mask + (Unnamed Layer* 1358) [Shuffle]]: Converted FP32 value in weights (either FP32 infinity or FP32 value outside FP16 range) to corresponding FP16 infinity. If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.[0m
	[38;5;13m[V] Detected 3 inputs and 1 output network tensors.[0m
	[38;5;13m[V] Total Host Persistent Memory: 304[0m
	[38;5;13m[V] Total Device Persistent Memory: 0[0m
	[38;5;13m[V] Total Scratch Memory: 1248067584[0m
	[38;5;13m[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 3 MiB, GPU 2401 MiB[0m
	[38;5;13m[V] [BlockAssignment] Algorithm ShiftNTopDown took 0.013374ms to assign 5 blocks to 5 nodes requiring 1249788416 bytes.[0m
	[38;5;13m[V] Total Activation Memory: 1249788416[0m
	[38;5;13m[V] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2540, GPU 1696 (MiB)[0m
	[38;5;13m[V] [MemUsageChange] Init cuDNN: CPU +1, GPU +8, now: CPU 2541, GPU 1704 (MiB)[0m
	[38;5;13m[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +7, now: CPU 0, GPU 7 (MiB)[0m
	[38;5;10m[I] Finished engine building in 821.608 seconds[0m
	[38;5;13m[V] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2545, GPU 1662 (MiB)[0m
	[38;5;13m[V] Loaded engine size: 28 MiB[0m
	[38;5;13m[V] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2567, GPU 1680 (MiB)[0m
	[38;5;13m[V] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2567, GPU 1688 (MiB)[0m
	[38;5;13m[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +6, now: CPU 0, GPU 6 (MiB)[0m
	[I] Saving engine to /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan
	[38;5;13m[V] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2539, GPU 1680 (MiB)[0m
	[38;5;13m[V] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2539, GPU 1688 (MiB)[0m
	[38;5;13m[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +1192, now: CPU 0, GPU 1198 (MiB)[0m
	[38;5;13m[V] Found candidate CUDA libraries: ['/usr/local/cuda/lib64/libcudart.so.11.5.50', '/usr/local/cuda/lib64/libcudart.so', '/usr/local/cuda/lib64/libcudart.so.11.0'][0m
	[38;5;11m[W] Input tensor: s_cat__0 | Buffer dtype (int64) does not match expected input dtype (int32), attempting to cast. [0m
	[I] trt-runner-N0-03/04/23-04:00:39
	    ---- Inference Input(s) ----
	    {s_cat__0 [dtype=int32, shape=(1, 192, 1)],
	     k_cont__3 [dtype=float32, shape=(1, 192, 3)],
	     target__6 [dtype=float32, shape=(1, 192, 1)]}
	[38;5;13m[V] Runner input metadata is: {s_cat__0 [dtype=int32, shape=(-1, 192, 1)],
	     k_cont__3 [dtype=float32, shape=(-1, 192, 3)],
	     target__6 [dtype=float32, shape=(-1, 192, 1)]}[0m
	[38;5;13m[V] Setting binding: s_cat__0 (index: 0) to shape: (1, 192, 1)[0m
	[38;5;13m[V] Setting binding: k_cont__3 (index: 1) to shape: (1, 192, 3)[0m
	[38;5;13m[V] Setting binding: target__6 (index: 2) to shape: (1, 192, 1)[0m
	[I] trt-runner-N0-03/04/23-04:00:39
	    ---- Inference Output(s) ----
	    {target__0 [dtype=float32, shape=(1, 24, 3)]}
	[38;5;10m[I] trt-runner-N0-03/04/23-04:00:39     | Completed 1 iteration(s) in 3.428 ms | Average inference time: 3.428 ms.[0m
	[38;5;13m[V] Successfully ran: ['onnxrt-runner-N0-03/04/23-04:00:39', 'trt-runner-N0-03/04/23-04:00:39'][0m
	[I] Saving inference results to /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_outputs.json
	[38;5;14m[I] Accuracy Comparison | onnxrt-runner-N0-03/04/23-04:00:39 vs. trt-runner-N0-03/04/23-04:00:39[0m
	[38;5;14m[I]     Comparing Output: 'target__0' (dtype=float32, shape=(1, 24, 3)) with 'target__0' (dtype=float32, shape=(1, 24, 3))[0m
	[I]         Tolerance: [abs=100, rel=100] | Checking elemwise error
	[I]         onnxrt-runner-N0-03/04/23-04:00:39: target__0 | Stats: mean=0.17072, std-dev=0.34407, var=0.11839, median=0.19297, min=-0.44968 at (0, 23, 0), max=0.83435 at (0, 0, 2), avg-magnitude=0.33957
	[38;5;13m[V]             ---- Histogram ----
	                Bin Range          |  Num Elems | Visualization
	                (-0.451 , -0.323 ) |          7 | #############
	                (-0.323 , -0.194 ) |         11 | ####################
	                (-0.194 , -0.0657) |          3 | #####
	                (-0.0657, 0.0629 ) |          2 | ###
	                (0.0629 , 0.191  ) |         11 | ####################
	                (0.191  , 0.32   ) |         11 | ####################
	                (0.32   , 0.449  ) |          2 | ###
	                (0.449  , 0.577  ) |         21 | ########################################
	                (0.577  , 0.706  ) |          2 | ###
	                (0.706  , 0.834  ) |          2 | ###[0m
	[I]         trt-runner-N0-03/04/23-04:00:39: target__0 | Stats: mean=0.17056, std-dev=0.3446, var=0.11875, median=0.19366, min=-0.45142 at (0, 23, 0), max=0.8335 at (0, 0, 2), avg-magnitude=0.33997
	[38;5;13m[V]             ---- Histogram ----
	                Bin Range          |  Num Elems | Visualization
	                (-0.451 , -0.323 ) |          7 | ##############
	                (-0.323 , -0.194 ) |         11 | ######################
	                (-0.194 , -0.0657) |          3 | ######
	                (-0.0657, 0.0629 ) |          2 | ####
	                (0.0629 , 0.191  ) |         11 | ######################
	                (0.191  , 0.32   ) |         11 | ######################
	                (0.32   , 0.449  ) |          2 | ####
	                (0.449  , 0.577  ) |         20 | ########################################
	                (0.577  , 0.706  ) |          3 | ######
	                (0.706  , 0.834  ) |          2 | ####[0m
	[I]         Error Metrics: target__0
	[I]             Minimum Required Tolerance: elemwise error | [abs=0.002208] OR [rel=0.042907] (requirements may be lower if both abs/rel tolerances are set)
	[I]             Absolute Difference | Stats: mean=0.00072023, std-dev=0.00052738, var=2.7813e-07, median=0.00065389, min=3.3528e-06 at (0, 20, 1), max=0.002208 at (0, 0, 0), avg-magnitude=0.00072023
	[38;5;13m[V]                 ---- Histogram ----
	                    Bin Range            |  Num Elems | Visualization
	                    (3.35e-06, 0.000224) |         16 | ########################################
	                    (0.000224, 0.000444) |         10 | #########################
	                    (0.000444, 0.000665) |         10 | #########################
	                    (0.000665, 0.000885) |         13 | ################################
	                    (0.000885, 0.00111 ) |          8 | ####################
	                    (0.00111 , 0.00133 ) |          6 | ###############
	                    (0.00133 , 0.00155 ) |          3 | #######
	                    (0.00155 , 0.00177 ) |          2 | #####
	                    (0.00177 , 0.00199 ) |          1 | ##
	                    (0.00199 , 0.00221 ) |          3 | #######[0m
	[I]             Relative Difference | Stats: mean=0.0036972, std-dev=0.0066062, var=4.3642e-05, median=0.0018478, min=1.3963e-05 at (0, 20, 1), max=0.042907 at (0, 2, 0), avg-magnitude=0.0036972
	[38;5;13m[V]                 ---- Histogram ----
	                    Bin Range          |  Num Elems | Visualization
	                    (1.4e-05, 0.0043 ) |         60 | ########################################
	                    (0.0043 , 0.00859) |          7 | ####
	                    (0.00859, 0.0129 ) |          1 |
	                    (0.0129 , 0.0172 ) |          1 |
	                    (0.0172 , 0.0215 ) |          0 |
	                    (0.0215 , 0.0257 ) |          1 |
	                    (0.0257 , 0.03   ) |          1 |
	                    (0.03   , 0.0343 ) |          0 |
	                    (0.0343 , 0.0386 ) |          0 |
	                    (0.0386 , 0.0429 ) |          1 | [0m
	[38;5;10m[I]         PASSED | Output: 'target__0' | Difference is within tolerance (rel=100.0, abs=100.0)[0m
	[38;5;10m[I]     PASSED | All outputs matched | Outputs: ['target__0'][0m
	[38;5;10m[I] Accuracy Summary | onnxrt-runner-N0-03/04/23-04:00:39 vs. trt-runner-N0-03/04/23-04:00:39 | Passed: 1/1 iterations | Pass Rate: 100.0%[0m
	[38;5;14m[I] Output Validation | Runners: ['onnxrt-runner-N0-03/04/23-04:00:39', 'trt-runner-N0-03/04/23-04:00:39'][0m
	[38;5;14m[I]     onnxrt-runner-N0-03/04/23-04:00:39  | Validating output: target__0 (check_inf=True, check_nan=True)[0m
	[I]         mean=0.17072, std-dev=0.34407, var=0.11839, median=0.19297, min=-0.44968 at (0, 23, 0), max=0.83435 at (0, 0, 2), avg-magnitude=0.33957
	[38;5;13m[V]             ---- Histogram ----
	                Bin Range          |  Num Elems | Visualization
	                (-0.45  , -0.321 ) |          7 | #############
	                (-0.321 , -0.193 ) |         11 | ####################
	                (-0.193 , -0.0645) |          3 | #####
	                (-0.0645, 0.0639 ) |          2 | ###
	                (0.0639 , 0.192  ) |         13 | ########################
	                (0.192  , 0.321  ) |          9 | #################
	                (0.321  , 0.449  ) |          2 | ###
	                (0.449  , 0.578  ) |         21 | ########################################
	                (0.578  , 0.706  ) |          2 | ###
	                (0.706  , 0.834  ) |          2 | ###[0m
	[38;5;10m[I]         PASSED | Output: target__0 is valid[0m
	[38;5;14m[I]     trt-runner-N0-03/04/23-04:00:39     | Validating output: target__0 (check_inf=True, check_nan=True)[0m
	[I]         mean=0.17056, std-dev=0.3446, var=0.11875, median=0.19366, min=-0.45142 at (0, 23, 0), max=0.8335 at (0, 0, 2), avg-magnitude=0.33997
	[38;5;13m[V]             ---- Histogram ----
	                Bin Range          |  Num Elems | Visualization
	                (-0.451 , -0.323 ) |          7 | ##############
	                (-0.323 , -0.194 ) |         11 | #######################
	                (-0.194 , -0.0659) |          3 | ######
	                (-0.0659, 0.0625 ) |          2 | ####
	                (0.0625 , 0.191  ) |         11 | #######################
	                (0.191  , 0.32   ) |         11 | #######################
	                (0.32   , 0.448  ) |          2 | ####
	                (0.448  , 0.577  ) |         19 | ########################################
	                (0.577  , 0.705  ) |          4 | ########
	                (0.705  , 0.833  ) |          2 | ####[0m
	[38;5;10m[I]         PASSED | Output: target__0 is valid[0m
	[38;5;10m[I]     PASSED | Output Validation[0m
	[38;5;10m[I] PASSED | Runtime: 827.783s | Command: /opt/conda/bin/polygraphy run --onnxrt --trt /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx --model-type onnx --inputs s_cat__0:[1,192,1] k_cont__3:[1,192,3] target__6:[1,192,1] --onnx-outputs target__0 --shape-inference --tf32 --fp16 --trt-min-shapes s_cat__0:[1,192,1] k_cont__3:[1,192,3] target__6:[1,192,1] --trt-opt-shapes s_cat__0:[1024,192,1] k_cont__3:[1024,192,3] target__6:[1024,192,1] --trt-max-shapes s_cat__0:[1024,192,1] k_cont__3:[1024,192,3] target__6:[1024,192,1] --save-inputs /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_inputs.json --save-outputs /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_outputs.json --validate --rtol target__0:100.0 1e-05 --atol target__0:100.0 1e-05 --workspace 10000000000 --save-engine /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan -v[0m
2023-03-04 04:14:28 - INFO - polygraphy.transformers: Polygraphy onnx2trt succeed.
2023-03-04 04:14:28 - DEBUG - model_navigator.results: Saving results of convert_model stage into /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/convert_model_results.yaml
2023-03-04 04:14:28 - DEBUG - model_navigator.results: ConversionResult(status=Status(state=<State.SUCCEEDED: 'succeeded'>, message='ONNX converted to TensorRT', log_path='/home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.log'), source_model_config=ModelConfig(model_name='TFT', model_path=PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx'), model_format=None, model_version='1'), conversion_config=ConversionConfig(target_format=<Format.TENSORRT: 'trt'>, onnx_opset=13, tensorrt_precision=<TensorRTPrecision.FP16: 'fp16'>, tensorrt_precision_mode=<TensorRTPrecisionMode.HIERARCHY: 'hierarchy'>, tensorrt_explicit_precision=False, tensorrt_strict_types=False, tensorrt_sparse_weights=False), tensorrt_common_config=None, model_signature_config=None, comparator_config=ComparatorConfig(atol={'target__0': 100.0, '': 1e-05}, rtol={'target__0': 100.0, '': 1e-05}, max_batch_size=1024), dataset_profile=DatasetProfileConfig(min_shapes=None, opt_shapes=None, max_shapes=None, value_ranges=None, dtypes=None), output_model=Model(name='TFT.polygraphyonnx2trt_fp16_mh', path=PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan'), format=<Format.TENSORRT: 'trt'>, signature=None, properties=TensorRTProperties(), num_required_gpus=None), framework_docker_image=None)
2023-03-04 04:14:28 - DEBUG - convert: Copy /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan to /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model
2023-03-04 04:14:28 - DEBUG - convert: Copy /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_inputs.json to /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model.json
2023-03-04 04:14:28 - DEBUG - convert: Copy /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_outputs.json to /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model.json
2023-03-04 04:14:28 - DEBUG - convert: Copy /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.log to /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model.log
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ [[ trt == \t\s\-\t\r\a\c\e ]]
+ [[ trt == \t\s\-\s\c\r\i\p\t ]]
+ export CONFIG_FORMAT=trt
+ CONFIG_FORMAT=trt
+ model-navigator triton-config-model --model-repository /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models --model-name TFT --model-version 1 --model-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model --model-format trt --model-control-mode explicit --load-model --load-model-timeout-s 100 --verbose --backend-accelerator none --tensorrt-precision fp16 --tensorrt-capture-cuda-graph --tensorrt-max-workspace-size 10000000000 --max-batch-size 1024 --batching dynamic --preferred-batch-sizes 512 1024 --max-queue-delay-us 1 --engine-count-per-device gpu=2
2023-03-04 04:14:29 - DEBUG - model_navigator.log: initialized logger verbose=True
2023-03-04 04:14:29 - DEBUG - triton_config_model: Running 'model-navigator triton-config-model' with config_path: None
2023-03-04 04:14:29 - INFO - model_navigator.log: Environment state
2023-03-04 04:14:29 - INFO - model_navigator.log: 	docker_container_id = 98c4b3fc07
2023-03-04 04:14:29 - INFO - model_navigator.log: 	docker_container_ip_address = None
2023-03-04 04:14:29 - INFO - model_navigator.log: 	docker_container_mounts = [{'host_path': PosixPath('/var/run/docker.sock'), 'container_path': PosixPath('/var/run/docker.sock'), 'mount_type': 'bind'}, {'host_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/dataset/processed'), 'container_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/datasets'), 'mount_type': 'bind'}, {'host_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT'), 'container_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT'), 'mount_type': 'bind'}]
2023-03-04 04:14:29 - DEBUG - model_navigator.validators: Running command validator: <model_navigator.validators.ModelNavigatorBatchingConfiguration object at 0x7f5410b14a90>
2023-03-04 04:14:29 - DEBUG - model_navigator.validators: ModelNavigatorBatchingConfiguration 
batching=Batching.DYNAMIC 
max_batch_size=1024 
2023-03-04 04:14:29 - INFO - model_navigator.log: triton-config-model args:
2023-03-04 04:14:29 - INFO - model_navigator.log: 	model_name = TFT
2023-03-04 04:14:29 - INFO - model_navigator.log: 	model_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model
2023-03-04 04:14:29 - INFO - model_navigator.log: 	model_format = Format.TENSORRT
2023-03-04 04:14:29 - INFO - model_navigator.log: 	model_version = 1
2023-03-04 04:14:29 - INFO - model_navigator.log: 	model_repository = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models
2023-03-04 04:14:29 - INFO - model_navigator.log: 	load_model = True
2023-03-04 04:14:29 - INFO - model_navigator.log: 	load_model_timeout_s = 100
2023-03-04 04:14:29 - INFO - model_navigator.log: 	model_control_mode = ModelControlMode.EXPLICIT
2023-03-04 04:14:29 - INFO - model_navigator.log: 	inputs = None
2023-03-04 04:14:29 - INFO - model_navigator.log: 	outputs = None
2023-03-04 04:14:29 - INFO - model_navigator.log: 	max_batch_size = 1024
2023-03-04 04:14:29 - INFO - model_navigator.log: 	batching = Batching.DYNAMIC
2023-03-04 04:14:29 - INFO - model_navigator.log: 	backend_accelerator = BackendAccelerator.NONE
2023-03-04 04:14:29 - INFO - model_navigator.log: 	tensorrt_precision = TensorRTOptPrecision.FP16
2023-03-04 04:14:29 - INFO - model_navigator.log: 	tensorrt_capture_cuda_graph = True
2023-03-04 04:14:29 - INFO - model_navigator.log: 	tensorrt_max_workspace_size = 10000000000
2023-03-04 04:14:29 - INFO - model_navigator.log: 	preferred_batch_sizes = [512, 1024]
2023-03-04 04:14:29 - INFO - model_navigator.log: 	max_queue_delay_us = 1
2023-03-04 04:14:29 - INFO - model_navigator.log: 	engine_count_per_device = {<DeviceKind.GPU: 'gpu'>: 2}
2023-03-04 04:14:29 - INFO - model_navigator.log: 	triton_backend_parameters = {}
2023-03-04 04:14:29 - INFO - model_navigator.log: 	server_url = grpc://localhost:8001
2023-03-04 04:14:29 - INFO - model_navigator.log: model:
2023-03-04 04:14:29 - INFO - model_navigator.log: 	name = TFT
2023-03-04 04:14:29 - INFO - model_navigator.log: 	path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model
2023-03-04 04:14:29 - INFO - model_navigator.log: 	format = Format.TENSORRT
2023-03-04 04:14:29 - INFO - model_navigator.log: 	signature = None
2023-03-04 04:14:29 - INFO - model_navigator.log: 	properties = {}
2023-03-04 04:14:29 - INFO - model_navigator.log: 	num_required_gpus = None
2023-03-04 04:14:29 - DEBUG - model_navigator.triton.model_store: Deploying model /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model in Triton Model Store /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models with: 
batching config: TritonBatchingConfig(max_batch_size=1024, batching=<Batching.DYNAMIC: 'dynamic'>)
optimization config: TritonModelOptimizationConfig(backend_accelerator=<BackendAccelerator.NONE: 'none'>, tensorrt_precision=<TensorRTOptPrecision.FP16: 'fp16'>, tensorrt_capture_cuda_graph=True)
dynamic batching config: TritonDynamicBatchingConfig(preferred_batch_sizes=[512, 1024], max_queue_delay_us=1)
2023-03-04 04:14:29 - DEBUG - model_navigator.triton.model_store: Copying /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model to /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models/TFT/1/model.plan
2023-03-04 04:14:29 - DEBUG - model_navigator.triton.model_config: Generated Triton config:
name: "TFT"
platform: "tensorrt_plan"
max_batch_size: 1024
instance_group {
  count: 2
  kind: KIND_GPU
}
dynamic_batching {
  preferred_batch_size: 512
  preferred_batch_size: 1024
  max_queue_delay_microseconds: 1
}
optimization {
  cuda {
    graphs: true
  }
}

2023-03-04 04:14:29 - DEBUG - triton_config_model: Loading model TFT:1 model_control_mode: ModelControlMode.EXPLICIT
2023-03-04 04:14:29 - DEBUG - model_navigator.triton.client: Connecting to grpc://localhost:8001
2023-03-04 04:14:29 - DEBUG - triton_config_model: Waiting for server (timeout=5s)
is_server_ready, metadata ()

ready: true

is_server_live, metadata ()

live: true

2023-03-04 04:14:29 - DEBUG - triton_config_model: Sending load_model request
load_model, metadata ()
override files omitted:
model_name: "TFT"

Loaded model 'TFT'
2023-03-04 04:14:35 - DEBUG - triton_config_model: Polling for model availability (timeout=100s)
get_model_repository_index, metadata ()

models {
  name: "TFT"
  version: "1"
  state: "READY"
}

is_model_ready, metadata ()
name: "TFT"

ready: true

get_model_metadata, metadata ()
name: "TFT"

name: "TFT"
versions: "1"
platform: "tensorrt_plan"
inputs {
  name: "s_cat__0"
  datatype: "INT32"
  shape: -1
  shape: 192
  shape: 1
}
inputs {
  name: "k_cont__3"
  datatype: "FP32"
  shape: -1
  shape: 192
  shape: 3
}
inputs {
  name: "target__6"
  datatype: "FP32"
  shape: -1
  shape: 192
  shape: 1
}
outputs {
  name: "target__0"
  datatype: "FP32"
  shape: -1
  shape: 24
  shape: 3
}

2023-03-04 04:14:35 - DEBUG - model_navigator.utils.workspace: Workspace path /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:14:35 - DEBUG - model_navigator.results: Saving results of triton_config_model stage into /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/triton_config_model_results.yaml
2023-03-04 04:14:35 - DEBUG - model_navigator.results: ConfigModelResult(status=Status(state=<State.SUCCEEDED: 'succeeded'>, message='Model configured and loaded correctly', log_path=None), model_config=ModelConfig(model_name='TFT', model_path=PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model'), model_format=<Format.TENSORRT: 'trt'>, model_version='1'), model_version='1', batching_config=TritonBatchingConfig(max_batch_size=1024, batching=<Batching.DYNAMIC: 'dynamic'>), optimization_config=TritonModelOptimizationConfig(backend_accelerator=<BackendAccelerator.NONE: 'none'>, tensorrt_precision=<TensorRTOptPrecision.FP16: 'fp16'>, tensorrt_capture_cuda_graph=True), dynamic_batching_config=TritonDynamicBatchingConfig(preferred_batch_sizes=[512, 1024], max_queue_delay_us=1), instances_config=TritonModelInstancesConfig(engine_count_per_device={<DeviceKind.GPU: 'gpu'>: 2}), tensorrt_common_config=TensorRTCommonConfig(tensorrt_max_workspace_size=10000000000), model_dir_in_model_store=PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models/TFT'))
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ mkdir -p /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data
+ python triton/prepare_input_data.py --input-data-dir /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/ --dataset datasets/electricity_bin --checkpoint /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/electricity_bin/
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ python triton/run_performance_on_triton.py --model-repository /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models --model-name TFT --input-data /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/data.json --batch-sizes 1,2,4,8,16,32,64,128,256,512,1024 --number-of-triton-instances 1 --batching-mode static --evaluation-mode offline --measurement-request-count 500 --warmup --performance-tool perf_analyzer --result-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/triton_performance_offline.csv
2023-03-04 04:14:39,554 INFO run_performance_on_triton Running warmup before the main test
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 1
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 1000
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 1223
    Throughput: 611.5 infer/sec
    Avg latency: 1623 usec (standard deviation 143 usec)
    p50 latency: 1687 usec
    p90 latency: 1767 usec
    p95 latency: 1773 usec
    p99 latency: 1788 usec
    Avg HTTP time: 1615 usec (send/recv 64 usec + response wait 1551 usec)
  Server:
    Inference count: 1224
    Execution count: 1224
    Successful request count: 1224
    Avg request latency: 1396 usec (overhead 14 usec + queue 32 usec + compute input 112 usec + compute infer 1224 usec + compute output 14 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 611.5 infer/sec, latency 1623 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 1024
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 1000
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 1017
    Throughput: 52065.2 infer/sec
    Avg latency: 19644 usec (standard deviation 245 usec)
    p50 latency: 19623 usec
    p90 latency: 19955 usec
    p95 latency: 20019 usec
    p99 latency: 20194 usec
    Avg HTTP time: 19634 usec (send/recv 87 usec + response wait 19547 usec)
  Server:
    Inference count: 1042432
    Execution count: 1018
    Successful request count: 1018
    Avg request latency: 19360 usec (overhead 43 usec + queue 97 usec + compute input 514 usec + compute infer 18611 usec + compute output 95 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 52065.2 infer/sec, latency 19644 usec
2023-03-04 04:15:45,604 INFO run_performance_on_triton Using Perf Analyzer for performance evaluation
2023-03-04 04:15:45,605 INFO run_performance_on_triton Selected configuration
2023-03-04 04:15:45,605 INFO run_performance_on_triton 	server_url = http://127.0.0.1:8000
2023-03-04 04:15:45,605 INFO run_performance_on_triton 	model_name = TFT
2023-03-04 04:15:45,605 INFO run_performance_on_triton 	input_data = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/data.json
2023-03-04 04:15:45,605 INFO run_performance_on_triton 	input_shapes = []
2023-03-04 04:15:45,605 INFO run_performance_on_triton 	batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
2023-03-04 04:15:45,605 INFO run_performance_on_triton 	number_of_triton_instances = 1
2023-03-04 04:15:45,605 INFO run_performance_on_triton 	number_of_model_instances = 1
2023-03-04 04:15:45,605 INFO run_performance_on_triton 	measurement_mode = MeasurementMode.COUNT_WINDOWS
2023-03-04 04:15:45,605 INFO run_performance_on_triton 	measurement_interval = 5000
2023-03-04 04:15:45,605 INFO run_performance_on_triton 	measurement_request_count = 500
2023-03-04 04:15:45,605 INFO run_performance_on_triton 	concurrency_steps = 32
2023-03-04 04:15:45,605 INFO run_performance_on_triton 	batching_mode = BatchingMode.STATIC
2023-03-04 04:15:45,605 INFO run_performance_on_triton 	evaluation_mode = EvaluationMode.OFFLINE
2023-03-04 04:15:45,605 INFO run_performance_on_triton 	offline_mode = OfflineMode.SYSTEM
2023-03-04 04:15:45,605 INFO run_performance_on_triton 	output_shared_memory_size = 100240
2023-03-04 04:15:45,605 INFO run_performance_on_triton 	result_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/triton_performance_offline.csv
2023-03-04 04:15:45,605 INFO run_performance_on_triton 	verbose = False
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 1
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 639
    Throughput: 639 infer/sec
    Avg latency: 1555 usec (standard deviation 115 usec)
    p50 latency: 1604 usec
    p90 latency: 1660 usec
    p95 latency: 1664 usec
    p99 latency: 1671 usec
    Avg HTTP time: 1551 usec (send/recv 41 usec + response wait 1510 usec)
  Server:
    Inference count: 640
    Execution count: 640
    Successful request count: 640
    Avg request latency: 1370 usec (overhead 15 usec + queue 26 usec + compute input 96 usec + compute infer 1223 usec + compute output 10 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 639 infer/sec, latency 1555 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 2
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 959
    Throughput: 959 infer/sec
    Avg latency: 2082 usec (standard deviation 40 usec)
    p50 latency: 2094 usec
    p90 latency: 2106 usec
    p95 latency: 2108 usec
    p99 latency: 2114 usec
    Avg HTTP time: 2081 usec (send/recv 13 usec + response wait 2068 usec)
  Server:
    Inference count: 1918
    Execution count: 959
    Successful request count: 959
    Avg request latency: 2019 usec (overhead 6 usec + queue 13 usec + compute input 41 usec + compute infer 1947 usec + compute output 12 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 959 infer/sec, latency 2082 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 4
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 804
    Throughput: 1608 infer/sec
    Avg latency: 2483 usec (standard deviation 33 usec)
    p50 latency: 2473 usec
    p90 latency: 2531 usec
    p95 latency: 2536 usec
    p99 latency: 2543 usec
    Avg HTTP time: 2482 usec (send/recv 14 usec + response wait 2468 usec)
  Server:
    Inference count: 3220
    Execution count: 805
    Successful request count: 805
    Avg request latency: 2419 usec (overhead 5 usec + queue 13 usec + compute input 44 usec + compute infer 2345 usec + compute output 12 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 1608 infer/sec, latency 2483 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 8
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 803
    Throughput: 3212 infer/sec
    Avg latency: 2487 usec (standard deviation 11 usec)
    p50 latency: 2487 usec
    p90 latency: 2499 usec
    p95 latency: 2503 usec
    p99 latency: 2511 usec
    Avg HTTP time: 2486 usec (send/recv 14 usec + response wait 2472 usec)
  Server:
    Inference count: 6424
    Execution count: 803
    Successful request count: 803
    Avg request latency: 2424 usec (overhead 6 usec + queue 13 usec + compute input 47 usec + compute infer 2346 usec + compute output 12 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 3212 infer/sec, latency 2487 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 737
    Throughput: 5896 infer/sec
    Avg latency: 2708 usec (standard deviation 15 usec)
    p50 latency: 2708 usec
    p90 latency: 2727 usec
    p95 latency: 2731 usec
    p99 latency: 2740 usec
    Avg HTTP time: 2707 usec (send/recv 13 usec + response wait 2694 usec)
  Server:
    Inference count: 11808
    Execution count: 738
    Successful request count: 738
    Avg request latency: 2645 usec (overhead 6 usec + queue 12 usec + compute input 48 usec + compute infer 2567 usec + compute output 12 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 5896 infer/sec, latency 2708 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 32
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 716
    Throughput: 11456 infer/sec
    Avg latency: 2789 usec (standard deviation 30 usec)
    p50 latency: 2787 usec
    p90 latency: 2830 usec
    p95 latency: 2837 usec
    p99 latency: 2848 usec
    Avg HTTP time: 2788 usec (send/recv 14 usec + response wait 2774 usec)
  Server:
    Inference count: 22912
    Execution count: 716
    Successful request count: 716
    Avg request latency: 2725 usec (overhead 6 usec + queue 12 usec + compute input 57 usec + compute infer 2638 usec + compute output 12 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 11456 infer/sec, latency 2789 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 64
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 603
    Throughput: 19296 infer/sec
    Avg latency: 3309 usec (standard deviation 22 usec)
    p50 latency: 3308 usec
    p90 latency: 3330 usec
    p95 latency: 3336 usec
    p99 latency: 3347 usec
    Avg HTTP time: 3308 usec (send/recv 14 usec + response wait 3294 usec)
  Server:
    Inference count: 38656
    Execution count: 604
    Successful request count: 604
    Avg request latency: 3243 usec (overhead 6 usec + queue 12 usec + compute input 63 usec + compute infer 3149 usec + compute output 13 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 19296 infer/sec, latency 3309 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 128
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 611
    Throughput: 26060.6 infer/sec
    Avg latency: 4903 usec (standard deviation 66 usec)
    p50 latency: 4934 usec
    p90 latency: 4979 usec
    p95 latency: 4989 usec
    p99 latency: 5009 usec
    Avg HTTP time: 4902 usec (send/recv 14 usec + response wait 4888 usec)
  Server:
    Inference count: 78336
    Execution count: 612
    Successful request count: 612
    Avg request latency: 4839 usec (overhead 13 usec + queue 12 usec + compute input 90 usec + compute infer 4649 usec + compute output 75 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 26060.6 infer/sec, latency 4903 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 256
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 582
    Throughput: 37248 infer/sec
    Avg latency: 6862 usec (standard deviation 26 usec)
    p50 latency: 6860 usec
    p90 latency: 6891 usec
    p95 latency: 6897 usec
    p99 latency: 6932 usec
    Avg HTTP time: 6860 usec (send/recv 13 usec + response wait 6847 usec)
  Server:
    Inference count: 149248
    Execution count: 583
    Successful request count: 583
    Avg request latency: 6798 usec (overhead 19 usec + queue 12 usec + compute input 133 usec + compute infer 6499 usec + compute output 135 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 37248 infer/sec, latency 6862 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 512
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 541
    Throughput: 46157.6 infer/sec
    Avg latency: 11063 usec (standard deviation 260 usec)
    p50 latency: 11035 usec
    p90 latency: 11415 usec
    p95 latency: 11563 usec
    p99 latency: 11812 usec
    Avg HTTP time: 11054 usec (send/recv 87 usec + response wait 10967 usec)
  Server:
    Inference count: 277504
    Execution count: 542
    Successful request count: 542
    Avg request latency: 10664 usec (overhead 40 usec + queue 150 usec + compute input 394 usec + compute infer 10014 usec + compute output 66 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 46157.6 infer/sec, latency 11063 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 1024
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 505
    Throughput: 51712 infer/sec
    Avg latency: 19751 usec (standard deviation 208 usec)
    p50 latency: 19737 usec
    p90 latency: 19982 usec
    p95 latency: 20045 usec
    p99 latency: 20214 usec
    Avg HTTP time: 19740 usec (send/recv 95 usec + response wait 19645 usec)
  Server:
    Inference count: 518144
    Execution count: 506
    Successful request count: 506
    Avg request latency: 19449 usec (overhead 44 usec + queue 156 usec + compute input 476 usec + compute infer 18690 usec + compute output 83 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 51712 infer/sec, latency 19751 usec
  Batch    Concurrency    Inferences/Second    Client Send    Network+Server Send/Recv    Server Queue    Server Compute Input    Server Compute Infer    Server Compute Output    Client Recv    p50 latency    p90 latency    p95 latency    p99 latency    avg latency
-------  -------------  -------------------  -------------  --------------------------  --------------  ----------------------  ----------------------  -----------------------  -------------  -------------  -------------  -------------  -------------  -------------
      1              1                639               41                         157              26                      96                    1223                       10              0           1604           1660           1664           1671           1553
      2              1                959               13                          53              13                      41                    1947                       12              0           2094           2106           2108           2114           2079
      4              1               1608               14                          53              13                      44                    2345                       12              0           2473           2531           2536           2543           2481
      8              1               3212               14                          53              13                      47                    2346                       12              0           2487           2499           2503           2511           2485
     16              1               5896               13                          53              12                      48                    2567                       12              0           2708           2727           2731           2740           2705
     32              1              11456               14                          54              12                      57                    2638                       12              0           2787           2830           2837           2848           2787
     64              1              19296               14                          55              12                      63                    3149                       13              0           3308           3330           3336           3347           3306
    128              1              26060.6             14                          60              12                      90                    4649                       75              0           4934           4979           4989           5009           4900
    256              1              37248               13                          66              12                     133                    6499                      135              0           6860           6891           6897           6932           6858
    512              1              46157.6             87                         349             150                     394                   10014                       66              0          11035          11415          11563          11812          11060
   1024              1              51712               95                         247             156                     476                   18690                       83              0          19737          19982          20045          20214          19747
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed 's/^$/unknown/'
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ python triton/run_performance_on_triton.py --model-repository /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models --model-name TFT --input-data /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/data.json --batch-sizes 1,2,4,8,16,32,64,128,256,512,1024 --number-of-triton-instances 1 --number-of-model-instances 2 --batching-mode dynamic --evaluation-mode online --measurement-request-count 500 --warmup --performance-tool perf_analyzer --result-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/triton_performance_online.csv
2023-03-04 04:17:33,982 INFO run_performance_on_triton Running warmup before the main test
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 1000
  Latency limit: 0 msec
  Concurrency limit: 128 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 128
  Client:
    Request count: 2007
    Throughput: 32112 infer/sec
    Avg latency: 61680 usec (standard deviation 5639 usec)
    p50 latency: 63315 usec
    p90 latency: 63540 usec
    p95 latency: 63606 usec
    p99 latency: 63770 usec
    Avg HTTP time: 61687 usec (send/recv 100 usec + response wait 61587 usec)
  Server:
    Inference count: 32912
    Execution count: 47
    Successful request count: 47
    Avg request latency: 60649 usec (overhead 340 usec + queue 39088 usec + compute input 346 usec + compute infer 20820 usec + compute output 55 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 128, throughput: 32112 infer/sec, latency 61680 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 1000
  Latency limit: 0 msec
  Concurrency limit: 256 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 256
  Client:
    Request count: 2635
    Throughput: 42160 infer/sec
    Avg latency: 93341 usec (standard deviation 13058 usec)
    p50 latency: 92053 usec
    p90 latency: 108423 usec
    p95 latency: 115858 usec
    p99 latency: 117284 usec
    Avg HTTP time: 92272 usec (send/recv 877 usec + response wait 91395 usec)
  Server:
    Inference count: 45024
    Execution count: 54
    Successful request count: 54
    Avg request latency: 89984 usec (overhead 768 usec + queue 63505 usec + compute input 662 usec + compute infer 24974 usec + compute output 75 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 256, throughput: 42160 infer/sec, latency 93341 usec
2023-03-04 04:17:40,165 INFO run_performance_on_triton Using Perf Analyzer for performance evaluation
2023-03-04 04:17:40,165 INFO run_performance_on_triton Selected configuration
2023-03-04 04:17:40,165 INFO run_performance_on_triton 	server_url = http://127.0.0.1:8000
2023-03-04 04:17:40,165 INFO run_performance_on_triton 	model_name = TFT
2023-03-04 04:17:40,165 INFO run_performance_on_triton 	input_data = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/data.json
2023-03-04 04:17:40,165 INFO run_performance_on_triton 	input_shapes = []
2023-03-04 04:17:40,165 INFO run_performance_on_triton 	batch_sizes = [16]
2023-03-04 04:17:40,165 INFO run_performance_on_triton 	number_of_triton_instances = 1
2023-03-04 04:17:40,165 INFO run_performance_on_triton 	number_of_model_instances = 2
2023-03-04 04:17:40,166 INFO run_performance_on_triton 	measurement_mode = MeasurementMode.COUNT_WINDOWS
2023-03-04 04:17:40,166 INFO run_performance_on_triton 	measurement_interval = 5000
2023-03-04 04:17:40,166 INFO run_performance_on_triton 	measurement_request_count = 500
2023-03-04 04:17:40,166 INFO run_performance_on_triton 	concurrency_steps = 32
2023-03-04 04:17:40,166 INFO run_performance_on_triton 	batching_mode = BatchingMode.DYNAMIC
2023-03-04 04:17:40,166 INFO run_performance_on_triton 	evaluation_mode = EvaluationMode.ONLINE
2023-03-04 04:17:40,166 INFO run_performance_on_triton 	offline_mode = OfflineMode.SYSTEM
2023-03-04 04:17:40,166 INFO run_performance_on_triton 	output_shared_memory_size = 100240
2023-03-04 04:17:40,166 INFO run_performance_on_triton 	result_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/triton_performance_online.csv
2023-03-04 04:17:40,166 INFO run_performance_on_triton 	verbose = False
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 8 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 8
  Client:
    Request count: 1232
    Throughput: 19712 infer/sec
    Avg latency: 6448 usec (standard deviation 27 usec)
    p50 latency: 6446 usec
    p90 latency: 6483 usec
    p95 latency: 6495 usec
    p99 latency: 6513 usec
    Avg HTTP time: 6446 usec (send/recv 16 usec + response wait 6430 usec)
  Server:
    Inference count: 19776
    Execution count: 309
    Successful request count: 309
    Avg request latency: 6353 usec (overhead 27 usec + queue 3090 usec + compute input 71 usec + compute infer 3146 usec + compute output 19 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 8, throughput: 19712 infer/sec, latency 6448 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 16 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 16
  Client:
    Request count: 1323
    Throughput: 21168 infer/sec
    Avg latency: 11994 usec (standard deviation 2699 usec)
    p50 latency: 11853 usec
    p90 latency: 15976 usec
    p95 latency: 16206 usec
    p99 latency: 16407 usec
    Avg HTTP time: 11978 usec (send/recv 34 usec + response wait 11944 usec)
  Server:
    Inference count: 21200
    Execution count: 206
    Successful request count: 206
    Avg request latency: 11717 usec (overhead 55 usec + queue 6440 usec + compute input 92 usec + compute infer 5047 usec + compute output 83 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 16, throughput: 21168 infer/sec, latency 11994 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 24 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 24
  Client:
    Request count: 1606
    Throughput: 25696 infer/sec
    Avg latency: 14822 usec (standard deviation 3091 usec)
    p50 latency: 16369 usec
    p90 latency: 18100 usec
    p95 latency: 18391 usec
    p99 latency: 20356 usec
    Avg HTTP time: 14821 usec (send/recv 36 usec + response wait 14785 usec)
  Server:
    Inference count: 25920
    Execution count: 177
    Successful request count: 177
    Avg request latency: 14552 usec (overhead 88 usec + queue 7966 usec + compute input 123 usec + compute infer 6255 usec + compute output 120 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 24, throughput: 25696 infer/sec, latency 14822 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 32 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 32
  Client:
    Request count: 1690
    Throughput: 27040 infer/sec
    Avg latency: 18483 usec (standard deviation 3612 usec)
    p50 latency: 20137 usec
    p90 latency: 20422 usec
    p95 latency: 20515 usec
    p99 latency: 20688 usec
    Avg HTTP time: 18480 usec (send/recv 39 usec + response wait 18441 usec)
  Server:
    Inference count: 27600
    Execution count: 149
    Successful request count: 149
    Avg request latency: 17933 usec (overhead 150 usec + queue 7780 usec + compute input 240 usec + compute infer 9657 usec + compute output 106 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 32, throughput: 27040 infer/sec, latency 18483 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 40 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 40
  Client:
    Request count: 1564
    Throughput: 25024 infer/sec
    Avg latency: 25423 usec (standard deviation 4951 usec)
    p50 latency: 27572 usec
    p90 latency: 29200 usec
    p95 latency: 29398 usec
    p99 latency: 29649 usec
    Avg HTTP time: 25309 usec (send/recv 40 usec + response wait 25269 usec)
  Server:
    Inference count: 25456
    Execution count: 110
    Successful request count: 110
    Avg request latency: 24888 usec (overhead 130 usec + queue 13980 usec + compute input 207 usec + compute infer 10486 usec + compute output 85 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 40, throughput: 25024 infer/sec, latency 25423 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 48 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 48
  Client:
    Request count: 1790
    Throughput: 28640 infer/sec
    Avg latency: 26403 usec (standard deviation 4737 usec)
    p50 latency: 25638 usec
    p90 latency: 31114 usec
    p95 latency: 31309 usec
    p99 latency: 36092 usec
    Avg HTTP time: 26353 usec (send/recv 40 usec + response wait 26313 usec)
  Server:
    Inference count: 29088
    Execution count: 106
    Successful request count: 106
    Avg request latency: 25989 usec (overhead 135 usec + queue 16004 usec + compute input 197 usec + compute infer 9573 usec + compute output 80 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 48, throughput: 28640 infer/sec, latency 26403 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 56 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 56
  Client:
    Request count: 1774
    Throughput: 28384 infer/sec
    Avg latency: 31410 usec (standard deviation 5083 usec)
    p50 latency: 32629 usec
    p90 latency: 34491 usec
    p95 latency: 34548 usec
    p99 latency: 34715 usec
    Avg HTTP time: 31377 usec (send/recv 40 usec + response wait 31337 usec)
  Server:
    Inference count: 29088
    Execution count: 92
    Successful request count: 92
    Avg request latency: 30808 usec (overhead 201 usec + queue 14647 usec + compute input 234 usec + compute infer 15631 usec + compute output 95 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 56, throughput: 28384 infer/sec, latency 31410 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 64 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 64
  Client:
    Request count: 1925
    Throughput: 30800 infer/sec
    Avg latency: 32272 usec (standard deviation 6114 usec)
    p50 latency: 31176 usec
    p90 latency: 40004 usec
    p95 latency: 40435 usec
    p99 latency: 45662 usec
    Avg HTTP time: 32307 usec (send/recv 46 usec + response wait 32261 usec)
  Server:
    Inference count: 31632
    Execution count: 87
    Successful request count: 87
    Avg request latency: 31749 usec (overhead 199 usec + queue 18091 usec + compute input 278 usec + compute infer 13110 usec + compute output 71 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 64, throughput: 30800 infer/sec, latency 32272 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 72 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 72
  Client:
    Request count: 1931
    Throughput: 30896 infer/sec
    Avg latency: 36238 usec (standard deviation 7976 usec)
    p50 latency: 37967 usec
    p90 latency: 45613 usec
    p95 latency: 46863 usec
    p99 latency: 47001 usec
    Avg HTTP time: 36216 usec (send/recv 45 usec + response wait 36171 usec)
  Server:
    Inference count: 31744
    Execution count: 78
    Successful request count: 78
    Avg request latency: 35554 usec (overhead 275 usec + queue 17115 usec + compute input 315 usec + compute infer 17778 usec + compute output 71 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 72, throughput: 30896 infer/sec, latency 36238 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 80 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 80
  Client:
    Request count: 2031
    Throughput: 32496 infer/sec
    Avg latency: 38484 usec (standard deviation 5982 usec)
    p50 latency: 39857 usec
    p90 latency: 40758 usec
    p95 latency: 48710 usec
    p99 latency: 50279 usec
    Avg HTTP time: 38491 usec (send/recv 45 usec + response wait 38446 usec)
  Server:
    Inference count: 33344
    Execution count: 75
    Successful request count: 75
    Avg request latency: 37570 usec (overhead 250 usec + queue 20806 usec + compute input 294 usec + compute infer 16157 usec + compute output 63 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 80, throughput: 32496 infer/sec, latency 38484 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 88 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 88
  Client:
    Request count: 2157
    Throughput: 34512 infer/sec
    Avg latency: 40236 usec (standard deviation 5599 usec)
    p50 latency: 40172 usec
    p90 latency: 45344 usec
    p95 latency: 45417 usec
    p99 latency: 45504 usec
    Avg HTTP time: 40191 usec (send/recv 50 usec + response wait 40141 usec)
  Server:
    Inference count: 35040
    Execution count: 72
    Successful request count: 72
    Avg request latency: 39366 usec (overhead 316 usec + queue 21839 usec + compute input 333 usec + compute infer 16812 usec + compute output 66 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 88, throughput: 34512 infer/sec, latency 40236 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 96 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 96
  Client:
    Request count: 2037
    Throughput: 32592 infer/sec
    Avg latency: 46005 usec (standard deviation 5311 usec)
    p50 latency: 47084 usec
    p90 latency: 48889 usec
    p95 latency: 48967 usec
    p99 latency: 49060 usec
    Avg HTTP time: 45987 usec (send/recv 42 usec + response wait 45945 usec)
  Server:
    Inference count: 33328
    Execution count: 63
    Successful request count: 63
    Avg request latency: 44786 usec (overhead 360 usec + queue 24326 usec + compute input 363 usec + compute infer 19675 usec + compute output 62 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 96, throughput: 32592 infer/sec, latency 46005 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 104 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 104
  Client:
    Request count: 2142
    Throughput: 34272 infer/sec
    Avg latency: 47594 usec (standard deviation 4982 usec)
    p50 latency: 48973 usec
    p90 latency: 49146 usec
    p95 latency: 49188 usec
    p99 latency: 49449 usec
    Avg HTTP time: 47600 usec (send/recv 41 usec + response wait 47559 usec)
  Server:
    Inference count: 35088
    Execution count: 61
    Successful request count: 61
    Avg request latency: 46432 usec (overhead 356 usec + queue 26792 usec + compute input 338 usec + compute infer 18884 usec + compute output 62 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 104, throughput: 34272 infer/sec, latency 47594 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 112 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 112
  Client:
    Request count: 2301
    Throughput: 36816 infer/sec
    Avg latency: 46856 usec (standard deviation 4873 usec)
    p50 latency: 47625 usec
    p90 latency: 49068 usec
    p95 latency: 49126 usec
    p99 latency: 49275 usec
    Avg HTTP time: 46823 usec (send/recv 65 usec + response wait 46758 usec)
  Server:
    Inference count: 37696
    Execution count: 62
    Successful request count: 62
    Avg request latency: 45727 usec (overhead 446 usec + queue 25217 usec + compute input 375 usec + compute infer 19614 usec + compute output 75 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 112, throughput: 36816 infer/sec, latency 46856 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 120 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 120
  Client:
    Request count: 2248
    Throughput: 35968 infer/sec
    Avg latency: 52358 usec (standard deviation 5407 usec)
    p50 latency: 54353 usec
    p90 latency: 54911 usec
    p95 latency: 54998 usec
    p99 latency: 55188 usec
    Avg HTTP time: 52261 usec (send/recv 47 usec + response wait 52214 usec)
  Server:
    Inference count: 36736
    Execution count: 56
    Successful request count: 56
    Avg request latency: 50867 usec (overhead 386 usec + queue 30717 usec + compute input 421 usec + compute infer 19260 usec + compute output 83 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 120, throughput: 35968 infer/sec, latency 52358 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 128 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 128
  Client:
    Request count: 2620
    Throughput: 41920 infer/sec
    Avg latency: 48055 usec (standard deviation 4431 usec)
    p50 latency: 49120 usec
    p90 latency: 49243 usec
    p95 latency: 49299 usec
    p99 latency: 49564 usec
    Avg HTTP time: 48061 usec (send/recv 58 usec + response wait 48003 usec)
  Server:
    Inference count: 42880
    Execution count: 61
    Successful request count: 61
    Avg request latency: 46587 usec (overhead 438 usec + queue 26612 usec + compute input 401 usec + compute infer 19045 usec + compute output 91 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 128, throughput: 41920 infer/sec, latency 48055 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 136 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 136
  Client:
    Request count: 2439
    Throughput: 38985 infer/sec
    Avg latency: 54052 usec (standard deviation 9182 usec)
    p50 latency: 50319 usec
    p90 latency: 63831 usec
    p95 latency: 63977 usec
    p99 latency: 64548 usec
    Avg HTTP time: 53831 usec (send/recv 64 usec + response wait 53767 usec)
  Server:
    Inference count: 40400
    Execution count: 55
    Successful request count: 55
    Avg request latency: 52297 usec (overhead 469 usec + queue 30682 usec + compute input 462 usec + compute infer 20604 usec + compute output 80 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 136, throughput: 38985 infer/sec, latency 54052 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 144 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 144
  Client:
    Request count: 2249
    Throughput: 35984 infer/sec
    Avg latency: 62251 usec (standard deviation 5195 usec)
    p50 latency: 63592 usec
    p90 latency: 64059 usec
    p95 latency: 64248 usec
    p99 latency: 64521 usec
    Avg HTTP time: 62250 usec (send/recv 207 usec + response wait 62043 usec)
  Server:
    Inference count: 36736
    Execution count: 47
    Successful request count: 47
    Avg request latency: 61196 usec (overhead 409 usec + queue 39524 usec + compute input 413 usec + compute infer 20774 usec + compute output 76 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 144, throughput: 35984 infer/sec, latency 62251 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 152 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 152
  Client:
    Request count: 2394
    Throughput: 38304 infer/sec
    Avg latency: 61995 usec (standard deviation 6111 usec)
    p50 latency: 63542 usec
    p90 latency: 63773 usec
    p95 latency: 63970 usec
    p99 latency: 73752 usec
    Avg HTTP time: 61761 usec (send/recv 128 usec + response wait 61633 usec)
  Server:
    Inference count: 38992
    Execution count: 48
    Successful request count: 48
    Avg request latency: 60253 usec (overhead 460 usec + queue 38112 usec + compute input 430 usec + compute infer 21179 usec + compute output 72 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 152, throughput: 38304 infer/sec, latency 61995 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 160 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 160
  Client:
    Request count: 2239
    Throughput: 35788.2 infer/sec
    Avg latency: 69710 usec (standard deviation 2857 usec)
    p50 latency: 70135 usec
    p90 latency: 70395 usec
    p95 latency: 70512 usec
    p99 latency: 70836 usec
    Avg HTTP time: 69699 usec (send/recv 150 usec + response wait 69549 usec)
  Server:
    Inference count: 36064
    Execution count: 56
    Successful request count: 56
    Avg request latency: 68415 usec (overhead 391 usec + queue 48050 usec + compute input 369 usec + compute infer 19534 usec + compute output 71 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 160, throughput: 35788.2 infer/sec, latency 69710 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 168 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 168
  Client:
    Request count: 2159
    Throughput: 34544 infer/sec
    Avg latency: 74985 usec (standard deviation 8949 usec)
    p50 latency: 75780 usec
    p90 latency: 84734 usec
    p95 latency: 84830 usec
    p99 latency: 85059 usec
    Avg HTTP time: 74947 usec (send/recv 281 usec + response wait 74666 usec)
  Server:
    Inference count: 35744
    Execution count: 53
    Successful request count: 53
    Avg request latency: 74023 usec (overhead 357 usec + queue 52562 usec + compute input 465 usec + compute infer 20569 usec + compute output 70 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 168, throughput: 34544 infer/sec, latency 74985 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 176 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 176
  Client:
    Request count: 2735
    Throughput: 43760 infer/sec
    Avg latency: 62702 usec (standard deviation 4615 usec)
    p50 latency: 63778 usec
    p90 latency: 64053 usec
    p95 latency: 64141 usec
    p99 latency: 64243 usec
    Avg HTTP time: 62699 usec (send/recv 132 usec + response wait 62567 usec)
  Server:
    Inference count: 44960
    Execution count: 47
    Successful request count: 47
    Avg request latency: 60758 usec (overhead 472 usec + queue 38945 usec + compute input 488 usec + compute infer 20768 usec + compute output 85 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 176, throughput: 43760 infer/sec, latency 62702 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 184 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 184
  Client:
    Request count: 2337
    Throughput: 37392 infer/sec
    Avg latency: 75590 usec (standard deviation 2779 usec)
    p50 latency: 75981 usec
    p90 latency: 76251 usec
    p95 latency: 76338 usec
    p99 latency: 76485 usec
    Avg HTTP time: 75579 usec (send/recv 183 usec + response wait 75396 usec)
  Server:
    Inference count: 38960
    Execution count: 53
    Successful request count: 53
    Avg request latency: 73954 usec (overhead 379 usec + queue 53735 usec + compute input 481 usec + compute infer 19271 usec + compute output 88 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 184, throughput: 37392 infer/sec, latency 75590 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 192 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 192
  Client:
    Request count: 2711
    Throughput: 43376 infer/sec
    Avg latency: 66977 usec (standard deviation 2583 usec)
    p50 latency: 67520 usec
    p90 latency: 67797 usec
    p95 latency: 67943 usec
    p99 latency: 68299 usec
    Avg HTTP time: 66970 usec (send/recv 614 usec + response wait 66356 usec)
  Server:
    Inference count: 45376
    Execution count: 59
    Successful request count: 59
    Avg request latency: 65491 usec (overhead 693 usec + queue 44685 usec + compute input 642 usec + compute infer 19389 usec + compute output 82 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 192, throughput: 43376 infer/sec, latency 66977 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 200 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 200
  Client:
    Request count: 2264
    Throughput: 36224 infer/sec
    Avg latency: 83099 usec (standard deviation 6099 usec)
    p50 latency: 84768 usec
    p90 latency: 85069 usec
    p95 latency: 86642 usec
    p99 latency: 94476 usec
    Avg HTTP time: 83164 usec (send/recv 502 usec + response wait 82662 usec)
  Server:
    Inference count: 37824
    Execution count: 48
    Successful request count: 48
    Avg request latency: 81803 usec (overhead 427 usec + queue 59473 usec + compute input 418 usec + compute infer 21415 usec + compute output 70 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 200, throughput: 36224 infer/sec, latency 83099 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 208 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 208
  Client:
    Request count: 2151
    Throughput: 34416 infer/sec
    Avg latency: 91515 usec (standard deviation 1673 usec)
    p50 latency: 91640 usec
    p90 latency: 91868 usec
    p95 latency: 91946 usec
    p99 latency: 92050 usec
    Avg HTTP time: 91518 usec (send/recv 484 usec + response wait 91034 usec)
  Server:
    Inference count: 36128
    Execution count: 54
    Successful request count: 54
    Avg request latency: 90018 usec (overhead 361 usec + queue 69274 usec + compute input 413 usec + compute infer 19914 usec + compute output 56 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 208, throughput: 34416 infer/sec, latency 91515 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 216 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 216
  Client:
    Request count: 2644
    Throughput: 42261.7 infer/sec
    Avg latency: 78946 usec (standard deviation 9783 usec)
    p50 latency: 81024 usec
    p90 latency: 85364 usec
    p95 latency: 95070 usec
    p99 latency: 101007 usec
    Avg HTTP time: 79093 usec (send/recv 653 usec + response wait 78440 usec)
  Server:
    Inference count: 44496
    Execution count: 60
    Successful request count: 60
    Avg request latency: 77402 usec (overhead 581 usec + queue 53846 usec + compute input 535 usec + compute infer 22373 usec + compute output 67 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 216, throughput: 42261.7 infer/sec, latency 78946 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 224 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 224
  Client:
    Request count: 2535
    Throughput: 40519.5 infer/sec
    Avg latency: 84852 usec (standard deviation 2620 usec)
    p50 latency: 85193 usec
    p90 latency: 85385 usec
    p95 latency: 85468 usec
    p99 latency: 85659 usec
    Avg HTTP time: 84843 usec (send/recv 621 usec + response wait 84222 usec)
  Server:
    Inference count: 42192
    Execution count: 47
    Successful request count: 47
    Avg request latency: 83093 usec (overhead 479 usec + queue 61210 usec + compute input 514 usec + compute infer 20821 usec + compute output 69 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 224, throughput: 40519.5 infer/sec, latency 84852 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 232 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 232
  Client:
    Request count: 2222
    Throughput: 35552 infer/sec
    Avg latency: 96894 usec (standard deviation 1774 usec)
    p50 latency: 97058 usec
    p90 latency: 97341 usec
    p95 latency: 97428 usec
    p99 latency: 97603 usec
    Avg HTTP time: 96891 usec (send/recv 640 usec + response wait 96251 usec)
  Server:
    Inference count: 37568
    Execution count: 51
    Successful request count: 51
    Avg request latency: 95435 usec (overhead 391 usec + queue 74737 usec + compute input 448 usec + compute infer 19785 usec + compute output 74 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 232, throughput: 35552 infer/sec, latency 96894 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 240 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 240
  Client:
    Request count: 2582
    Throughput: 41312 infer/sec
    Avg latency: 87393 usec (standard deviation 10043 usec)
    p50 latency: 90047 usec
    p90 latency: 97372 usec
    p95 latency: 101367 usec
    p99 latency: 106440 usec
    Avg HTTP time: 87551 usec (send/recv 750 usec + response wait 86801 usec)
  Server:
    Inference count: 43456
    Execution count: 57
    Successful request count: 57
    Avg request latency: 85703 usec (overhead 523 usec + queue 61984 usec + compute input 586 usec + compute infer 22547 usec + compute output 63 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 240, throughput: 41312 infer/sec, latency 87393 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 248 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 248
  Client:
    Request count: 2616
    Throughput: 41856 infer/sec
    Avg latency: 91042 usec (standard deviation 3739 usec)
    p50 latency: 92017 usec
    p90 latency: 92184 usec
    p95 latency: 92278 usec
    p99 latency: 100381 usec
    Avg HTTP time: 91070 usec (send/recv 625 usec + response wait 90445 usec)
  Server:
    Inference count: 43696
    Execution count: 55
    Successful request count: 55
    Avg request latency: 88975 usec (overhead 460 usec + queue 67234 usec + compute input 510 usec + compute infer 20678 usec + compute output 93 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 248, throughput: 41856 infer/sec, latency 91042 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 256 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 256
  Client:
    Request count: 2831
    Throughput: 45296 infer/sec
    Avg latency: 86800 usec (standard deviation 7553 usec)
    p50 latency: 87109 usec
    p90 latency: 92173 usec
    p95 latency: 98127 usec
    p99 latency: 102144 usec
    Avg HTTP time: 86538 usec (send/recv 944 usec + response wait 85594 usec)
  Server:
    Inference count: 47168
    Execution count: 57
    Successful request count: 57
    Avg request latency: 84198 usec (overhead 686 usec + queue 60915 usec + compute input 678 usec + compute infer 21819 usec + compute output 100 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 256, throughput: 45296 infer/sec, latency 86800 usec
  Batch    Concurrency    Inferences/Second    Client Send    Network+Server Send/Recv    Server Queue    Server Compute Input    Server Compute Infer    Server Compute Output    Client Recv    p50 latency    p90 latency    p95 latency    p99 latency    avg latency
-------  -------------  -------------------  -------------  --------------------------  --------------  ----------------------  ----------------------  -----------------------  -------------  -------------  -------------  -------------  -------------  -------------
     16              8              19712               16                         103            3090                      71                    3146                       19              0           6446           6483           6495           6513           6445
     16             16              21168               34                         296            6440                      92                    5047                       83              0          11853          15976          16206          16407          11992
     16             24              25696               36                         318            7966                     123                    6255                      120              0          16369          18100          18391          20356          14818
     16             32              27040               39                         658            7780                     240                    9657                      106              0          20137          20422          20515          20688          18480
     16             40              25024               40                         622           13980                     207                   10486                       85              0          27572          29200          29398          29649          25420
     16             48              28640               40                         505           16004                     197                    9573                       80              0          25638          31114          31309          36092          26399
     16             56              28384               40                         760           14647                     234                   15631                       95              0          32629          34491          34548          34715          31407
     16             64              30800               46                         673           18091                     278                   13110                       71              0          31176          40004          40435          45662          32269
     16             72              30896               45                         911           17115                     315                   17778                       71              0          37967          45613          46863          47001          36235
     16             80              32496               45                        1116           20806                     294                   16157                       63              0          39857          40758          48710          50279          38481
     16             88              34512               50                        1133           21839                     333                   16812                       66              0          40172          45344          45417          45504          40233
     16             96              32592               42                        1534           24326                     363                   19675                       62              0          47084          48889          48967          49060          46002
     16            104              34272               41                        1475           26792                     338                   18884                       62              0          48973          49146          49188          49449          47592
     16            112              36816               65                        1507           25217                     375                   19614                       75              0          47625          49068          49126          49275          46853
     16            120              35968               47                        1828           30717                     421                   19260                       83              0          54353          54911          54998          55188          52356
     16            128              41920               58                        1845           26612                     401                   19045                       91              0          49120          49243          49299          49564          48052
     16            136              38985               64                        2157           30682                     462                   20604                       80              0          50319          63831          63977          64548          54049
     16            144              35984              207                        1256           39524                     413                   20774                       76              0          63592          64059          64248          64521          62250
     16            152              38304              128                        2071           38112                     430                   21179                       72              0          63542          63773          63970          73752          61992
     16            160              35788.2            150                        1532           48050                     369                   19534                       71              0          70135          70395          70512          70836          69706
     16            168              34544              281                        1035           52562                     465                   20569                       70              0          75780          84734          84830          85059          74982
     16            176              43760              132                        2281           38945                     488                   20768                       85              0          63778          64053          64141          64243          62699
     16            184              37392              183                        1830           53735                     481                   19271                       88              0          75981          76251          76338          76485          75588
     16            192              43376              614                        1562           44685                     642                   19389                       82              0          67520          67797          67943          68299          66974
     16            200              36224              502                        1219           59473                     418                   21415                       70              0          84768          85069          86642          94476          83097
     16            208              34416              484                        1370           69274                     413                   19914                       56              0          91640          91868          91946          92050          91511
     16            216              42261.7            653                        1468           53846                     535                   22373                       67              0          81024          85364          95070         101007          78942
     16            224              40519.5            621                        1614           61210                     514                   20821                       69              0          85193          85385          85468          85659          84849
     16            232              35552              640                        1206           74737                     448                   19785                       74              0          97058          97341          97428          97603          96890
     16            240              41312              750                        1460           61984                     586                   22547                       63              0          90047          97372         101367         106440          87390
     16            248              41856              625                        1900           67234                     510                   20678                       93              0          92017          92184          92278         100381          91040
     16            256              45296              944                        2342           60915                     678                   21819                      100              0          87109          92173          98127         102144          86798
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ [[ onnx == \t\s\-\t\r\a\c\e ]]
+ [[ onnx == \t\s\-\s\c\r\i\p\t ]]
+ export FORMAT_SUFFIX=onnx
+ FORMAT_SUFFIX=onnx
+ python3 triton/export_model.py --input-path triton/model.py --input-type pyt --output-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx --output-type onnx --ignore-unknown-parameters --onnx-opset 13 --checkpoint /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/traffic_bin/ --precision fp32 --dataloader triton/dataloader.py --dataset datasets/traffic_bin --batch-size 1
Got additional args []
2023-03-04 04:19:22,703 INFO export_model args:
2023-03-04 04:19:22,703 INFO export_model     input_path = triton/model.py
2023-03-04 04:19:22,703 INFO export_model     input_type = pyt
2023-03-04 04:19:22,703 INFO export_model     output_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx
2023-03-04 04:19:22,703 INFO export_model     output_type = onnx
2023-03-04 04:19:22,703 INFO export_model     dataloader = triton/dataloader.py
2023-03-04 04:19:22,703 INFO export_model     verbose = False
2023-03-04 04:19:22,703 INFO export_model     ignore_unknown_parameters = True
2023-03-04 04:19:22,703 INFO export_model     precision = fp32
2023-03-04 04:19:22,703 INFO export_model     onnx_opset = 13
2023-03-04 04:19:22,703 INFO export_model     dataset = datasets/traffic_bin
2023-03-04 04:19:22,703 INFO export_model     checkpoint = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/traffic_bin/
2023-03-04 04:19:22,703 INFO export_model     batch_size = 1
2023-03-04 04:19:22,704 INFO triton.deployment_toolkit.args Initializing get_dataloader_fn({'dataset': 'datasets/traffic_bin', 'checkpoint': '/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/traffic_bin/', 'batch_size': '1'})
2023-03-04 04:19:24,368 INFO triton.deployment_toolkit.args Initializing PyTorchModelLoader({'checkpoint': '/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/traffic_bin/', 'precision': 'fp32'})
2023-03-04 04:19:25,693 INFO export_model inputs: {'s_cat__0': TensorSpec(name='s_cat__0', shape=(-1, 192, 1), dtype=dtype('int64')), 's_cont__1': TensorSpec(name='s_cont__1', shape=(-1, 1), dtype=dtype('float32')), 'k_cat__2': TensorSpec(name='k_cat__2', shape=(-1, 1), dtype=dtype('float32')), 'k_cont__3': TensorSpec(name='k_cont__3', shape=(-1, 192, 3), dtype=dtype('float32')), 'o_cat__4': TensorSpec(name='o_cat__4', shape=(-1, 1), dtype=dtype('float32')), 'o_cont__5': TensorSpec(name='o_cont__5', shape=(-1, 1), dtype=dtype('float32')), 'target__6': TensorSpec(name='target__6', shape=(-1, 192, 1), dtype=dtype('float32')), 'id__7': TensorSpec(name='id__7', shape=(-1, 192, 1), dtype=dtype('int64'))}
2023-03-04 04:19:25,693 INFO export_model outputs: {'target__0': TensorSpec(name='target__0', shape=(-1, 24, 3), dtype=dtype('float32'))}
2023-03-04 04:19:25,694 INFO triton.deployment_toolkit.args Initializing PYT2ONNXSaver({'onnx_opset': 13})
triton/model.py:33: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['s_cat'] = s_cat if s_cat.shape[1] != 1 else None
triton/model.py:34: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['s_cont'] = s_cont if s_cont.shape[1] != 1 else None
triton/model.py:35: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['k_cat'] = k_cat if k_cat.shape[1] != 1 else None
triton/model.py:36: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['k_cont'] = k_cont if k_cont.shape[1] != 1 else None
triton/model.py:37: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['o_cat'] = o_cat if o_cat.shape[1] != 1 else None
triton/model.py:38: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['o_cont'] = o_cont if o_cont.shape[1] != 1 else None
/opt/conda/lib/python3.8/site-packages/torch/onnx/symbolic_opset9.py:2161: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. 
  warnings.warn("Exporting a model to ONNX with a batch_size other than 1, " +
WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ [[ onnx == \t\s\-\t\r\a\c\e ]]
+ [[ onnx == \t\s\-\s\c\r\i\p\t ]]
+ export FORMAT_SUFFIX=onnx
+ FORMAT_SUFFIX=onnx
+ model-navigator convert --model-name TFT --model-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx --output-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model --target-formats trt --target-precisions fp16 --launch-mode local --override-workspace --verbose --onnx-opsets 13 --max-batch-size 1024 --container-version 21.08 --max-workspace-size 10000000000 --atol target__0=100 --rtol target__0=100
2023-03-04 04:19:29 - DEBUG - model_navigator.log: initialized logger verbose=True
2023-03-04 04:19:29 - DEBUG - convert: Running 'model-navigator convert' with config_path: None
2023-03-04 04:19:29 - INFO - model_navigator.log: Environment state
2023-03-04 04:19:29 - INFO - model_navigator.log: 	docker_container_id = 98c4b3fc07
2023-03-04 04:19:29 - INFO - model_navigator.log: 	docker_container_ip_address = None
2023-03-04 04:19:29 - INFO - model_navigator.log: 	docker_container_mounts = [{'host_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT'), 'container_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT'), 'mount_type': 'bind'}, {'host_path': PosixPath('/var/run/docker.sock'), 'container_path': PosixPath('/var/run/docker.sock'), 'mount_type': 'bind'}, {'host_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/dataset/processed'), 'container_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/datasets'), 'mount_type': 'bind'}]
2023-03-04 04:19:29 - DEBUG - model_navigator.validators: Running command validator: <model_navigator.validators.ModelNavigatorDockerContainerShouldHaveMountedWorkspaceDir object at 0x7f3dc9157190>
2023-03-04 04:19:29 - DEBUG - model_navigator.validators: ModelNavigatorDockerContainerShouldHaveMountedWorkspaceDir is_running_in_container=True has_mounted_workspace_path=True is_running_converter_in_docker=False
2023-03-04 04:19:29 - DEBUG - model_navigator.utils.workspace: Workspace path /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:19:29 - INFO - model_navigator.log: convert args:
2023-03-04 04:19:29 - INFO - model_navigator.log: 	model_name = TFT
2023-03-04 04:19:29 - INFO - model_navigator.log: 	model_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx
2023-03-04 04:19:29 - INFO - model_navigator.log: 	model_format = None
2023-03-04 04:19:29 - INFO - model_navigator.log: 	model_version = 1
2023-03-04 04:19:29 - INFO - model_navigator.log: 	target_formats = [<Format.TENSORRT: 'trt'>]
2023-03-04 04:19:29 - INFO - model_navigator.log: 	onnx_opsets = [13]
2023-03-04 04:19:29 - INFO - model_navigator.log: 	tensorrt_precisions = [<TensorRTPrecision.FP16: 'fp16'>]
2023-03-04 04:19:29 - INFO - model_navigator.log: 	tensorrt_precisions_mode = TensorRTPrecisionMode.HIERARCHY
2023-03-04 04:19:29 - INFO - model_navigator.log: 	tensorrt_explicit_precision = False
2023-03-04 04:19:29 - INFO - model_navigator.log: 	tensorrt_strict_types = False
2023-03-04 04:19:29 - INFO - model_navigator.log: 	tensorrt_sparse_weights = False
2023-03-04 04:19:29 - INFO - model_navigator.log: 	tensorrt_max_workspace_size = 10000000000
2023-03-04 04:19:29 - INFO - model_navigator.log: 	atol = {'target__0': 100.0}
2023-03-04 04:19:29 - INFO - model_navigator.log: 	rtol = {'target__0': 100.0}
2023-03-04 04:19:29 - INFO - model_navigator.log: 	max_batch_size = 1024
2023-03-04 04:19:29 - INFO - model_navigator.log: 	inputs = None
2023-03-04 04:19:29 - INFO - model_navigator.log: 	outputs = None
2023-03-04 04:19:29 - INFO - model_navigator.log: 	min_shapes = None
2023-03-04 04:19:29 - INFO - model_navigator.log: 	opt_shapes = None
2023-03-04 04:19:29 - INFO - model_navigator.log: 	max_shapes = None
2023-03-04 04:19:29 - INFO - model_navigator.log: 	value_ranges = None
2023-03-04 04:19:29 - INFO - model_navigator.log: 	dtypes = None
2023-03-04 04:19:29 - INFO - model_navigator.log: 	workspace_path = navigator_workspace
2023-03-04 04:19:29 - INFO - model_navigator.log: 	override_workspace = True
2023-03-04 04:19:29 - INFO - model_navigator.log: 	output_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model
2023-03-04 04:19:29 - INFO - model_navigator.log: 	container_version = 21.08
2023-03-04 04:19:29 - INFO - model_navigator.log: 	framework_docker_image = nvcr.io/nvidia/pytorch:21.08-py3
2023-03-04 04:19:29 - INFO - model_navigator.log: 	gpus = ('all',)
2023-03-04 04:19:29 - DEBUG - model_navigator.utils.workspace: Cleaning workspace dir /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:19:29 - DEBUG - model_navigator.converter.convert: Converter created; workspace=/home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:19:29 - DEBUG - model_navigator.converter.convert: Convert Commands Executor created; workspace=/home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:19:29 - DEBUG - model_navigator.converter.transformers: Running command polygraphyonnx2trt_fp16_mh on /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx
2023-03-04 04:19:29 - INFO - polygraphy.transformers: Polygraphy onnx2trt started.
2023-03-04 04:19:29 - WARNING - polygraphy.transformers: This conversion should be done on target GPU platform
2023-03-04 04:19:29 - DEBUG - polygraphy.transformers: Use min_shapes: {'s_cat__0': (1, 192, 1), 'k_cont__3': (1, 192, 3), 'target__6': (1, 192, 1)} as runtime shape
================================
/opt/conda/bin/polygraphy run --onnxrt --trt /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx --model-type onnx --inputs s_cat__0:[1,192,1] k_cont__3:[1,192,3] target__6:[1,192,1] --onnx-outputs target__0 --shape-inference --tf32 --fp16 --trt-min-shapes s_cat__0:[1,192,1] k_cont__3:[1,192,3] target__6:[1,192,1] --trt-opt-shapes s_cat__0:[1024,192,1] k_cont__3:[1024,192,3] target__6:[1024,192,1] --trt-max-shapes s_cat__0:[1024,192,1] k_cont__3:[1024,192,3] target__6:[1024,192,1] --save-inputs /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_inputs.json --save-outputs /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_outputs.json --validate --rtol target__0:100.0 1e-05 --atol target__0:100.0 1e-05 --workspace 10000000000 --save-engine /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan -v
================================


2023-03-04 04:19:29 - DEBUG - asyncio: Using selector: EpollSelector
	[38;5;13m[V] Model: /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx[0m
	[38;5;11m[W] --workspace is deprecated and will be removed in Polygraphy 0.45.0. Use --pool-limit workspace:10000000000 instead.[0m
	[38;5;14m[I] RUNNING | Command: /opt/conda/bin/polygraphy run --onnxrt --trt /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx --model-type onnx --inputs s_cat__0:[1,192,1] k_cont__3:[1,192,3] target__6:[1,192,1] --onnx-outputs target__0 --shape-inference --tf32 --fp16 --trt-min-shapes s_cat__0:[1,192,1] k_cont__3:[1,192,3] target__6:[1,192,1] --trt-opt-shapes s_cat__0:[1024,192,1] k_cont__3:[1024,192,3] target__6:[1024,192,1] --trt-max-shapes s_cat__0:[1024,192,1] k_cont__3:[1024,192,3] target__6:[1024,192,1] --save-inputs /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_inputs.json --save-outputs /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_outputs.json --validate --rtol target__0:100.0 1e-05 --atol target__0:100.0 1e-05 --workspace 10000000000 --save-engine /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan -v[0m
	[38;5;13m[V] Loaded Module: polygraphy | Version: 0.44.2 | Path: ['/opt/conda/lib/python3.8/site-packages/polygraphy'][0m
	[38;5;13m[V] Loaded extension modules: [][0m
	[38;5;13m[V] Will save runner results to: /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_outputs.json[0m
	[38;5;13m[V] Loaded Module: tensorrt | Version: 8.2.1.8 | Path: ['/opt/conda/lib/python3.8/site-packages/tensorrt'][0m
	[I] Will generate inference input data according to provided TensorMetadata: {s_cat__0 [shape=(1, 192, 1)],
	     k_cont__3 [shape=(1, 192, 3)],
	     target__6 [shape=(1, 192, 1)]}
	[38;5;14m[I] onnxrt-runner-N0-03/04/23-04:19:29  | Activating and starting inference[0m
	[38;5;13m[V] Starting shape inference[0m
	[38;5;11m[W] Falling back to `onnx.shape_inference` because `onnxruntime.tools.symbolic_shape_infer` could not be loaded.
	    Note that using ONNX-Runtime for shape inference may be faster and require less memory.
	    Consider installing ONNX-Runtime or settting POLYGRAPHY_AUTOINSTALL_DEPS=1 in your environment variables to allow Polygraphy to do so automatically.[0m
	[38;5;13m[V] Loaded Module: onnx | Version: 1.8.1 | Path: ['/opt/conda/lib/python3.8/site-packages/onnx'][0m
	[38;5;13m[V] Writing shape-inferred model to: /tmp/tmp_polygraphy_c625b0d2ece720a458519de52161257156b3dcccf7197040.onnx[0m
	[38;5;13m[V] Loaded Module: onnx.shape_inference[0m
	[I] Loading model: /tmp/tmp_polygraphy_c625b0d2ece720a458519de52161257156b3dcccf7197040.onnx
	[38;5;13m[V] Loading external data from: /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared[0m
	[38;5;13m[V] Loaded Module: onnx.external_data_helper[0m
	[38;5;13m[V] Shape inference completed successfully[0m
	[38;5;13m[V] Loaded Module: onnxruntime | Version: 1.8.1 | Path: ['/opt/conda/lib/python3.8/site-packages/onnxruntime'][0m
	[38;5;14m[I] Creating ONNX-Runtime Inference Session with providers: ['CPUExecutionProvider'][0m
	[38;5;13m[V] Loaded Module: numpy | Version: 1.21.4 | Path: ['/opt/conda/lib/python3.8/site-packages/numpy'][0m
	[38;5;13m[V] Loading inputs from data loader[0m
	[38;5;13m[V] Generating data using numpy seed: 1[0m
	[38;5;13m[V] Input tensor: s_cat__0 | Generating input data in range: [0, 1][0m
	[38;5;13m[V] Input tensor: k_cont__3 | Generating input data in range: [0.0, 1.0][0m
	[38;5;13m[V] Input tensor: target__6 | Generating input data in range: [0.0, 1.0][0m
	[I] Saving inference input data to /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_inputs.json
	[I] onnxrt-runner-N0-03/04/23-04:19:29
	    ---- Inference Input(s) ----
	    {s_cat__0 [dtype=int64, shape=(1, 192, 1)],
	     k_cont__3 [dtype=float32, shape=(1, 192, 3)],
	     target__6 [dtype=float32, shape=(1, 192, 1)]}
	[38;5;13m[V] Runner input metadata is: {s_cat__0 [dtype=int64, shape=('batch_size_0', 192, 1)],
	     k_cont__3 [dtype=float32, shape=('batch_size_0', 192, 3)],
	     target__6 [dtype=float32, shape=('batch_size_0', 192, 1)]}[0m
	[I] onnxrt-runner-N0-03/04/23-04:19:29
	    ---- Inference Output(s) ----
	    {target__0 [dtype=float32, shape=(1, 24, 3)]}
	[38;5;10m[I] onnxrt-runner-N0-03/04/23-04:19:29  | Completed 1 iteration(s) in 7.93 ms | Average inference time: 7.93 ms.[0m
	[38;5;14m[I] trt-runner-N0-03/04/23-04:19:29     | Activating and starting inference[0m
	[38;5;13m[V] [MemUsageChange] Init CUDA: CPU +721, GPU +0, now: CPU 745, GPU 1170 (MiB)[0m
	[38;5;13m[V] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 745 MiB, GPU 1170 MiB[0m
	[38;5;13m[V] [MemUsageSnapshot] End constructing builder kernel library: CPU 1013 MiB, GPU 1242 MiB[0m
	[38;5;13m[V] Starting shape inference[0m
	[38;5;13m[V] Writing shape-inferred model to: /tmp/tmp_polygraphy_6b660f5ffdf08593dab962b6949c26bbccef95ed0ad0822f.onnx[0m
	[I] Loading model: /tmp/tmp_polygraphy_6b660f5ffdf08593dab962b6949c26bbccef95ed0ad0822f.onnx
	[38;5;13m[V] Loading external data from: /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared[0m
	[38;5;13m[V] Shape inference completed successfully[0m
	[38;5;11m[W] parsers/onnx/onnx2trt_utils.cpp:364: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;11m[W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;11m[W] parsers/onnx/ShapedWeights.cpp:171: Weights 1007 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;11m[W] parsers/onnx/ShapedWeights.cpp:171: Weights 1045 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;11m[W] parsers/onnx/ShapedWeights.cpp:171: Weights 1080 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_529: broadcasting input1 to make tensors conform, dims(input0)=[-1,4,192,192][NONE] dims(input1)=[-1,1,192,32][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_529: broadcasting input1 to make tensors conform, dims(input0)=[-1,4,192,192][NONE] dims(input1)=[-1,1,192,32][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_529: broadcasting input1 to make tensors conform, dims(input0)=[-1,4,192,192][NONE] dims(input1)=[-1,1,192,32][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_529: broadcasting input1 to make tensors conform, dims(input0)=[-1,4,192,192][NONE] dims(input1)=[-1,1,192,32][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_529: broadcasting input1 to make tensors conform, dims(input0)=[-1,4,192,192][NONE] dims(input1)=[-1,1,192,32][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_529: broadcasting input1 to make tensors conform, dims(input0)=[-1,4,192,192][NONE] dims(input1)=[-1,1,192,32][NONE].[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_529: broadcasting input1 to make tensors conform, dims(input0)=[-1,4,192,192][NONE] dims(input1)=[-1,1,192,32][NONE].[0m
	[38;5;11m[W] Output type must be INT32 for shape outputs[0m
	[38;5;13m[V]     Setting TensorRT Optimization Profiles[0m
	[38;5;13m[V]     Input tensor: s_cat__0 (dtype=DataType.INT32, shape=(-1, 192, 1)) | Setting input tensor shapes to: (min=[1, 192, 1], opt=[1024, 192, 1], max=[1024, 192, 1])[0m
	[38;5;13m[V]     Input tensor: k_cont__3 (dtype=DataType.FLOAT, shape=(-1, 192, 3)) | Setting input tensor shapes to: (min=[1, 192, 3], opt=[1024, 192, 3], max=[1024, 192, 3])[0m
	[38;5;13m[V]     Input tensor: target__6 (dtype=DataType.FLOAT, shape=(-1, 192, 1)) | Setting input tensor shapes to: (min=[1, 192, 1], opt=[1024, 192, 1], max=[1024, 192, 1])[0m
	[I]     Configuring with profiles: [Profile().add('s_cat__0', min=[1, 192, 1], opt=[1024, 192, 1], max=[1024, 192, 1]).add('k_cont__3', min=[1, 192, 3], opt=[1024, 192, 3], max=[1024, 192, 3]).add('target__6', min=[1, 192, 1], opt=[1024, 192, 1], max=[1024, 192, 1])]
	[38;5;14m[I] Building engine with configuration:
	    Flags                  | [FP16, TF32]
	    Engine Capability      | EngineCapability.DEFAULT
	    Tactic Sources         | [CUBLAS, CUBLAS_LT, CUDNN]
	    DLA                    | Default Device Type: DeviceType.GPU, Core: 0
	    Profiling Verbosity    | ProfilingVerbosity.DETAILED[0m
	[38;5;13m[V] MatMul_16: broadcasting input1 to make tensors conform, dims(input0)=[-1,192,1,1,1][NONE] dims(input1)=[1,1,1,1,128][NONE].[0m
	[38;5;13m[V] MatMul_529: broadcasting input1 to make tensors conform, dims(input0)=[-1,4,192,192][NONE] dims(input1)=[-1,1,192,32][NONE].[0m
	[38;5;13m[V] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1239, GPU +352, now: CPU 2264, GPU 1594 (MiB)[0m
	[38;5;13m[V] [MemUsageChange] Init cuDNN: CPU +253, GPU +58, now: CPU 2517, GPU 1652 (MiB)[0m
	[38;5;13m[V] Global timing cache in use. Profiling results in this builder pass will be stored.[0m
	[38;5;11m[W] Weights [name=model.attention._mask + (Unnamed Layer* 1358) [Shuffle]]: Converted FP32 value in weights (either FP32 infinity or FP32 value outside FP16 range) to corresponding FP16 infinity. If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.[0m
	[38;5;13m[V] Detected 3 inputs and 1 output network tensors.[0m
	[38;5;13m[V] Total Host Persistent Memory: 304[0m
	[38;5;13m[V] Total Device Persistent Memory: 0[0m
	[38;5;13m[V] Total Scratch Memory: 1248067584[0m
	[38;5;13m[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 3 MiB, GPU 2401 MiB[0m
	[38;5;13m[V] [BlockAssignment] Algorithm ShiftNTopDown took 0.014053ms to assign 5 blocks to 5 nodes requiring 1249788416 bytes.[0m
	[38;5;13m[V] Total Activation Memory: 1249788416[0m
	[38;5;13m[V] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2541, GPU 1696 (MiB)[0m
	[38;5;13m[V] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2541, GPU 1704 (MiB)[0m
	[38;5;13m[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +7, now: CPU 0, GPU 7 (MiB)[0m
	[38;5;10m[I] Finished engine building in 816.072 seconds[0m
	[38;5;13m[V] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2546, GPU 1662 (MiB)[0m
	[38;5;13m[V] Loaded engine size: 28 MiB[0m
	[38;5;13m[V] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2568, GPU 1680 (MiB)[0m
	[38;5;13m[V] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2568, GPU 1688 (MiB)[0m
	[38;5;13m[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +6, now: CPU 0, GPU 6 (MiB)[0m
	[I] Saving engine to /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan
	[38;5;13m[V] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2539, GPU 1680 (MiB)[0m
	[38;5;13m[V] [MemUsageChange] Init cuDNN: CPU +1, GPU +8, now: CPU 2540, GPU 1688 (MiB)[0m
	[38;5;13m[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +1192, now: CPU 0, GPU 1198 (MiB)[0m
	[38;5;13m[V] Found candidate CUDA libraries: ['/usr/local/cuda/lib64/libcudart.so.11.5.50', '/usr/local/cuda/lib64/libcudart.so', '/usr/local/cuda/lib64/libcudart.so.11.0'][0m
	[38;5;11m[W] Input tensor: s_cat__0 | Buffer dtype (int64) does not match expected input dtype (int32), attempting to cast. [0m
	[I] trt-runner-N0-03/04/23-04:19:29
	    ---- Inference Input(s) ----
	    {s_cat__0 [dtype=int32, shape=(1, 192, 1)],
	     k_cont__3 [dtype=float32, shape=(1, 192, 3)],
	     target__6 [dtype=float32, shape=(1, 192, 1)]}
	[38;5;13m[V] Runner input metadata is: {s_cat__0 [dtype=int32, shape=(-1, 192, 1)],
	     k_cont__3 [dtype=float32, shape=(-1, 192, 3)],
	     target__6 [dtype=float32, shape=(-1, 192, 1)]}[0m
	[38;5;13m[V] Setting binding: s_cat__0 (index: 0) to shape: (1, 192, 1)[0m
	[38;5;13m[V] Setting binding: k_cont__3 (index: 1) to shape: (1, 192, 3)[0m
	[38;5;13m[V] Setting binding: target__6 (index: 2) to shape: (1, 192, 1)[0m
	[I] trt-runner-N0-03/04/23-04:19:29
	    ---- Inference Output(s) ----
	    {target__0 [dtype=float32, shape=(1, 24, 3)]}
	[38;5;10m[I] trt-runner-N0-03/04/23-04:19:29     | Completed 1 iteration(s) in 3.118 ms | Average inference time: 3.118 ms.[0m
	[38;5;13m[V] Successfully ran: ['onnxrt-runner-N0-03/04/23-04:19:29', 'trt-runner-N0-03/04/23-04:19:29'][0m
	[I] Saving inference results to /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_outputs.json
	[38;5;14m[I] Accuracy Comparison | onnxrt-runner-N0-03/04/23-04:19:29 vs. trt-runner-N0-03/04/23-04:19:29[0m
	[38;5;14m[I]     Comparing Output: 'target__0' (dtype=float32, shape=(1, 24, 3)) with 'target__0' (dtype=float32, shape=(1, 24, 3))[0m
	[I]         Tolerance: [abs=100, rel=100] | Checking elemwise error
	[I]         onnxrt-runner-N0-03/04/23-04:19:29: target__0 | Stats: mean=-0.74923, std-dev=0.51102, var=0.26115, median=-0.80455, min=-1.4737 at (0, 19, 0), max=0.38399 at (0, 1, 2), avg-magnitude=0.76887
	[38;5;13m[V]             ---- Histogram ----
	                Bin Range        |  Num Elems | Visualization
	                (-1.47 , -1.29 ) |         16 | ########################################
	                (-1.29 , -1.1  ) |          6 | ###############
	                (-1.1  , -0.916) |         10 | #########################
	                (-0.916, -0.73 ) |          7 | #################
	                (-0.73 , -0.544) |          3 | #######
	                (-0.544, -0.358) |         10 | #########################
	                (-0.358, -0.172) |          9 | ######################
	                (-0.172, 0.0135) |          8 | ####################
	                (0.0135, 0.199 ) |          1 | ##
	                (0.199 , 0.385 ) |          2 | #####[0m
	[I]         trt-runner-N0-03/04/23-04:19:29: target__0 | Stats: mean=-0.74882, std-dev=0.51129, var=0.26142, median=-0.80322, min=-1.4736 at (0, 19, 0), max=0.38525 at (0, 1, 2), avg-magnitude=0.76847
	[38;5;13m[V]             ---- Histogram ----
	                Bin Range        |  Num Elems | Visualization
	                (-1.47 , -1.29 ) |         16 | ########################################
	                (-1.29 , -1.1  ) |          6 | ###############
	                (-1.1  , -0.916) |         10 | #########################
	                (-0.916, -0.73 ) |          7 | #################
	                (-0.73 , -0.544) |          3 | #######
	                (-0.544, -0.358) |         10 | #########################
	                (-0.358, -0.172) |          9 | ######################
	                (-0.172, 0.0135) |          8 | ####################
	                (0.0135, 0.199 ) |          1 | ##
	                (0.199 , 0.385 ) |          2 | #####[0m
	[I]         Error Metrics: target__0
	[I]             Minimum Required Tolerance: elemwise error | [abs=0.0024977] OR [rel=7.3164] (requirements may be lower if both abs/rel tolerances are set)
	[I]             Absolute Difference | Stats: mean=0.00079476, std-dev=0.00060874, var=3.7057e-07, median=0.00068323, min=1.8358e-05 at (0, 17, 1), max=0.0024977 at (0, 6, 2), avg-magnitude=0.00079476
	[38;5;13m[V]                 ---- Histogram ----
	                    Bin Range            |  Num Elems | Visualization
	                    (1.84e-05, 0.000266) |         15 | ########################################
	                    (0.000266, 0.000514) |         12 | ################################
	                    (0.000514, 0.000762) |         11 | #############################
	                    (0.000762, 0.00101 ) |         12 | ################################
	                    (0.00101 , 0.00126 ) |          7 | ##################
	                    (0.00126 , 0.00151 ) |          6 | ################
	                    (0.00151 , 0.00175 ) |          4 | ##########
	                    (0.00175 , 0.002   ) |          1 | ##
	                    (0.002   , 0.00225 ) |          1 | ##
	                    (0.00225 , 0.0025  ) |          3 | ########[0m
	[I]             Relative Difference | Stats: mean=0.11069, std-dev=0.85618, var=0.73304, median=0.00081672, min=1.683e-05 at (0, 17, 1), max=7.3164 at (0, 1, 1), avg-magnitude=0.11069
	[38;5;13m[V]                 ---- Histogram ----
	                    Bin Range         |  Num Elems | Visualization
	                    (1.68e-05, 0.732) |         71 | ########################################
	                    (0.732   , 1.46 ) |          0 |
	                    (1.46    , 2.19 ) |          0 |
	                    (2.19    , 2.93 ) |          0 |
	                    (2.93    , 3.66 ) |          0 |
	                    (3.66    , 4.39 ) |          0 |
	                    (4.39    , 5.12 ) |          0 |
	                    (5.12    , 5.85 ) |          0 |
	                    (5.85    , 6.58 ) |          0 |
	                    (6.58    , 7.32 ) |          1 | [0m
	[38;5;10m[I]         PASSED | Output: 'target__0' | Difference is within tolerance (rel=100.0, abs=100.0)[0m
	[38;5;10m[I]     PASSED | All outputs matched | Outputs: ['target__0'][0m
	[38;5;10m[I] Accuracy Summary | onnxrt-runner-N0-03/04/23-04:19:29 vs. trt-runner-N0-03/04/23-04:19:29 | Passed: 1/1 iterations | Pass Rate: 100.0%[0m
	[38;5;14m[I] Output Validation | Runners: ['onnxrt-runner-N0-03/04/23-04:19:29', 'trt-runner-N0-03/04/23-04:19:29'][0m
	[38;5;14m[I]     onnxrt-runner-N0-03/04/23-04:19:29  | Validating output: target__0 (check_inf=True, check_nan=True)[0m
	[I]         mean=-0.74923, std-dev=0.51102, var=0.26115, median=-0.80455, min=-1.4737 at (0, 19, 0), max=0.38399 at (0, 1, 2), avg-magnitude=0.76887
	[38;5;13m[V]             ---- Histogram ----
	                Bin Range        |  Num Elems | Visualization
	                (-1.47 , -1.29 ) |         16 | ########################################
	                (-1.29 , -1.1  ) |          6 | ###############
	                (-1.1  , -0.916) |         10 | #########################
	                (-0.916, -0.731) |          7 | #################
	                (-0.731, -0.545) |          3 | #######
	                (-0.545, -0.359) |         10 | #########################
	                (-0.359, -0.173) |          9 | ######################
	                (-0.173, 0.0125) |          8 | ####################
	                (0.0125, 0.198 ) |          1 | ##
	                (0.198 , 0.384 ) |          2 | #####[0m
	[38;5;10m[I]         PASSED | Output: target__0 is valid[0m
	[38;5;14m[I]     trt-runner-N0-03/04/23-04:19:29     | Validating output: target__0 (check_inf=True, check_nan=True)[0m
	[I]         mean=-0.74882, std-dev=0.51129, var=0.26142, median=-0.80322, min=-1.4736 at (0, 19, 0), max=0.38525 at (0, 1, 2), avg-magnitude=0.76847
	[38;5;13m[V]             ---- Histogram ----
	                Bin Range        |  Num Elems | Visualization
	                (-1.47 , -1.29 ) |         16 | ########################################
	                (-1.29 , -1.1  ) |          6 | ###############
	                (-1.1  , -0.916) |         10 | #########################
	                (-0.916, -0.73 ) |          7 | #################
	                (-0.73 , -0.544) |          3 | #######
	                (-0.544, -0.358) |         10 | #########################
	                (-0.358, -0.172) |          9 | ######################
	                (-0.172, 0.0135) |          8 | ####################
	                (0.0135, 0.199 ) |          1 | ##
	                (0.199 , 0.385 ) |          2 | #####[0m
	[38;5;10m[I]         PASSED | Output: target__0 is valid[0m
	[38;5;10m[I]     PASSED | Output Validation[0m
	[38;5;10m[I] PASSED | Runtime: 821.378s | Command: /opt/conda/bin/polygraphy run --onnxrt --trt /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx --model-type onnx --inputs s_cat__0:[1,192,1] k_cont__3:[1,192,3] target__6:[1,192,1] --onnx-outputs target__0 --shape-inference --tf32 --fp16 --trt-min-shapes s_cat__0:[1,192,1] k_cont__3:[1,192,3] target__6:[1,192,1] --trt-opt-shapes s_cat__0:[1024,192,1] k_cont__3:[1024,192,3] target__6:[1024,192,1] --trt-max-shapes s_cat__0:[1024,192,1] k_cont__3:[1024,192,3] target__6:[1024,192,1] --save-inputs /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_inputs.json --save-outputs /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_outputs.json --validate --rtol target__0:100.0 1e-05 --atol target__0:100.0 1e-05 --workspace 10000000000 --save-engine /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan -v[0m
2023-03-04 04:33:12 - INFO - polygraphy.transformers: Polygraphy onnx2trt succeed.
2023-03-04 04:33:12 - DEBUG - model_navigator.results: Saving results of convert_model stage into /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/convert_model_results.yaml
2023-03-04 04:33:12 - DEBUG - model_navigator.results: ConversionResult(status=Status(state=<State.SUCCEEDED: 'succeeded'>, message='ONNX converted to TensorRT', log_path='/home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.log'), source_model_config=ModelConfig(model_name='TFT', model_path=PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.onnx'), model_format=None, model_version='1'), conversion_config=ConversionConfig(target_format=<Format.TENSORRT: 'trt'>, onnx_opset=13, tensorrt_precision=<TensorRTPrecision.FP16: 'fp16'>, tensorrt_precision_mode=<TensorRTPrecisionMode.HIERARCHY: 'hierarchy'>, tensorrt_explicit_precision=False, tensorrt_strict_types=False, tensorrt_sparse_weights=False), tensorrt_common_config=None, model_signature_config=None, comparator_config=ComparatorConfig(atol={'target__0': 100.0, '': 1e-05}, rtol={'target__0': 100.0, '': 1e-05}, max_batch_size=1024), dataset_profile=DatasetProfileConfig(min_shapes=None, opt_shapes=None, max_shapes=None, value_ranges=None, dtypes=None), output_model=Model(name='TFT.polygraphyonnx2trt_fp16_mh', path=PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan'), format=<Format.TENSORRT: 'trt'>, signature=None, properties=TensorRTProperties(), num_required_gpus=None), framework_docker_image=None)
2023-03-04 04:33:12 - DEBUG - convert: Copy /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan to /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model
2023-03-04 04:33:12 - DEBUG - convert: Copy /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_inputs.json to /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model.json
2023-03-04 04:33:12 - DEBUG - convert: Copy /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.comparator_outputs.json to /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model.json
2023-03-04 04:33:12 - DEBUG - convert: Copy /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/converted/exported_model-polygraphyonnx2trt_fp16_mh.plan.log to /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model.log
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed 's/^$/unknown/'
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ [[ trt == \t\s\-\t\r\a\c\e ]]
+ [[ trt == \t\s\-\s\c\r\i\p\t ]]
+ export CONFIG_FORMAT=trt
+ CONFIG_FORMAT=trt
+ model-navigator triton-config-model --model-repository /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models --model-name TFT --model-version 1 --model-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model --model-format trt --model-control-mode explicit --load-model --load-model-timeout-s 100 --verbose --backend-accelerator none --tensorrt-precision fp16 --tensorrt-capture-cuda-graph --tensorrt-max-workspace-size 10000000000 --max-batch-size 1024 --batching dynamic --preferred-batch-sizes 512 1024 --max-queue-delay-us 1 --engine-count-per-device gpu=2
2023-03-04 04:33:12 - DEBUG - model_navigator.log: initialized logger verbose=True
2023-03-04 04:33:12 - DEBUG - triton_config_model: Running 'model-navigator triton-config-model' with config_path: None
2023-03-04 04:33:12 - INFO - model_navigator.log: Environment state
2023-03-04 04:33:12 - INFO - model_navigator.log: 	docker_container_id = 98c4b3fc07
2023-03-04 04:33:12 - INFO - model_navigator.log: 	docker_container_ip_address = None
2023-03-04 04:33:12 - INFO - model_navigator.log: 	docker_container_mounts = [{'host_path': PosixPath('/var/run/docker.sock'), 'container_path': PosixPath('/var/run/docker.sock'), 'mount_type': 'bind'}, {'host_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/dataset/processed'), 'container_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/datasets'), 'mount_type': 'bind'}, {'host_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT'), 'container_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT'), 'mount_type': 'bind'}]
2023-03-04 04:33:12 - DEBUG - model_navigator.validators: Running command validator: <model_navigator.validators.ModelNavigatorBatchingConfiguration object at 0x7fda430c4a90>
2023-03-04 04:33:12 - DEBUG - model_navigator.validators: ModelNavigatorBatchingConfiguration 
batching=Batching.DYNAMIC 
max_batch_size=1024 
2023-03-04 04:33:12 - INFO - model_navigator.log: triton-config-model args:
2023-03-04 04:33:12 - INFO - model_navigator.log: 	model_name = TFT
2023-03-04 04:33:12 - INFO - model_navigator.log: 	model_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model
2023-03-04 04:33:12 - INFO - model_navigator.log: 	model_format = Format.TENSORRT
2023-03-04 04:33:12 - INFO - model_navigator.log: 	model_version = 1
2023-03-04 04:33:12 - INFO - model_navigator.log: 	model_repository = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models
2023-03-04 04:33:12 - INFO - model_navigator.log: 	load_model = True
2023-03-04 04:33:12 - INFO - model_navigator.log: 	load_model_timeout_s = 100
2023-03-04 04:33:12 - INFO - model_navigator.log: 	model_control_mode = ModelControlMode.EXPLICIT
2023-03-04 04:33:12 - INFO - model_navigator.log: 	inputs = None
2023-03-04 04:33:12 - INFO - model_navigator.log: 	outputs = None
2023-03-04 04:33:12 - INFO - model_navigator.log: 	max_batch_size = 1024
2023-03-04 04:33:12 - INFO - model_navigator.log: 	batching = Batching.DYNAMIC
2023-03-04 04:33:12 - INFO - model_navigator.log: 	backend_accelerator = BackendAccelerator.NONE
2023-03-04 04:33:12 - INFO - model_navigator.log: 	tensorrt_precision = TensorRTOptPrecision.FP16
2023-03-04 04:33:12 - INFO - model_navigator.log: 	tensorrt_capture_cuda_graph = True
2023-03-04 04:33:12 - INFO - model_navigator.log: 	tensorrt_max_workspace_size = 10000000000
2023-03-04 04:33:12 - INFO - model_navigator.log: 	preferred_batch_sizes = [512, 1024]
2023-03-04 04:33:12 - INFO - model_navigator.log: 	max_queue_delay_us = 1
2023-03-04 04:33:12 - INFO - model_navigator.log: 	engine_count_per_device = {<DeviceKind.GPU: 'gpu'>: 2}
2023-03-04 04:33:12 - INFO - model_navigator.log: 	triton_backend_parameters = {}
2023-03-04 04:33:12 - INFO - model_navigator.log: 	server_url = grpc://localhost:8001
2023-03-04 04:33:12 - INFO - model_navigator.log: model:
2023-03-04 04:33:12 - INFO - model_navigator.log: 	name = TFT
2023-03-04 04:33:12 - INFO - model_navigator.log: 	path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model
2023-03-04 04:33:12 - INFO - model_navigator.log: 	format = Format.TENSORRT
2023-03-04 04:33:12 - INFO - model_navigator.log: 	signature = None
2023-03-04 04:33:12 - INFO - model_navigator.log: 	properties = {}
2023-03-04 04:33:12 - INFO - model_navigator.log: 	num_required_gpus = None
2023-03-04 04:33:12 - DEBUG - model_navigator.triton.model_store: Deploying model /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model in Triton Model Store /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models with: 
batching config: TritonBatchingConfig(max_batch_size=1024, batching=<Batching.DYNAMIC: 'dynamic'>)
optimization config: TritonModelOptimizationConfig(backend_accelerator=<BackendAccelerator.NONE: 'none'>, tensorrt_precision=<TensorRTOptPrecision.FP16: 'fp16'>, tensorrt_capture_cuda_graph=True)
dynamic batching config: TritonDynamicBatchingConfig(preferred_batch_sizes=[512, 1024], max_queue_delay_us=1)
2023-03-04 04:33:12 - DEBUG - model_navigator.triton.model_store: Copying /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model to /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models/TFT/1/model.plan
2023-03-04 04:33:12 - DEBUG - model_navigator.triton.model_config: Generated Triton config:
name: "TFT"
platform: "tensorrt_plan"
max_batch_size: 1024
instance_group {
  count: 2
  kind: KIND_GPU
}
dynamic_batching {
  preferred_batch_size: 512
  preferred_batch_size: 1024
  max_queue_delay_microseconds: 1
}
optimization {
  cuda {
    graphs: true
  }
}

2023-03-04 04:33:12 - DEBUG - triton_config_model: Loading model TFT:1 model_control_mode: ModelControlMode.EXPLICIT
2023-03-04 04:33:12 - DEBUG - model_navigator.triton.client: Connecting to grpc://localhost:8001
2023-03-04 04:33:12 - DEBUG - triton_config_model: Waiting for server (timeout=5s)
is_server_ready, metadata ()

ready: true

is_server_live, metadata ()

live: true

2023-03-04 04:33:12 - DEBUG - triton_config_model: Sending load_model request
load_model, metadata ()
override files omitted:
model_name: "TFT"

Loaded model 'TFT'
2023-03-04 04:33:19 - DEBUG - triton_config_model: Polling for model availability (timeout=100s)
get_model_repository_index, metadata ()

models {
  name: "TFT"
  version: "1"
  state: "READY"
}

is_model_ready, metadata ()
name: "TFT"

ready: true

get_model_metadata, metadata ()
name: "TFT"

name: "TFT"
versions: "1"
platform: "tensorrt_plan"
inputs {
  name: "s_cat__0"
  datatype: "INT32"
  shape: -1
  shape: 192
  shape: 1
}
inputs {
  name: "k_cont__3"
  datatype: "FP32"
  shape: -1
  shape: 192
  shape: 3
}
inputs {
  name: "target__6"
  datatype: "FP32"
  shape: -1
  shape: 192
  shape: 1
}
outputs {
  name: "target__0"
  datatype: "FP32"
  shape: -1
  shape: 24
  shape: 3
}

2023-03-04 04:33:19 - DEBUG - model_navigator.utils.workspace: Workspace path /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:33:19 - DEBUG - model_navigator.results: Saving results of triton_config_model stage into /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/triton_config_model_results.yaml
2023-03-04 04:33:19 - DEBUG - model_navigator.results: ConfigModelResult(status=Status(state=<State.SUCCEEDED: 'succeeded'>, message='Model configured and loaded correctly', log_path=None), model_config=ModelConfig(model_name='TFT', model_path=PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model'), model_format=<Format.TENSORRT: 'trt'>, model_version='1'), model_version='1', batching_config=TritonBatchingConfig(max_batch_size=1024, batching=<Batching.DYNAMIC: 'dynamic'>), optimization_config=TritonModelOptimizationConfig(backend_accelerator=<BackendAccelerator.NONE: 'none'>, tensorrt_precision=<TensorRTOptPrecision.FP16: 'fp16'>, tensorrt_capture_cuda_graph=True), dynamic_batching_config=TritonDynamicBatchingConfig(preferred_batch_sizes=[512, 1024], max_queue_delay_us=1), instances_config=TritonModelInstancesConfig(engine_count_per_device={<DeviceKind.GPU: 'gpu'>: 2}), tensorrt_common_config=TensorRTCommonConfig(tensorrt_max_workspace_size=10000000000), model_dir_in_model_store=PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models/TFT'))
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed 's/^$/unknown/'
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ mkdir -p /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data
+ python triton/prepare_input_data.py --input-data-dir /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/ --dataset datasets/traffic_bin --checkpoint /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/traffic_bin/
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed 's/^$/unknown/'
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ python triton/run_performance_on_triton.py --model-repository /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models --model-name TFT --input-data /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/data.json --batch-sizes 1,2,4,8,16,32,64,128,256,512,1024 --number-of-triton-instances 1 --batching-mode static --evaluation-mode offline --measurement-request-count 500 --warmup --performance-tool perf_analyzer --result-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/triton_performance_offline.csv
2023-03-04 04:33:23,343 INFO run_performance_on_triton Running warmup before the main test
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 1
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 1000
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 1296
    Throughput: 648 infer/sec
    Avg latency: 1535 usec (standard deviation 136 usec)
    p50 latency: 1604 usec
    p90 latency: 1662 usec
    p95 latency: 1667 usec
    p99 latency: 1681 usec
    Avg HTTP time: 1531 usec (send/recv 40 usec + response wait 1491 usec)
  Server:
    Inference count: 1297
    Execution count: 1297
    Successful request count: 1297
    Avg request latency: 1354 usec (overhead 17 usec + queue 26 usec + compute input 93 usec + compute infer 1207 usec + compute output 11 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 648 infer/sec, latency 1535 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 1024
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 1000
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 1010
    Throughput: 51706.8 infer/sec
    Avg latency: 19779 usec (standard deviation 226 usec)
    p50 latency: 19768 usec
    p90 latency: 20054 usec
    p95 latency: 20131 usec
    p99 latency: 20304 usec
    Avg HTTP time: 19768 usec (send/recv 99 usec + response wait 19669 usec)
  Server:
    Inference count: 1035264
    Execution count: 1011
    Successful request count: 1011
    Avg request latency: 19467 usec (overhead 45 usec + queue 123 usec + compute input 450 usec + compute infer 18765 usec + compute output 84 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 51706.8 infer/sec, latency 19779 usec
2023-03-04 04:34:29,389 INFO run_performance_on_triton Using Perf Analyzer for performance evaluation
2023-03-04 04:34:29,389 INFO run_performance_on_triton Selected configuration
2023-03-04 04:34:29,389 INFO run_performance_on_triton 	server_url = http://127.0.0.1:8000
2023-03-04 04:34:29,389 INFO run_performance_on_triton 	model_name = TFT
2023-03-04 04:34:29,389 INFO run_performance_on_triton 	input_data = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/data.json
2023-03-04 04:34:29,389 INFO run_performance_on_triton 	input_shapes = []
2023-03-04 04:34:29,389 INFO run_performance_on_triton 	batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
2023-03-04 04:34:29,389 INFO run_performance_on_triton 	number_of_triton_instances = 1
2023-03-04 04:34:29,389 INFO run_performance_on_triton 	number_of_model_instances = 1
2023-03-04 04:34:29,389 INFO run_performance_on_triton 	measurement_mode = MeasurementMode.COUNT_WINDOWS
2023-03-04 04:34:29,389 INFO run_performance_on_triton 	measurement_interval = 5000
2023-03-04 04:34:29,389 INFO run_performance_on_triton 	measurement_request_count = 500
2023-03-04 04:34:29,389 INFO run_performance_on_triton 	concurrency_steps = 32
2023-03-04 04:34:29,389 INFO run_performance_on_triton 	batching_mode = BatchingMode.STATIC
2023-03-04 04:34:29,389 INFO run_performance_on_triton 	evaluation_mode = EvaluationMode.OFFLINE
2023-03-04 04:34:29,389 INFO run_performance_on_triton 	offline_mode = OfflineMode.SYSTEM
2023-03-04 04:34:29,389 INFO run_performance_on_triton 	output_shared_memory_size = 100240
2023-03-04 04:34:29,389 INFO run_performance_on_triton 	result_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/triton_performance_offline.csv
2023-03-04 04:34:29,389 INFO run_performance_on_triton 	verbose = False
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 1
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 652
    Throughput: 652 infer/sec
    Avg latency: 1525 usec (standard deviation 139 usec)
    p50 latency: 1604 usec
    p90 latency: 1662 usec
    p95 latency: 1667 usec
    p99 latency: 1676 usec
    Avg HTTP time: 1522 usec (send/recv 39 usec + response wait 1483 usec)
  Server:
    Inference count: 652
    Execution count: 652
    Successful request count: 652
    Avg request latency: 1349 usec (overhead 16 usec + queue 25 usec + compute input 90 usec + compute infer 1207 usec + compute output 11 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 652 infer/sec, latency 1525 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 2
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 966
    Throughput: 966 infer/sec
    Avg latency: 2066 usec (standard deviation 49 usec)
    p50 latency: 2071 usec
    p90 latency: 2089 usec
    p95 latency: 2094 usec
    p99 latency: 2198 usec
    Avg HTTP time: 2065 usec (send/recv 14 usec + response wait 2051 usec)
  Server:
    Inference count: 1934
    Execution count: 967
    Successful request count: 967
    Avg request latency: 2002 usec (overhead 7 usec + queue 12 usec + compute input 40 usec + compute infer 1931 usec + compute output 12 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 966 infer/sec, latency 2066 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 4
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 845
    Throughput: 1689.16 infer/sec
    Avg latency: 2362 usec (standard deviation 39 usec)
    p50 latency: 2353 usec
    p90 latency: 2418 usec
    p95 latency: 2423 usec
    p99 latency: 2429 usec
    Avg HTTP time: 2361 usec (send/recv 13 usec + response wait 2348 usec)
  Server:
    Inference count: 3384
    Execution count: 846
    Successful request count: 846
    Avg request latency: 2299 usec (overhead 10 usec + queue 12 usec + compute input 43 usec + compute infer 2220 usec + compute output 14 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 1689.16 infer/sec, latency 2362 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 8
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 807
    Throughput: 3226.39 infer/sec
    Avg latency: 2475 usec (standard deviation 12 usec)
    p50 latency: 2475 usec
    p90 latency: 2490 usec
    p95 latency: 2494 usec
    p99 latency: 2499 usec
    Avg HTTP time: 2474 usec (send/recv 14 usec + response wait 2460 usec)
  Server:
    Inference count: 6456
    Execution count: 807
    Successful request count: 807
    Avg request latency: 2411 usec (overhead 6 usec + queue 12 usec + compute input 46 usec + compute infer 2336 usec + compute output 11 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 3226.39 infer/sec, latency 2475 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 766
    Throughput: 6128 infer/sec
    Avg latency: 2608 usec (standard deviation 20 usec)
    p50 latency: 2607 usec
    p90 latency: 2635 usec
    p95 latency: 2642 usec
    p99 latency: 2651 usec
    Avg HTTP time: 2606 usec (send/recv 13 usec + response wait 2593 usec)
  Server:
    Inference count: 12272
    Execution count: 767
    Successful request count: 767
    Avg request latency: 2544 usec (overhead 6 usec + queue 12 usec + compute input 49 usec + compute infer 2465 usec + compute output 12 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 6128 infer/sec, latency 2608 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 32
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 699
    Throughput: 11184 infer/sec
    Avg latency: 2856 usec (standard deviation 22 usec)
    p50 latency: 2856 usec
    p90 latency: 2886 usec
    p95 latency: 2895 usec
    p99 latency: 2911 usec
    Avg HTTP time: 2855 usec (send/recv 14 usec + response wait 2841 usec)
  Server:
    Inference count: 22400
    Execution count: 700
    Successful request count: 700
    Avg request latency: 2793 usec (overhead 7 usec + queue 12 usec + compute input 54 usec + compute infer 2707 usec + compute output 13 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 11184 infer/sec, latency 2856 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 64
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 604
    Throughput: 19328 infer/sec
    Avg latency: 3306 usec (standard deviation 35 usec)
    p50 latency: 3307 usec
    p90 latency: 3352 usec
    p95 latency: 3360 usec
    p99 latency: 3376 usec
    Avg HTTP time: 3305 usec (send/recv 13 usec + response wait 3292 usec)
  Server:
    Inference count: 38720
    Execution count: 605
    Successful request count: 605
    Avg request latency: 3243 usec (overhead 7 usec + queue 12 usec + compute input 58 usec + compute infer 3153 usec + compute output 13 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 19328 infer/sec, latency 3306 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 128
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 631
    Throughput: 26922.7 infer/sec
    Avg latency: 4745 usec (standard deviation 55 usec)
    p50 latency: 4721 usec
    p90 latency: 4843 usec
    p95 latency: 4851 usec
    p99 latency: 4869 usec
    Avg HTTP time: 4744 usec (send/recv 14 usec + response wait 4730 usec)
  Server:
    Inference count: 80896
    Execution count: 632
    Successful request count: 632
    Avg request latency: 4681 usec (overhead 10 usec + queue 12 usec + compute input 83 usec + compute infer 4533 usec + compute output 43 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 26922.7 infer/sec, latency 4745 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 256
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 565
    Throughput: 36160 infer/sec
    Avg latency: 7077 usec (standard deviation 83 usec)
    p50 latency: 7035 usec
    p90 latency: 7175 usec
    p95 latency: 7186 usec
    p99 latency: 7204 usec
    Avg HTTP time: 7075 usec (send/recv 13 usec + response wait 7062 usec)
  Server:
    Inference count: 144640
    Execution count: 565
    Successful request count: 565
    Avg request latency: 7013 usec (overhead 13 usec + queue 13 usec + compute input 117 usec + compute infer 6795 usec + compute output 75 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 36160 infer/sec, latency 7077 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 512
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 545
    Throughput: 46498.9 infer/sec
    Avg latency: 10978 usec (standard deviation 241 usec)
    p50 latency: 10992 usec
    p90 latency: 11270 usec
    p95 latency: 11400 usec
    p99 latency: 11605 usec
    Avg HTTP time: 10968 usec (send/recv 93 usec + response wait 10875 usec)
  Server:
    Inference count: 279552
    Execution count: 546
    Successful request count: 546
    Avg request latency: 10558 usec (overhead 42 usec + queue 131 usec + compute input 378 usec + compute infer 9966 usec + compute output 41 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 46498.9 infer/sec, latency 10978 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 1024
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 505
    Throughput: 51706.8 infer/sec
    Avg latency: 19769 usec (standard deviation 222 usec)
    p50 latency: 19742 usec
    p90 latency: 20068 usec
    p95 latency: 20126 usec
    p99 latency: 20278 usec
    Avg HTTP time: 19758 usec (send/recv 97 usec + response wait 19661 usec)
  Server:
    Inference count: 517120
    Execution count: 505
    Successful request count: 505
    Avg request latency: 19463 usec (overhead 43 usec + queue 152 usec + compute input 442 usec + compute infer 18759 usec + compute output 67 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 51706.8 infer/sec, latency 19769 usec
  Batch    Concurrency    Inferences/Second    Client Send    Network+Server Send/Recv    Server Queue    Server Compute Input    Server Compute Infer    Server Compute Output    Client Recv    p50 latency    p90 latency    p95 latency    p99 latency    avg latency
-------  -------------  -------------------  -------------  --------------------------  --------------  ----------------------  ----------------------  -----------------------  -------------  -------------  -------------  -------------  -------------  -------------
      1              1               652                39                         150              25                      90                    1207                       11              0           1604           1662           1667           1676           1522
      2              1               966                14                          55              12                      40                    1931                       12              0           2071           2089           2094           2198           2064
      4              1              1689.16             13                          57              12                      43                    2220                       14              0           2353           2418           2423           2429           2359
      8              1              3226.39             14                          54              12                      46                    2336                       11              0           2475           2490           2494           2499           2473
     16              1              6128                13                          54              12                      49                    2465                       12              0           2607           2635           2642           2651           2605
     32              1             11184                14                          54              12                      54                    2707                       13              0           2856           2886           2895           2911           2854
     64              1             19328                13                          54              12                      58                    3153                       13              0           3307           3352           3360           3376           3303
    128              1             26922.7              14                          58              12                      83                    4533                       43              0           4721           4843           4851           4869           4743
    256              1             36160                13                          61              13                     117                    6795                       75              0           7035           7175           7186           7204           7074
    512              1             46498.9              93                         365             131                     378                    9966                       41              0          10992          11270          11400          11605          10974
   1024              1             51706.8              97                         249             152                     442                   18759                       67              0          19742          20068          20126          20278          19766
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ python triton/run_performance_on_triton.py --model-repository /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models --model-name TFT --input-data /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/data.json --batch-sizes 1,2,4,8,16,32,64,128,256,512,1024 --number-of-triton-instances 1 --number-of-model-instances 2 --batching-mode dynamic --evaluation-mode online --measurement-request-count 500 --warmup --performance-tool perf_analyzer --result-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/triton_performance_online.csv
2023-03-04 04:36:17,763 INFO run_performance_on_triton Running warmup before the main test
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 1000
  Latency limit: 0 msec
  Concurrency limit: 128 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 128
  Client:
    Request count: 2587
    Throughput: 41392 infer/sec
    Avg latency: 48298 usec (standard deviation 7796 usec)
    p50 latency: 49896 usec
    p90 latency: 54615 usec
    p95 latency: 55015 usec
    p99 latency: 57605 usec
    Avg HTTP time: 48300 usec (send/recv 77 usec + response wait 48223 usec)
  Server:
    Inference count: 42304
    Execution count: 61
    Successful request count: 61
    Avg request latency: 47152 usec (overhead 435 usec + queue 25624 usec + compute input 451 usec + compute infer 20550 usec + compute output 92 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 128, throughput: 41392 infer/sec, latency 48298 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 1000
  Latency limit: 0 msec
  Concurrency limit: 256 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 256
  Client:
    Request count: 2772
    Throughput: 44352 infer/sec
    Avg latency: 86493 usec (standard deviation 13354 usec)
    p50 latency: 88834 usec
    p90 latency: 102317 usec
    p95 latency: 104211 usec
    p99 latency: 117685 usec
    Avg HTTP time: 86628 usec (send/recv 1072 usec + response wait 85556 usec)
  Server:
    Inference count: 46096
    Execution count: 56
    Successful request count: 56
    Avg request latency: 84231 usec (overhead 869 usec + queue 56353 usec + compute input 788 usec + compute infer 26117 usec + compute output 104 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 256, throughput: 44352 infer/sec, latency 86493 usec
2023-03-04 04:36:23,924 INFO run_performance_on_triton Using Perf Analyzer for performance evaluation
2023-03-04 04:36:23,924 INFO run_performance_on_triton Selected configuration
2023-03-04 04:36:23,924 INFO run_performance_on_triton 	server_url = http://127.0.0.1:8000
2023-03-04 04:36:23,924 INFO run_performance_on_triton 	model_name = TFT
2023-03-04 04:36:23,924 INFO run_performance_on_triton 	input_data = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/data.json
2023-03-04 04:36:23,924 INFO run_performance_on_triton 	input_shapes = []
2023-03-04 04:36:23,924 INFO run_performance_on_triton 	batch_sizes = [16]
2023-03-04 04:36:23,924 INFO run_performance_on_triton 	number_of_triton_instances = 1
2023-03-04 04:36:23,924 INFO run_performance_on_triton 	number_of_model_instances = 2
2023-03-04 04:36:23,924 INFO run_performance_on_triton 	measurement_mode = MeasurementMode.COUNT_WINDOWS
2023-03-04 04:36:23,924 INFO run_performance_on_triton 	measurement_interval = 5000
2023-03-04 04:36:23,925 INFO run_performance_on_triton 	measurement_request_count = 500
2023-03-04 04:36:23,925 INFO run_performance_on_triton 	concurrency_steps = 32
2023-03-04 04:36:23,925 INFO run_performance_on_triton 	batching_mode = BatchingMode.DYNAMIC
2023-03-04 04:36:23,925 INFO run_performance_on_triton 	evaluation_mode = EvaluationMode.ONLINE
2023-03-04 04:36:23,925 INFO run_performance_on_triton 	offline_mode = OfflineMode.SYSTEM
2023-03-04 04:36:23,925 INFO run_performance_on_triton 	output_shared_memory_size = 100240
2023-03-04 04:36:23,925 INFO run_performance_on_triton 	result_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/triton_performance_online.csv
2023-03-04 04:36:23,925 INFO run_performance_on_triton 	verbose = False
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 8 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 8
  Client:
    Request count: 1232
    Throughput: 19692.3 infer/sec
    Avg latency: 6436 usec (standard deviation 25 usec)
    p50 latency: 6436 usec
    p90 latency: 6466 usec
    p95 latency: 6473 usec
    p99 latency: 6489 usec
    Avg HTTP time: 6434 usec (send/recv 16 usec + response wait 6418 usec)
  Server:
    Inference count: 19776
    Execution count: 309
    Successful request count: 309
    Avg request latency: 6336 usec (overhead 29 usec + queue 3077 usec + compute input 71 usec + compute infer 3140 usec + compute output 19 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 8, throughput: 19692.3 infer/sec, latency 6436 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 16 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 16
  Client:
    Request count: 1365
    Throughput: 21840 infer/sec
    Avg latency: 11610 usec (standard deviation 2424 usec)
    p50 latency: 12083 usec
    p90 latency: 14401 usec
    p95 latency: 14946 usec
    p99 latency: 16193 usec
    Avg HTTP time: 11604 usec (send/recv 34 usec + response wait 11570 usec)
  Server:
    Inference count: 22064
    Execution count: 214
    Successful request count: 214
    Avg request latency: 11319 usec (overhead 52 usec + queue 6407 usec + compute input 89 usec + compute infer 4697 usec + compute output 74 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 16, throughput: 21840 infer/sec, latency 11610 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 24 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 24
  Client:
    Request count: 1572
    Throughput: 25152 infer/sec
    Avg latency: 15038 usec (standard deviation 2880 usec)
    p50 latency: 16338 usec
    p90 latency: 18641 usec
    p95 latency: 18918 usec
    p99 latency: 19203 usec
    Avg HTTP time: 15037 usec (send/recv 35 usec + response wait 15002 usec)
  Server:
    Inference count: 25504
    Execution count: 176
    Successful request count: 176
    Avg request latency: 14729 usec (overhead 88 usec + queue 8491 usec + compute input 115 usec + compute infer 5913 usec + compute output 122 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 24, throughput: 25152 infer/sec, latency 15038 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 32 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 32
  Client:
    Request count: 1652
    Throughput: 26432 infer/sec
    Avg latency: 19140 usec (standard deviation 3983 usec)
    p50 latency: 19306 usec
    p90 latency: 23066 usec
    p95 latency: 24437 usec
    p99 latency: 27365 usec
    Avg HTTP time: 19113 usec (send/recv 39 usec + response wait 19074 usec)
  Server:
    Inference count: 26768
    Execution count: 142
    Successful request count: 142
    Avg request latency: 18722 usec (overhead 108 usec + queue 10576 usec + compute input 143 usec + compute infer 7718 usec + compute output 177 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 32, throughput: 26432 infer/sec, latency 19140 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 40 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 40
  Client:
    Request count: 1770
    Throughput: 28291.7 infer/sec
    Avg latency: 22391 usec (standard deviation 4357 usec)
    p50 latency: 23569 usec
    p90 latency: 28515 usec
    p95 latency: 28857 usec
    p99 latency: 29679 usec
    Avg HTTP time: 22381 usec (send/recv 38 usec + response wait 22343 usec)
  Server:
    Inference count: 28688
    Execution count: 123
    Successful request count: 123
    Avg request latency: 22019 usec (overhead 158 usec + queue 11908 usec + compute input 181 usec + compute infer 9615 usec + compute output 157 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 40, throughput: 28291.7 infer/sec, latency 22391 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 48 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 48
  Client:
    Request count: 1823
    Throughput: 29168 infer/sec
    Avg latency: 26192 usec (standard deviation 5199 usec)
    p50 latency: 26263 usec
    p90 latency: 31109 usec
    p95 latency: 31390 usec
    p99 latency: 35838 usec
    Avg HTTP time: 26164 usec (send/recv 41 usec + response wait 26123 usec)
  Server:
    Inference count: 29232
    Execution count: 104
    Successful request count: 104
    Avg request latency: 25689 usec (overhead 166 usec + queue 15006 usec + compute input 202 usec + compute infer 10251 usec + compute output 64 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 48, throughput: 29168 infer/sec, latency 26192 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 56 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 56
  Client:
    Request count: 1765
    Throughput: 28211.8 infer/sec
    Avg latency: 30899 usec (standard deviation 6295 usec)
    p50 latency: 31101 usec
    p90 latency: 37821 usec
    p95 latency: 38081 usec
    p99 latency: 38419 usec
    Avg HTTP time: 30853 usec (send/recv 43 usec + response wait 30810 usec)
  Server:
    Inference count: 28816
    Execution count: 91
    Successful request count: 91
    Avg request latency: 30138 usec (overhead 220 usec + queue 13958 usec + compute input 260 usec + compute infer 15623 usec + compute output 77 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 56, throughput: 28211.8 infer/sec, latency 30899 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 64 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 64
  Client:
    Request count: 1888
    Throughput: 30208 infer/sec
    Avg latency: 33022 usec (standard deviation 5706 usec)
    p50 latency: 36013 usec
    p90 latency: 37642 usec
    p95 latency: 37964 usec
    p99 latency: 38220 usec
    Avg HTTP time: 33002 usec (send/recv 43 usec + response wait 32959 usec)
  Server:
    Inference count: 30544
    Execution count: 86
    Successful request count: 86
    Avg request latency: 32087 usec (overhead 267 usec + queue 14451 usec + compute input 309 usec + compute infer 17003 usec + compute output 57 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 64, throughput: 30208 infer/sec, latency 33022 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 72 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 72
  Client:
    Request count: 1847
    Throughput: 29522.5 infer/sec
    Avg latency: 38752 usec (standard deviation 5291 usec)
    p50 latency: 40118 usec
    p90 latency: 40773 usec
    p95 latency: 42432 usec
    p99 latency: 45779 usec
    Avg HTTP time: 38879 usec (send/recv 47 usec + response wait 38832 usec)
  Server:
    Inference count: 30144
    Execution count: 74
    Successful request count: 74
    Avg request latency: 38206 usec (overhead 240 usec + queue 21570 usec + compute input 350 usec + compute infer 15986 usec + compute output 60 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 72, throughput: 29522.5 infer/sec, latency 38752 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 80 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 80
  Client:
    Request count: 2260
    Throughput: 36160 infer/sec
    Avg latency: 34498 usec (standard deviation 5087 usec)
    p50 latency: 35453 usec
    p90 latency: 40310 usec
    p95 latency: 40690 usec
    p99 latency: 44944 usec
    Avg HTTP time: 34454 usec (send/recv 44 usec + response wait 34410 usec)
  Server:
    Inference count: 37104
    Execution count: 83
    Successful request count: 83
    Avg request latency: 33510 usec (overhead 280 usec + queue 16452 usec + compute input 378 usec + compute infer 16287 usec + compute output 113 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 80, throughput: 36160 infer/sec, latency 34498 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 88 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 88
  Client:
    Request count: 2291
    Throughput: 36656 infer/sec
    Avg latency: 37245 usec (standard deviation 6532 usec)
    p50 latency: 40272 usec
    p90 latency: 45220 usec
    p95 latency: 45390 usec
    p99 latency: 45556 usec
    Avg HTTP time: 37419 usec (send/recv 54 usec + response wait 37365 usec)
  Server:
    Inference count: 37776
    Execution count: 78
    Successful request count: 78
    Avg request latency: 36447 usec (overhead 443 usec + queue 17938 usec + compute input 390 usec + compute infer 17582 usec + compute output 94 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 88, throughput: 36656 infer/sec, latency 37245 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 96 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 96
  Client:
    Request count: 1946
    Throughput: 31136 infer/sec
    Avg latency: 47927 usec (standard deviation 5093 usec)
    p50 latency: 49399 usec
    p90 latency: 49575 usec
    p95 latency: 49686 usec
    p99 latency: 49824 usec
    Avg HTTP time: 47954 usec (send/recv 42 usec + response wait 47912 usec)
  Server:
    Inference count: 31696
    Execution count: 60
    Successful request count: 60
    Avg request latency: 46800 usec (overhead 326 usec + queue 27488 usec + compute input 312 usec + compute infer 18617 usec + compute output 57 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 96, throughput: 31136 infer/sec, latency 47927 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 104 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 104
  Client:
    Request count: 2077
    Throughput: 33232 infer/sec
    Avg latency: 49098 usec (standard deviation 5634 usec)
    p50 latency: 49495 usec
    p90 latency: 54292 usec
    p95 latency: 54402 usec
    p99 latency: 54611 usec
    Avg HTTP time: 49067 usec (send/recv 44 usec + response wait 49023 usec)
  Server:
    Inference count: 33680
    Execution count: 59
    Successful request count: 59
    Avg request latency: 47838 usec (overhead 352 usec + queue 28026 usec + compute input 342 usec + compute infer 19057 usec + compute output 61 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 104, throughput: 33232 infer/sec, latency 49098 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 112 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 112
  Client:
    Request count: 2353
    Throughput: 37648 infer/sec
    Avg latency: 46781 usec (standard deviation 4872 usec)
    p50 latency: 47366 usec
    p90 latency: 49689 usec
    p95 latency: 49760 usec
    p99 latency: 49812 usec
    Avg HTTP time: 46765 usec (send/recv 46 usec + response wait 46719 usec)
  Server:
    Inference count: 38640
    Execution count: 63
    Successful request count: 63
    Avg request latency: 45236 usec (overhead 412 usec + queue 24460 usec + compute input 429 usec + compute infer 19859 usec + compute output 76 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 112, throughput: 37648 infer/sec, latency 46781 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 120 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 120
  Client:
    Request count: 2527
    Throughput: 40432 infer/sec
    Avg latency: 46080 usec (standard deviation 6364 usec)
    p50 latency: 48110 usec
    p90 latency: 50616 usec
    p95 latency: 50860 usec
    p99 latency: 51195 usec
    Avg HTTP time: 46050 usec (send/recv 48 usec + response wait 46002 usec)
  Server:
    Inference count: 41184
    Execution count: 81
    Successful request count: 81
    Avg request latency: 44419 usec (overhead 418 usec + queue 23331 usec + compute input 406 usec + compute infer 20207 usec + compute output 57 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 120, throughput: 40432 infer/sec, latency 46080 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 128 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 128
  Client:
    Request count: 1986
    Throughput: 31744.3 infer/sec
    Avg latency: 62422 usec (standard deviation 5316 usec)
    p50 latency: 63864 usec
    p90 latency: 64302 usec
    p95 latency: 64423 usec
    p99 latency: 64575 usec
    Avg HTTP time: 62443 usec (send/recv 71 usec + response wait 62372 usec)
  Server:
    Inference count: 32880
    Execution count: 47
    Successful request count: 47
    Avg request latency: 61257 usec (overhead 362 usec + queue 39518 usec + compute input 366 usec + compute infer 20924 usec + compute output 87 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 128, throughput: 31744.3 infer/sec, latency 62422 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 136 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 136
  Client:
    Request count: 2404
    Throughput: 38464 infer/sec
    Avg latency: 55138 usec (standard deviation 6965 usec)
    p50 latency: 54794 usec
    p90 latency: 63961 usec
    p95 latency: 64218 usec
    p99 latency: 64561 usec
    Avg HTTP time: 55078 usec (send/recv 58 usec + response wait 55020 usec)
  Server:
    Inference count: 39808
    Execution count: 55
    Successful request count: 55
    Avg request latency: 53739 usec (overhead 413 usec + queue 33313 usec + compute input 408 usec + compute infer 19498 usec + compute output 107 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 136, throughput: 38464 infer/sec, latency 55138 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 144 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 144
  Client:
    Request count: 2424
    Throughput: 38784 infer/sec
    Avg latency: 57411 usec (standard deviation 10145 usec)
    p50 latency: 62040 usec
    p90 latency: 66422 usec
    p95 latency: 66591 usec
    p99 latency: 77222 usec
    Avg HTTP time: 57597 usec (send/recv 159 usec + response wait 57438 usec)
  Server:
    Inference count: 40320
    Execution count: 60
    Successful request count: 60
    Avg request latency: 56674 usec (overhead 499 usec + queue 34251 usec + compute input 485 usec + compute infer 21334 usec + compute output 105 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 144, throughput: 38784 infer/sec, latency 57411 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 152 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 152
  Client:
    Request count: 2326
    Throughput: 37178.8 infer/sec
    Avg latency: 62924 usec (standard deviation 4943 usec)
    p50 latency: 64105 usec
    p90 latency: 64484 usec
    p95 latency: 64557 usec
    p99 latency: 65001 usec
    Avg HTTP time: 62912 usec (send/recv 160 usec + response wait 62752 usec)
  Server:
    Inference count: 39088
    Execution count: 47
    Successful request count: 47
    Avg request latency: 61406 usec (overhead 450 usec + queue 39493 usec + compute input 450 usec + compute infer 20930 usec + compute output 83 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 152, throughput: 37178.8 infer/sec, latency 62924 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 160 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 160
  Client:
    Request count: 2447
    Throughput: 39152 infer/sec
    Avg latency: 62872 usec (standard deviation 4803 usec)
    p50 latency: 64039 usec
    p90 latency: 64238 usec
    p95 latency: 64310 usec
    p99 latency: 64593 usec
    Avg HTTP time: 62861 usec (send/recv 178 usec + response wait 62683 usec)
  Server:
    Inference count: 40832
    Execution count: 47
    Successful request count: 47
    Avg request latency: 61296 usec (overhead 423 usec + queue 39471 usec + compute input 470 usec + compute infer 20857 usec + compute output 75 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 160, throughput: 39152 infer/sec, latency 62872 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 168 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 168
  Client:
    Request count: 2190
    Throughput: 35040 infer/sec
    Avg latency: 73961 usec (standard deviation 9613 usec)
    p50 latency: 75946 usec
    p90 latency: 85050 usec
    p95 latency: 85118 usec
    p99 latency: 85279 usec
    Avg HTTP time: 74008 usec (send/recv 264 usec + response wait 73744 usec)
  Server:
    Inference count: 37104
    Execution count: 54
    Successful request count: 54
    Avg request latency: 72707 usec (overhead 436 usec + queue 51163 usec + compute input 431 usec + compute infer 20587 usec + compute output 90 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 168, throughput: 35040 infer/sec, latency 73961 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 176 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 176
  Client:
    Request count: 2233
    Throughput: 35728 infer/sec
    Avg latency: 74076 usec (standard deviation 11237 usec)
    p50 latency: 75817 usec
    p90 latency: 85256 usec
    p95 latency: 85361 usec
    p99 latency: 85685 usec
    Avg HTTP time: 74172 usec (send/recv 272 usec + response wait 73900 usec)
  Server:
    Inference count: 37472
    Execution count: 52
    Successful request count: 52
    Avg request latency: 72999 usec (overhead 454 usec + queue 51037 usec + compute input 454 usec + compute infer 20966 usec + compute output 88 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 176, throughput: 35728 infer/sec, latency 74076 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 184 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 184
  Client:
    Request count: 2355
    Throughput: 37680 infer/sec
    Avg latency: 75133 usec (standard deviation 9236 usec)
    p50 latency: 75866 usec
    p90 latency: 85276 usec
    p95 latency: 85390 usec
    p99 latency: 86290 usec
    Avg HTTP time: 75281 usec (send/recv 388 usec + response wait 74893 usec)
  Server:
    Inference count: 38272
    Execution count: 52
    Successful request count: 52
    Avg request latency: 74049 usec (overhead 446 usec + queue 52474 usec + compute input 464 usec + compute infer 20595 usec + compute output 70 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 184, throughput: 37680 infer/sec, latency 75133 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 192 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 192
  Client:
    Request count: 2631
    Throughput: 42096 infer/sec
    Avg latency: 71042 usec (standard deviation 2661 usec)
    p50 latency: 71416 usec
    p90 latency: 71658 usec
    p95 latency: 71775 usec
    p99 latency: 72105 usec
    Avg HTTP time: 71032 usec (send/recv 558 usec + response wait 70474 usec)
  Server:
    Inference count: 43248
    Execution count: 56
    Successful request count: 56
    Avg request latency: 69679 usec (overhead 596 usec + queue 48562 usec + compute input 579 usec + compute infer 19793 usec + compute output 149 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 192, throughput: 42096 infer/sec, latency 71042 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 200 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 200
  Client:
    Request count: 2316
    Throughput: 37056 infer/sec
    Avg latency: 83729 usec (standard deviation 6654 usec)
    p50 latency: 86669 usec
    p90 latency: 87562 usec
    p95 latency: 87822 usec
    p99 latency: 96963 usec
    Avg HTTP time: 83797 usec (send/recv 315 usec + response wait 83482 usec)
  Server:
    Inference count: 38816
    Execution count: 47
    Successful request count: 47
    Avg request latency: 81694 usec (overhead 685 usec + queue 58936 usec + compute input 741 usec + compute infer 21191 usec + compute output 141 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 200, throughput: 37056 infer/sec, latency 83729 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 208 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 208
  Client:
    Request count: 2470
    Throughput: 39520 infer/sec
    Avg latency: 80196 usec (standard deviation 8884 usec)
    p50 latency: 85211 usec
    p90 latency: 85722 usec
    p95 latency: 85933 usec
    p99 latency: 86628 usec
    Avg HTTP time: 80388 usec (send/recv 576 usec + response wait 79812 usec)
  Server:
    Inference count: 41248
    Execution count: 49
    Successful request count: 49
    Avg request latency: 78853 usec (overhead 431 usec + queue 55622 usec + compute input 518 usec + compute infer 22210 usec + compute output 72 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 208, throughput: 39520 infer/sec, latency 80196 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 216 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 216
  Client:
    Request count: 2495
    Throughput: 39920 infer/sec
    Avg latency: 83170 usec (standard deviation 1744 usec)
    p50 latency: 83310 usec
    p90 latency: 83729 usec
    p95 latency: 83865 usec
    p99 latency: 84026 usec
    Avg HTTP time: 83174 usec (send/recv 548 usec + response wait 82626 usec)
  Server:
    Inference count: 41536
    Execution count: 60
    Successful request count: 60
    Avg request latency: 81816 usec (overhead 400 usec + queue 62142 usec + compute input 529 usec + compute infer 18653 usec + compute output 92 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 216, throughput: 39920 infer/sec, latency 83170 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 224 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 224
  Client:
    Request count: 2310
    Throughput: 36923.1 infer/sec
    Avg latency: 92440 usec (standard deviation 1616 usec)
    p50 latency: 92556 usec
    p90 latency: 92841 usec
    p95 latency: 92871 usec
    p99 latency: 92965 usec
    Avg HTTP time: 92428 usec (send/recv 656 usec + response wait 91772 usec)
  Server:
    Inference count: 38800
    Execution count: 54
    Successful request count: 54
    Avg request latency: 90881 usec (overhead 398 usec + queue 69986 usec + compute input 457 usec + compute infer 19949 usec + compute output 91 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 224, throughput: 36923.1 infer/sec, latency 92440 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 232 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 232
  Client:
    Request count: 2624
    Throughput: 41984 infer/sec
    Avg latency: 85388 usec (standard deviation 2689 usec)
    p50 latency: 85740 usec
    p90 latency: 86015 usec
    p95 latency: 86101 usec
    p99 latency: 86350 usec
    Avg HTTP time: 85488 usec (send/recv 785 usec + response wait 84703 usec)
  Server:
    Inference count: 43904
    Execution count: 47
    Successful request count: 47
    Avg request latency: 83646 usec (overhead 443 usec + queue 61141 usec + compute input 556 usec + compute infer 21404 usec + compute output 102 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 232, throughput: 41984 infer/sec, latency 85388 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 240 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 240
  Client:
    Request count: 2689
    Throughput: 43024 infer/sec
    Avg latency: 85843 usec (standard deviation 9940 usec)
    p50 latency: 86515 usec
    p90 latency: 92813 usec
    p95 latency: 101430 usec
    p99 latency: 116902 usec
    Avg HTTP time: 85371 usec (send/recv 888 usec + response wait 84483 usec)
  Server:
    Inference count: 45936
    Execution count: 56
    Successful request count: 56
    Avg request latency: 82638 usec (overhead 727 usec + queue 56055 usec + compute input 784 usec + compute infer 24967 usec + compute output 105 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 240, throughput: 43024 infer/sec, latency 85843 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 248 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 248
  Client:
    Request count: 2501
    Throughput: 40016 infer/sec
    Avg latency: 93193 usec (standard deviation 6690 usec)
    p50 latency: 93711 usec
    p90 latency: 99134 usec
    p95 latency: 102719 usec
    p99 latency: 104559 usec
    Avg HTTP time: 93521 usec (send/recv 626 usec + response wait 92895 usec)
  Server:
    Inference count: 41744
    Execution count: 52
    Successful request count: 52
    Avg request latency: 91275 usec (overhead 606 usec + queue 68629 usec + compute input 690 usec + compute infer 21270 usec + compute output 80 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 248, throughput: 40016 infer/sec, latency 93193 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 256 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 256
  Client:
    Request count: 2690
    Throughput: 43040 infer/sec
    Avg latency: 90148 usec (standard deviation 12199 usec)
    p50 latency: 90435 usec
    p90 latency: 106607 usec
    p95 latency: 107533 usec
    p99 latency: 115925 usec
    Avg HTTP time: 90093 usec (send/recv 962 usec + response wait 89131 usec)
  Server:
    Inference count: 45920
    Execution count: 55
    Successful request count: 55
    Avg request latency: 87760 usec (overhead 709 usec + queue 59661 usec + compute input 671 usec + compute infer 26608 usec + compute output 111 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 256, throughput: 43040 infer/sec, latency 90148 usec
  Batch    Concurrency    Inferences/Second    Client Send    Network+Server Send/Recv    Server Queue    Server Compute Input    Server Compute Infer    Server Compute Output    Client Recv    p50 latency    p90 latency    p95 latency    p99 latency    avg latency
-------  -------------  -------------------  -------------  --------------------------  --------------  ----------------------  ----------------------  -----------------------  -------------  -------------  -------------  -------------  -------------  -------------
     16              8              19692.3             16                         110            3077                      71                    3140                       19              0           6436           6466           6473           6489           6433
     16             16              21840               34                         307            6407                      89                    4697                       74              0          12083          14401          14946          16193          11608
     16             24              25152               35                         358            8491                     115                    5913                      122              0          16338          18641          18918          19203          15034
     16             32              26432               39                         485           10576                     143                    7718                      177              0          19306          23066          24437          27365          19138
     16             40              28291.7             38                         489           11908                     181                    9615                      157              0          23569          28515          28857          29679          22388
     16             48              29168               41                         625           15006                     202                   10251                       64              0          26263          31109          31390          35838          26189
     16             56              28211.8             43                         935           13958                     260                   15623                       77              0          31101          37821          38081          38419          30896
     16             64              30208               43                        1155           14451                     309                   17003                       57              0          36013          37642          37964          38220          33018
     16             72              29522.5             47                         736           21570                     350                   15986                       60              0          40118          40773          42432          45779          38749
     16             80              36160               44                        1222           16452                     378                   16287                      113              0          35453          40310          40690          44944          34496
     16             88              36656               54                        1185           17938                     390                   17582                       94              0          40272          45220          45390          45556          37243
     16             96              31136               42                        1409           27488                     312                   18617                       57              0          49399          49575          49686          49824          47925
     16            104              33232               44                        1565           28026                     342                   19057                       61              0          49495          54292          54402          54611          49095
     16            112              37648               46                        1909           24460                     429                   19859                       76              0          47366          49689          49760          49812          46779
     16            120              40432               48                        2028           23331                     406                   20207                       57              0          48110          50616          50860          51195          46077
     16            128              31744.3             71                        1453           39518                     366                   20924                       87              0          63864          64302          64423          64575          62419
     16            136              38464               58                        1753           33313                     408                   19498                      107              0          54794          63961          64218          64561          55137
     16            144              38784              159                        1074           34251                     485                   21334                      105              0          62040          66422          66591          77222          57408
     16            152              37178.8            160                        1806           39493                     450                   20930                       83              0          64105          64484          64557          65001          62922
     16            160              39152              178                        1818           39471                     470                   20857                       75              0          64039          64238          64310          64593          62869
     16            168              35040              264                        1424           51163                     431                   20587                       90              0          75946          85050          85118          85279          73959
     16            176              35728              272                        1255           51037                     454                   20966                       88              0          75817          85256          85361          85685          74072
     16            184              37680              388                        1138           52474                     464                   20595                       70              0          75866          85276          85390          86290          75129
     16            192              42096              558                        1398           48562                     579                   19793                      149              0          71416          71658          71775          72105          71039
     16            200              37056              315                        2401           58936                     741                   21191                      141              0          86669          87562          87822          96963          83725
     16            208              39520              576                        1194           55622                     518                   22210                       72              0          85211          85722          85933          86628          80192
     16            216              39920              548                        1204           62142                     529                   18653                       92              0          83310          83729          83865          84026          83168
     16            224              36923.1            656                        1299           69986                     457                   19949                       91              0          92556          92841          92871          92965          92438
     16            232              41984              785                        1397           61141                     556                   21404                      102              0          85740          86015          86101          86350          85385
     16            240              43024              888                        3040           56055                     784                   24967                      105              0          86515          92813         101430         116902          85839
     16            248              40016              626                        1895           68629                     690                   21270                       80              0          93711          99134         102719         104559          93190
     16            256              43040              962                        2132           59661                     671                   26608                      111              0          90435         106607         107533         115925          90145
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ [[ ts-trace == \t\s\-\t\r\a\c\e ]]
+ export FORMAT_SUFFIX=pt
+ FORMAT_SUFFIX=pt
+ python3 triton/export_model.py --input-path triton/model.py --input-type pyt --output-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.pt --output-type ts-trace --ignore-unknown-parameters --onnx-opset 13 --checkpoint /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/electricity_bin/ --precision fp32 --dataloader triton/dataloader.py --dataset datasets/electricity_bin --batch-size 1
Got additional args ['--onnx-opset', '13']
2023-03-04 04:38:05,642 INFO export_model args:
2023-03-04 04:38:05,642 INFO export_model     input_path = triton/model.py
2023-03-04 04:38:05,642 INFO export_model     input_type = pyt
2023-03-04 04:38:05,643 INFO export_model     output_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.pt
2023-03-04 04:38:05,643 INFO export_model     output_type = ts-trace
2023-03-04 04:38:05,643 INFO export_model     dataloader = triton/dataloader.py
2023-03-04 04:38:05,643 INFO export_model     verbose = False
2023-03-04 04:38:05,643 INFO export_model     ignore_unknown_parameters = True
2023-03-04 04:38:05,643 INFO export_model     precision = fp32
2023-03-04 04:38:05,643 INFO export_model     dataset = datasets/electricity_bin
2023-03-04 04:38:05,643 INFO export_model     checkpoint = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/electricity_bin/
2023-03-04 04:38:05,643 INFO export_model     batch_size = 1
2023-03-04 04:38:05,643 INFO triton.deployment_toolkit.args Initializing get_dataloader_fn({'dataset': 'datasets/electricity_bin', 'checkpoint': '/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/electricity_bin/', 'batch_size': '1'})
2023-03-04 04:38:07,116 INFO triton.deployment_toolkit.args Initializing PyTorchModelLoader({'checkpoint': '/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/electricity_bin/', 'precision': 'fp32'})
triton/model.py:33: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['s_cat'] = s_cat if s_cat.shape[1] != 1 else None
triton/model.py:34: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['s_cont'] = s_cont if s_cont.shape[1] != 1 else None
triton/model.py:35: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['k_cat'] = k_cat if k_cat.shape[1] != 1 else None
triton/model.py:36: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['k_cont'] = k_cont if k_cont.shape[1] != 1 else None
triton/model.py:37: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['o_cat'] = o_cat if o_cat.shape[1] != 1 else None
triton/model.py:38: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['o_cont'] = o_cont if o_cont.shape[1] != 1 else None
2023-03-04 04:38:09,389 INFO export_model inputs: {'s_cat__0': TensorSpec(name='s_cat__0', shape=(-1, 192, 1), dtype=dtype('int64')), 's_cont__1': TensorSpec(name='s_cont__1', shape=(-1, 1), dtype=dtype('float32')), 'k_cat__2': TensorSpec(name='k_cat__2', shape=(-1, 1), dtype=dtype('float32')), 'k_cont__3': TensorSpec(name='k_cont__3', shape=(-1, 192, 3), dtype=dtype('float32')), 'o_cat__4': TensorSpec(name='o_cat__4', shape=(-1, 1), dtype=dtype('float32')), 'o_cont__5': TensorSpec(name='o_cont__5', shape=(-1, 1), dtype=dtype('float32')), 'target__6': TensorSpec(name='target__6', shape=(-1, 192, 1), dtype=dtype('float32')), 'id__7': TensorSpec(name='id__7', shape=(-1, 192, 1), dtype=dtype('int64'))}
2023-03-04 04:38:09,390 INFO export_model outputs: {'target__0': TensorSpec(name='target__0', shape=(-1, 24, 3), dtype=dtype('float32'))}
2023-03-04 04:38:09,392 INFO triton.deployment_toolkit.args Initializing TorchScriptSaver({})
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed 's/^$/unknown/'
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ [[ ts-trace == \t\s\-\t\r\a\c\e ]]
+ export FORMAT_SUFFIX=pt
+ FORMAT_SUFFIX=pt
+ model-navigator convert --model-name TFT --model-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.pt --output-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model --target-formats ts-trace --target-precisions fp16 --launch-mode local --override-workspace --verbose --onnx-opsets 13 --max-batch-size 1024 --container-version 21.08 --max-workspace-size 10000000000 --atol target__0=100 --rtol target__0=100
2023-03-04 04:38:11 - DEBUG - model_navigator.log: initialized logger verbose=True
2023-03-04 04:38:11 - DEBUG - convert: Running 'model-navigator convert' with config_path: None
2023-03-04 04:38:11 - INFO - model_navigator.log: Environment state
2023-03-04 04:38:11 - INFO - model_navigator.log: 	docker_container_id = 98c4b3fc07
2023-03-04 04:38:11 - INFO - model_navigator.log: 	docker_container_ip_address = None
2023-03-04 04:38:11 - INFO - model_navigator.log: 	docker_container_mounts = [{'host_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/dataset/processed'), 'container_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/datasets'), 'mount_type': 'bind'}, {'host_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT'), 'container_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT'), 'mount_type': 'bind'}, {'host_path': PosixPath('/var/run/docker.sock'), 'container_path': PosixPath('/var/run/docker.sock'), 'mount_type': 'bind'}]
2023-03-04 04:38:11 - DEBUG - model_navigator.validators: Running command validator: <model_navigator.validators.ModelNavigatorDockerContainerShouldHaveMountedWorkspaceDir object at 0x7f3c6038a190>
2023-03-04 04:38:11 - DEBUG - model_navigator.validators: ModelNavigatorDockerContainerShouldHaveMountedWorkspaceDir is_running_in_container=True has_mounted_workspace_path=True is_running_converter_in_docker=False
2023-03-04 04:38:11 - DEBUG - model_navigator.utils.workspace: Workspace path /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:38:11 - INFO - model_navigator.log: convert args:
2023-03-04 04:38:11 - INFO - model_navigator.log: 	model_name = TFT
2023-03-04 04:38:11 - INFO - model_navigator.log: 	model_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.pt
2023-03-04 04:38:11 - INFO - model_navigator.log: 	model_format = None
2023-03-04 04:38:11 - INFO - model_navigator.log: 	model_version = 1
2023-03-04 04:38:11 - INFO - model_navigator.log: 	target_formats = [<Format.TORCHSCRIPT: 'torchscript'>]
2023-03-04 04:38:11 - INFO - model_navigator.log: 	onnx_opsets = [13]
2023-03-04 04:38:11 - INFO - model_navigator.log: 	tensorrt_precisions = [<TensorRTPrecision.FP16: 'fp16'>]
2023-03-04 04:38:11 - INFO - model_navigator.log: 	tensorrt_precisions_mode = TensorRTPrecisionMode.HIERARCHY
2023-03-04 04:38:11 - INFO - model_navigator.log: 	tensorrt_explicit_precision = False
2023-03-04 04:38:11 - INFO - model_navigator.log: 	tensorrt_strict_types = False
2023-03-04 04:38:11 - INFO - model_navigator.log: 	tensorrt_sparse_weights = False
2023-03-04 04:38:11 - INFO - model_navigator.log: 	tensorrt_max_workspace_size = 10000000000
2023-03-04 04:38:11 - INFO - model_navigator.log: 	atol = {'target__0': 100.0}
2023-03-04 04:38:11 - INFO - model_navigator.log: 	rtol = {'target__0': 100.0}
2023-03-04 04:38:11 - INFO - model_navigator.log: 	max_batch_size = 1024
2023-03-04 04:38:11 - INFO - model_navigator.log: 	inputs = None
2023-03-04 04:38:11 - INFO - model_navigator.log: 	outputs = None
2023-03-04 04:38:11 - INFO - model_navigator.log: 	min_shapes = None
2023-03-04 04:38:11 - INFO - model_navigator.log: 	opt_shapes = None
2023-03-04 04:38:11 - INFO - model_navigator.log: 	max_shapes = None
2023-03-04 04:38:11 - INFO - model_navigator.log: 	value_ranges = None
2023-03-04 04:38:11 - INFO - model_navigator.log: 	dtypes = None
2023-03-04 04:38:11 - INFO - model_navigator.log: 	workspace_path = navigator_workspace
2023-03-04 04:38:11 - INFO - model_navigator.log: 	override_workspace = True
2023-03-04 04:38:11 - INFO - model_navigator.log: 	output_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model
2023-03-04 04:38:11 - INFO - model_navigator.log: 	container_version = 21.08
2023-03-04 04:38:11 - INFO - model_navigator.log: 	framework_docker_image = nvcr.io/nvidia/pytorch:21.08-py3
2023-03-04 04:38:11 - INFO - model_navigator.log: 	gpus = ('all',)
2023-03-04 04:38:11 - DEBUG - model_navigator.utils.workspace: Cleaning workspace dir /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:38:11 - DEBUG - model_navigator.converter.convert: Converter created; workspace=/home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:38:11 - DEBUG - model_navigator.converter.convert: Convert Commands Executor created; workspace=/home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:38:11 - DEBUG - model_navigator.results: Saving results of convert_model stage into /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/convert_model_results.yaml
2023-03-04 04:38:11 - DEBUG - model_navigator.results: ConversionResult(status=Status(state=<State.SUCCEEDED: 'succeeded'>, message='Source model', log_path=None), source_model_config=ModelConfig(model_name='TFT', model_path=PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.pt'), model_format=None, model_version='1'), conversion_config=ConversionConfig(target_format=<Format.TORCHSCRIPT: 'torchscript'>, onnx_opset=None, tensorrt_precision=None, tensorrt_precision_mode=<TensorRTPrecisionMode.HIERARCHY: 'hierarchy'>, tensorrt_explicit_precision=False, tensorrt_strict_types=False, tensorrt_sparse_weights=False), tensorrt_common_config=None, model_signature_config=None, comparator_config=None, dataset_profile=None, output_model=Model(name='TFT', path=PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.pt'), format=<Format.TORCHSCRIPT: 'torchscript'>, signature=ModelSignatureConfig(inputs={'s_cat__0': TensorSpec(name='s_cat__0', shape=(-1, 192, 1), dtype=dtype('int64')), 's_cont__1': TensorSpec(name='s_cont__1', shape=(-1, 1), dtype=dtype('float32')), 'k_cat__2': TensorSpec(name='k_cat__2', shape=(-1, 1), dtype=dtype('float32')), 'k_cont__3': TensorSpec(name='k_cont__3', shape=(-1, 192, 3), dtype=dtype('float32')), 'o_cat__4': TensorSpec(name='o_cat__4', shape=(-1, 1), dtype=dtype('float32')), 'o_cont__5': TensorSpec(name='o_cont__5', shape=(-1, 1), dtype=dtype('float32')), 'target__6': TensorSpec(name='target__6', shape=(-1, 192, 1), dtype=dtype('float32')), 'id__7': TensorSpec(name='id__7', shape=(-1, 192, 1), dtype=dtype('int64'))}, outputs={'target__0': TensorSpec(name='target__0', shape=(-1, 24, 3), dtype=dtype('float32'))}), properties=TorchScriptProperties(), num_required_gpus=None), framework_docker_image=None)
2023-03-04 04:38:11 - DEBUG - convert: Copy /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.pt to /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model
2023-03-04 04:38:11 - DEBUG - convert: Copy /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.pt.yaml to /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model.yaml
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ [[ ts-trace == \t\s\-\t\r\a\c\e ]]
+ export CONFIG_FORMAT=torchscript
+ CONFIG_FORMAT=torchscript
+ model-navigator triton-config-model --model-repository /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models --model-name TFT --model-version 1 --model-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model --model-format torchscript --model-control-mode explicit --load-model --load-model-timeout-s 100 --verbose --backend-accelerator none --tensorrt-precision fp16 --tensorrt-capture-cuda-graph --tensorrt-max-workspace-size 10000000000 --max-batch-size 1024 --batching dynamic --preferred-batch-sizes 512 1024 --max-queue-delay-us 1 --engine-count-per-device gpu=2
2023-03-04 04:38:11 - DEBUG - model_navigator.log: initialized logger verbose=True
2023-03-04 04:38:11 - DEBUG - triton_config_model: Running 'model-navigator triton-config-model' with config_path: None
2023-03-04 04:38:11 - INFO - model_navigator.log: Environment state
2023-03-04 04:38:11 - INFO - model_navigator.log: 	docker_container_id = 98c4b3fc07
2023-03-04 04:38:11 - INFO - model_navigator.log: 	docker_container_ip_address = None
2023-03-04 04:38:11 - INFO - model_navigator.log: 	docker_container_mounts = [{'host_path': PosixPath('/var/run/docker.sock'), 'container_path': PosixPath('/var/run/docker.sock'), 'mount_type': 'bind'}, {'host_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/dataset/processed'), 'container_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/datasets'), 'mount_type': 'bind'}, {'host_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT'), 'container_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT'), 'mount_type': 'bind'}]
2023-03-04 04:38:11 - DEBUG - model_navigator.validators: Running command validator: <model_navigator.validators.ModelNavigatorBatchingConfiguration object at 0x7fc4397f5a90>
2023-03-04 04:38:11 - DEBUG - model_navigator.validators: ModelNavigatorBatchingConfiguration 
batching=Batching.DYNAMIC 
max_batch_size=1024 
2023-03-04 04:38:11 - INFO - model_navigator.log: triton-config-model args:
2023-03-04 04:38:11 - INFO - model_navigator.log: 	model_name = TFT
2023-03-04 04:38:11 - INFO - model_navigator.log: 	model_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model
2023-03-04 04:38:11 - INFO - model_navigator.log: 	model_format = Format.TORCHSCRIPT
2023-03-04 04:38:11 - INFO - model_navigator.log: 	model_version = 1
2023-03-04 04:38:11 - INFO - model_navigator.log: 	model_repository = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models
2023-03-04 04:38:11 - INFO - model_navigator.log: 	load_model = True
2023-03-04 04:38:11 - INFO - model_navigator.log: 	load_model_timeout_s = 100
2023-03-04 04:38:11 - INFO - model_navigator.log: 	model_control_mode = ModelControlMode.EXPLICIT
2023-03-04 04:38:11 - INFO - model_navigator.log: 	inputs = None
2023-03-04 04:38:11 - INFO - model_navigator.log: 	outputs = None
2023-03-04 04:38:11 - INFO - model_navigator.log: 	max_batch_size = 1024
2023-03-04 04:38:11 - INFO - model_navigator.log: 	batching = Batching.DYNAMIC
2023-03-04 04:38:11 - INFO - model_navigator.log: 	backend_accelerator = BackendAccelerator.NONE
2023-03-04 04:38:11 - INFO - model_navigator.log: 	tensorrt_precision = TensorRTOptPrecision.FP16
2023-03-04 04:38:11 - INFO - model_navigator.log: 	tensorrt_capture_cuda_graph = True
2023-03-04 04:38:11 - INFO - model_navigator.log: 	tensorrt_max_workspace_size = 10000000000
2023-03-04 04:38:11 - INFO - model_navigator.log: 	preferred_batch_sizes = [512, 1024]
2023-03-04 04:38:11 - INFO - model_navigator.log: 	max_queue_delay_us = 1
2023-03-04 04:38:11 - INFO - model_navigator.log: 	engine_count_per_device = {<DeviceKind.GPU: 'gpu'>: 2}
2023-03-04 04:38:11 - INFO - model_navigator.log: 	triton_backend_parameters = {}
2023-03-04 04:38:11 - INFO - model_navigator.log: 	server_url = grpc://localhost:8001
2023-03-04 04:38:11 - INFO - model_navigator.log: model:
2023-03-04 04:38:11 - INFO - model_navigator.log: 	name = TFT
2023-03-04 04:38:11 - INFO - model_navigator.log: 	path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model
2023-03-04 04:38:11 - INFO - model_navigator.log: 	format = Format.TORCHSCRIPT
2023-03-04 04:38:11 - INFO - model_navigator.log: 	signature = {'inputs': {'s_cat__0': {'name': 's_cat__0', 'shape': (-1, 192, 1), 'dtype': dtype('int64')}, 's_cont__1': {'name': 's_cont__1', 'shape': (-1, 1), 'dtype': dtype('float32')}, 'k_cat__2': {'name': 'k_cat__2', 'shape': (-1, 1), 'dtype': dtype('float32')}, 'k_cont__3': {'name': 'k_cont__3', 'shape': (-1, 192, 3), 'dtype': dtype('float32')}, 'o_cat__4': {'name': 'o_cat__4', 'shape': (-1, 1), 'dtype': dtype('float32')}, 'o_cont__5': {'name': 'o_cont__5', 'shape': (-1, 1), 'dtype': dtype('float32')}, 'target__6': {'name': 'target__6', 'shape': (-1, 192, 1), 'dtype': dtype('float32')}, 'id__7': {'name': 'id__7', 'shape': (-1, 192, 1), 'dtype': dtype('int64')}}, 'outputs': {'target__0': {'name': 'target__0', 'shape': (-1, 24, 3), 'dtype': dtype('float32')}}}
2023-03-04 04:38:11 - INFO - model_navigator.log: 	properties = {}
2023-03-04 04:38:11 - INFO - model_navigator.log: 	num_required_gpus = None
2023-03-04 04:38:11 - DEBUG - model_navigator.triton.model_store: Deploying model /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model in Triton Model Store /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models with: 
batching config: TritonBatchingConfig(max_batch_size=1024, batching=<Batching.DYNAMIC: 'dynamic'>)
optimization config: TritonModelOptimizationConfig(backend_accelerator=<BackendAccelerator.NONE: 'none'>, tensorrt_precision=<TensorRTOptPrecision.FP16: 'fp16'>, tensorrt_capture_cuda_graph=True)
dynamic batching config: TritonDynamicBatchingConfig(preferred_batch_sizes=[512, 1024], max_queue_delay_us=1)
2023-03-04 04:38:11 - DEBUG - model_navigator.triton.model_store: Copying /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model to /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models/TFT/1/model.pt
2023-03-04 04:38:11 - DEBUG - model_navigator.triton.model_config: Generated Triton config:
name: "TFT"
max_batch_size: 1024
input {
  name: "s_cat__0"
  data_type: TYPE_INT64
  dims: 192
  dims: 1
}
input {
  name: "s_cont__1"
  data_type: TYPE_FP32
  dims: 1
}
input {
  name: "k_cat__2"
  data_type: TYPE_FP32
  dims: 1
}
input {
  name: "k_cont__3"
  data_type: TYPE_FP32
  dims: 192
  dims: 3
}
input {
  name: "o_cat__4"
  data_type: TYPE_FP32
  dims: 1
}
input {
  name: "o_cont__5"
  data_type: TYPE_FP32
  dims: 1
}
input {
  name: "target__6"
  data_type: TYPE_FP32
  dims: 192
  dims: 1
}
input {
  name: "id__7"
  data_type: TYPE_INT64
  dims: 192
  dims: 1
}
output {
  name: "target__0"
  data_type: TYPE_FP32
  dims: 24
  dims: 3
}
instance_group {
  count: 2
  kind: KIND_GPU
}
dynamic_batching {
  preferred_batch_size: 512
  preferred_batch_size: 1024
  max_queue_delay_microseconds: 1
}
backend: "pytorch"

2023-03-04 04:38:11 - DEBUG - triton_config_model: Loading model TFT:1 model_control_mode: ModelControlMode.EXPLICIT
2023-03-04 04:38:11 - DEBUG - model_navigator.triton.client: Connecting to grpc://localhost:8001
2023-03-04 04:38:11 - DEBUG - triton_config_model: Waiting for server (timeout=5s)
is_server_ready, metadata ()

ready: true

is_server_live, metadata ()

live: true

2023-03-04 04:38:11 - DEBUG - triton_config_model: Sending load_model request
load_model, metadata ()
override files omitted:
model_name: "TFT"

Loaded model 'TFT'
2023-03-04 04:38:13 - DEBUG - triton_config_model: Polling for model availability (timeout=100s)
get_model_repository_index, metadata ()

models {
  name: "TFT"
  version: "1"
  state: "READY"
}

is_model_ready, metadata ()
name: "TFT"

ready: true

get_model_metadata, metadata ()
name: "TFT"

name: "TFT"
versions: "1"
platform: "pytorch_libtorch"
inputs {
  name: "s_cat__0"
  datatype: "INT64"
  shape: -1
  shape: 192
  shape: 1
}
inputs {
  name: "s_cont__1"
  datatype: "FP32"
  shape: -1
  shape: 1
}
inputs {
  name: "k_cat__2"
  datatype: "FP32"
  shape: -1
  shape: 1
}
inputs {
  name: "k_cont__3"
  datatype: "FP32"
  shape: -1
  shape: 192
  shape: 3
}
inputs {
  name: "o_cat__4"
  datatype: "FP32"
  shape: -1
  shape: 1
}
inputs {
  name: "o_cont__5"
  datatype: "FP32"
  shape: -1
  shape: 1
}
inputs {
  name: "target__6"
  datatype: "FP32"
  shape: -1
  shape: 192
  shape: 1
}
inputs {
  name: "id__7"
  datatype: "INT64"
  shape: -1
  shape: 192
  shape: 1
}
outputs {
  name: "target__0"
  datatype: "FP32"
  shape: -1
  shape: 24
  shape: 3
}

2023-03-04 04:38:13 - DEBUG - model_navigator.utils.workspace: Workspace path /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:38:13 - DEBUG - model_navigator.results: Saving results of triton_config_model stage into /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/triton_config_model_results.yaml
2023-03-04 04:38:13 - DEBUG - model_navigator.results: ConfigModelResult(status=Status(state=<State.SUCCEEDED: 'succeeded'>, message='Model configured and loaded correctly', log_path=None), model_config=ModelConfig(model_name='TFT', model_path=PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model'), model_format=<Format.TORCHSCRIPT: 'torchscript'>, model_version='1'), model_version='1', batching_config=TritonBatchingConfig(max_batch_size=1024, batching=<Batching.DYNAMIC: 'dynamic'>), optimization_config=TritonModelOptimizationConfig(backend_accelerator=<BackendAccelerator.NONE: 'none'>, tensorrt_precision=<TensorRTOptPrecision.FP16: 'fp16'>, tensorrt_capture_cuda_graph=True), dynamic_batching_config=TritonDynamicBatchingConfig(preferred_batch_sizes=[512, 1024], max_queue_delay_us=1), instances_config=TritonModelInstancesConfig(engine_count_per_device={<DeviceKind.GPU: 'gpu'>: 2}), tensorrt_common_config=TensorRTCommonConfig(tensorrt_max_workspace_size=10000000000), model_dir_in_model_store=PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models/TFT'))
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ mkdir -p /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data
+ python triton/prepare_input_data.py --input-data-dir /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/ --dataset datasets/electricity_bin --checkpoint /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/electricity_bin/
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ python triton/run_performance_on_triton.py --model-repository /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models --model-name TFT --input-data /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/data.json --batch-sizes 1,2,4,8,16,32,64,128,256,512,1024 --number-of-triton-instances 1 --batching-mode static --evaluation-mode offline --measurement-request-count 500 --warmup --performance-tool perf_analyzer --result-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/triton_performance_offline.csv
2023-03-04 04:38:17,345 INFO run_performance_on_triton Running warmup before the main test
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 1
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 1000
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 1043
    Throughput: 260.75 infer/sec
    Avg latency: 3827 usec (standard deviation 76 usec)
    p50 latency: 3823 usec
    p90 latency: 3842 usec
    p95 latency: 3849 usec
    p99 latency: 3869 usec
    Avg HTTP time: 3825 usec (send/recv 14 usec + response wait 3811 usec)
  Server:
    Inference count: 1044
    Execution count: 1044
    Successful request count: 1044
    Avg request latency: 3745 usec (overhead 29 usec + queue 15 usec + compute input 54 usec + compute infer 3641 usec + compute output 6 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 260.75 infer/sec, latency 3827 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 1024
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 1000
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 1007
    Throughput: 24549.3 infer/sec
    Avg latency: 41699 usec (standard deviation 270 usec)
    p50 latency: 41668 usec
    p90 latency: 42081 usec
    p95 latency: 42179 usec
    p99 latency: 42388 usec
    Avg HTTP time: 41695 usec (send/recv 36 usec + response wait 41659 usec)
  Server:
    Inference count: 1031168
    Execution count: 1007
    Successful request count: 1007
    Avg request latency: 41404 usec (overhead 28 usec + queue 152 usec + compute input 463 usec + compute infer 4342 usec + compute output 36419 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 24549.3 infer/sec, latency 41699 usec
2023-03-04 04:40:42,401 INFO run_performance_on_triton Using Perf Analyzer for performance evaluation
2023-03-04 04:40:42,401 INFO run_performance_on_triton Selected configuration
2023-03-04 04:40:42,401 INFO run_performance_on_triton 	server_url = http://127.0.0.1:8000
2023-03-04 04:40:42,401 INFO run_performance_on_triton 	model_name = TFT
2023-03-04 04:40:42,401 INFO run_performance_on_triton 	input_data = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/data.json
2023-03-04 04:40:42,401 INFO run_performance_on_triton 	input_shapes = []
2023-03-04 04:40:42,401 INFO run_performance_on_triton 	batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
2023-03-04 04:40:42,401 INFO run_performance_on_triton 	number_of_triton_instances = 1
2023-03-04 04:40:42,401 INFO run_performance_on_triton 	number_of_model_instances = 1
2023-03-04 04:40:42,401 INFO run_performance_on_triton 	measurement_mode = MeasurementMode.COUNT_WINDOWS
2023-03-04 04:40:42,401 INFO run_performance_on_triton 	measurement_interval = 5000
2023-03-04 04:40:42,401 INFO run_performance_on_triton 	measurement_request_count = 500
2023-03-04 04:40:42,401 INFO run_performance_on_triton 	concurrency_steps = 32
2023-03-04 04:40:42,401 INFO run_performance_on_triton 	batching_mode = BatchingMode.STATIC
2023-03-04 04:40:42,401 INFO run_performance_on_triton 	evaluation_mode = EvaluationMode.OFFLINE
2023-03-04 04:40:42,401 INFO run_performance_on_triton 	offline_mode = OfflineMode.SYSTEM
2023-03-04 04:40:42,401 INFO run_performance_on_triton 	output_shared_memory_size = 100240
2023-03-04 04:40:42,401 INFO run_performance_on_triton 	result_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/triton_performance_offline.csv
2023-03-04 04:40:42,402 INFO run_performance_on_triton 	verbose = False
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 1
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 536
    Throughput: 268 infer/sec
    Avg latency: 3725 usec (standard deviation 22 usec)
    p50 latency: 3724 usec
    p90 latency: 3744 usec
    p95 latency: 3749 usec
    p99 latency: 3772 usec
    Avg HTTP time: 3724 usec (send/recv 13 usec + response wait 3711 usec)
  Server:
    Inference count: 537
    Execution count: 537
    Successful request count: 537
    Avg request latency: 3644 usec (overhead 26 usec + queue 12 usec + compute input 47 usec + compute infer 3553 usec + compute output 6 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 268 infer/sec, latency 3725 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 2
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 729
    Throughput: 486 infer/sec
    Avg latency: 4106 usec (standard deviation 28 usec)
    p50 latency: 4110 usec
    p90 latency: 4139 usec
    p95 latency: 4144 usec
    p99 latency: 4154 usec
    Avg HTTP time: 4104 usec (send/recv 14 usec + response wait 4090 usec)
  Server:
    Inference count: 1460
    Execution count: 730
    Successful request count: 730
    Avg request latency: 4024 usec (overhead 24 usec + queue 13 usec + compute input 48 usec + compute infer 3708 usec + compute output 231 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 486 infer/sec, latency 4106 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 4
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 719
    Throughput: 958.347 infer/sec
    Avg latency: 4164 usec (standard deviation 9 usec)
    p50 latency: 4163 usec
    p90 latency: 4176 usec
    p95 latency: 4179 usec
    p99 latency: 4185 usec
    Avg HTTP time: 4162 usec (send/recv 14 usec + response wait 4148 usec)
  Server:
    Inference count: 2880
    Execution count: 720
    Successful request count: 720
    Avg request latency: 4082 usec (overhead 27 usec + queue 12 usec + compute input 49 usec + compute infer 3745 usec + compute output 249 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 958.347 infer/sec, latency 4164 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 8
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 710
    Throughput: 1892.7 infer/sec
    Avg latency: 4216 usec (standard deviation 9 usec)
    p50 latency: 4216 usec
    p90 latency: 4228 usec
    p95 latency: 4232 usec
    p99 latency: 4239 usec
    Avg HTTP time: 4215 usec (send/recv 13 usec + response wait 4202 usec)
  Server:
    Inference count: 5680
    Execution count: 710
    Successful request count: 710
    Avg request latency: 4135 usec (overhead 26 usec + queue 12 usec + compute input 53 usec + compute infer 3798 usec + compute output 246 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 1892.7 infer/sec, latency 4216 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 682
    Throughput: 3637.33 infer/sec
    Avg latency: 4390 usec (standard deviation 98 usec)
    p50 latency: 4339 usec
    p90 latency: 4557 usec
    p95 latency: 4576 usec
    p99 latency: 4606 usec
    Avg HTTP time: 4389 usec (send/recv 14 usec + response wait 4375 usec)
  Server:
    Inference count: 10912
    Execution count: 682
    Successful request count: 682
    Avg request latency: 4308 usec (overhead 26 usec + queue 50 usec + compute input 61 usec + compute infer 3830 usec + compute output 341 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 3637.33 infer/sec, latency 4390 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 32
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 622
    Throughput: 6634.67 infer/sec
    Avg latency: 4816 usec (standard deviation 101 usec)
    p50 latency: 4875 usec
    p90 latency: 4913 usec
    p95 latency: 4924 usec
    p99 latency: 4959 usec
    Avg HTTP time: 4815 usec (send/recv 13 usec + response wait 4802 usec)
  Server:
    Inference count: 19936
    Execution count: 623
    Successful request count: 623
    Avg request latency: 4735 usec (overhead 29 usec + queue 107 usec + compute input 71 usec + compute infer 4125 usec + compute output 403 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 6634.67 infer/sec, latency 4816 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 64
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 503
    Throughput: 10730.7 infer/sec
    Avg latency: 5954 usec (standard deviation 72 usec)
    p50 latency: 5982 usec
    p90 latency: 6012 usec
    p95 latency: 6020 usec
    p99 latency: 6038 usec
    Avg HTTP time: 5953 usec (send/recv 14 usec + response wait 5939 usec)
  Server:
    Inference count: 32256
    Execution count: 504
    Successful request count: 504
    Avg request latency: 5871 usec (overhead 28 usec + queue 112 usec + compute input 85 usec + compute infer 4180 usec + compute output 1466 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 10730.7 infer/sec, latency 5954 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 128
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 601
    Throughput: 15382.5 infer/sec
    Avg latency: 8308 usec (standard deviation 36 usec)
    p50 latency: 8314 usec
    p90 latency: 8338 usec
    p95 latency: 8345 usec
    p99 latency: 8372 usec
    Avg HTTP time: 8307 usec (send/recv 14 usec + response wait 8293 usec)
  Server:
    Inference count: 77056
    Execution count: 602
    Successful request count: 602
    Avg request latency: 8226 usec (overhead 28 usec + queue 153 usec + compute input 108 usec + compute infer 4189 usec + compute output 3748 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 15382.5 infer/sec, latency 8308 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 256
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 538
    Throughput: 19675.4 infer/sec
    Avg latency: 12987 usec (standard deviation 81 usec)
    p50 latency: 12977 usec
    p90 latency: 13093 usec
    p95 latency: 13125 usec
    p99 latency: 13175 usec
    Avg HTTP time: 12983 usec (send/recv 36 usec + response wait 12947 usec)
  Server:
    Inference count: 137728
    Execution count: 538
    Successful request count: 538
    Avg request latency: 12763 usec (overhead 27 usec + queue 173 usec + compute input 162 usec + compute infer 4220 usec + compute output 8181 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 19675.4 infer/sec, latency 12987 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 512
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 531
    Throughput: 22654.1 infer/sec
    Avg latency: 22564 usec (standard deviation 123 usec)
    p50 latency: 22567 usec
    p90 latency: 22741 usec
    p95 latency: 22787 usec
    p99 latency: 22884 usec
    Avg HTTP time: 22560 usec (send/recv 37 usec + response wait 22523 usec)
  Server:
    Inference count: 272384
    Execution count: 532
    Successful request count: 532
    Avg request latency: 22386 usec (overhead 28 usec + queue 178 usec + compute input 264 usec + compute infer 4222 usec + compute output 17694 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 22654.1 infer/sec, latency 22564 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 1024
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 503
    Throughput: 24526.1 infer/sec
    Avg latency: 41717 usec (standard deviation 260 usec)
    p50 latency: 41676 usec
    p90 latency: 42092 usec
    p95 latency: 42202 usec
    p99 latency: 42363 usec
    Avg HTTP time: 41712 usec (send/recv 36 usec + response wait 41676 usec)
  Server:
    Inference count: 515072
    Execution count: 503
    Successful request count: 503
    Avg request latency: 41414 usec (overhead 31 usec + queue 148 usec + compute input 486 usec + compute infer 4381 usec + compute output 36368 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 24526.1 infer/sec, latency 41717 usec
  Batch    Concurrency    Inferences/Second    Client Send    Network+Server Send/Recv    Server Queue    Server Compute Input    Server Compute Infer    Server Compute Output    Client Recv    p50 latency    p90 latency    p95 latency    p99 latency    avg latency
-------  -------------  -------------------  -------------  --------------------------  --------------  ----------------------  ----------------------  -----------------------  -------------  -------------  -------------  -------------  -------------  -------------
      1              1              268                 13                          90              12                      47                    3553                        6              0           3724           3744           3749           3772           3721
      2              1              486                 14                          90              13                      48                    3708                      231              0           4110           4139           4144           4154           4104
      4              1              958.347             14                          91              12                      49                    3745                      249              0           4163           4176           4179           4185           4160
      8              1             1892.7               13                          91              12                      53                    3798                      246              0           4216           4228           4232           4239           4213
     16              1             3637.33              14                          93              50                      61                    3830                      341              0           4339           4557           4576           4606           4389
     32              1             6634.67              13                          93             107                      71                    4125                      403              0           4875           4913           4924           4959           4812
     64              1            10730.7               14                          95             112                      85                    4180                     1466              0           5982           6012           6020           6038           5952
    128              1            15382.5               14                          95             153                     108                    4189                     3748              0           8314           8338           8345           8372           8307
    256              1            19675.4               36                         213             173                     162                    4220                     8181              0          12977          13093          13125          13175          12985
    512              1            22654.1               37                         167             178                     264                    4222                    17694              0          22567          22741          22787          22884          22562
   1024              1            24526.1               36                         294             148                     486                    4381                    36368              0          41676          42092          42202          42363          41713
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ python triton/run_performance_on_triton.py --model-repository /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models --model-name TFT --input-data /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/data.json --batch-sizes 1,2,4,8,16,32,64,128,256,512,1024 --number-of-triton-instances 1 --number-of-model-instances 2 --batching-mode dynamic --evaluation-mode online --measurement-request-count 500 --warmup --performance-tool perf_analyzer --result-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/triton_performance_online.csv
2023-03-04 04:44:26,837 INFO run_performance_on_triton Running warmup before the main test
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 1000
  Latency limit: 0 msec
  Concurrency limit: 128 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 128
  Client:
    Request count: 1411
    Throughput: 22576 infer/sec
    Avg latency: 86100 usec (standard deviation 10896 usec)
    p50 latency: 88954 usec
    p90 latency: 96362 usec
    p95 latency: 98040 usec
    p99 latency: 104686 usec
    Avg HTTP time: 86112 usec (send/recv 136 usec + response wait 85976 usec)
  Server:
    Inference count: 23568
    Execution count: 44
    Successful request count: 44
    Avg request latency: 84926 usec (overhead 405 usec + queue 39939 usec + compute input 5742 usec + compute infer 6623 usec + compute output 32217 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 128, throughput: 22576 infer/sec, latency 86100 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 1000
  Latency limit: 0 msec
  Concurrency limit: 256 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 256
  Client:
    Request count: 1395
    Throughput: 22320 infer/sec
    Avg latency: 168068 usec (standard deviation 12360 usec)
    p50 latency: 169688 usec
    p90 latency: 186351 usec
    p95 latency: 188667 usec
    p99 latency: 191510 usec
    Avg HTTP time: 168483 usec (send/recv 749 usec + response wait 167734 usec)
  Server:
    Inference count: 23792
    Execution count: 29
    Successful request count: 29
    Avg request latency: 166501 usec (overhead 505 usec + queue 97479 usec + compute input 15845 usec + compute infer 6241 usec + compute output 46431 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 256, throughput: 22320 infer/sec, latency 168068 usec
2023-03-04 04:44:38,094 INFO run_performance_on_triton Using Perf Analyzer for performance evaluation
2023-03-04 04:44:38,094 INFO run_performance_on_triton Selected configuration
2023-03-04 04:44:38,094 INFO run_performance_on_triton 	server_url = http://127.0.0.1:8000
2023-03-04 04:44:38,094 INFO run_performance_on_triton 	model_name = TFT
2023-03-04 04:44:38,094 INFO run_performance_on_triton 	input_data = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/data.json
2023-03-04 04:44:38,094 INFO run_performance_on_triton 	input_shapes = []
2023-03-04 04:44:38,094 INFO run_performance_on_triton 	batch_sizes = [16]
2023-03-04 04:44:38,094 INFO run_performance_on_triton 	number_of_triton_instances = 1
2023-03-04 04:44:38,094 INFO run_performance_on_triton 	number_of_model_instances = 2
2023-03-04 04:44:38,094 INFO run_performance_on_triton 	measurement_mode = MeasurementMode.COUNT_WINDOWS
2023-03-04 04:44:38,094 INFO run_performance_on_triton 	measurement_interval = 5000
2023-03-04 04:44:38,094 INFO run_performance_on_triton 	measurement_request_count = 500
2023-03-04 04:44:38,094 INFO run_performance_on_triton 	concurrency_steps = 32
2023-03-04 04:44:38,094 INFO run_performance_on_triton 	batching_mode = BatchingMode.DYNAMIC
2023-03-04 04:44:38,094 INFO run_performance_on_triton 	evaluation_mode = EvaluationMode.ONLINE
2023-03-04 04:44:38,094 INFO run_performance_on_triton 	offline_mode = OfflineMode.SYSTEM
2023-03-04 04:44:38,094 INFO run_performance_on_triton 	output_shared_memory_size = 100240
2023-03-04 04:44:38,094 INFO run_performance_on_triton 	result_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/triton_performance_online.csv
2023-03-04 04:44:38,094 INFO run_performance_on_triton 	verbose = False
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 8 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 8
  Client:
    Request count: 510
    Throughput: 8160 infer/sec
    Avg latency: 15449 usec (standard deviation 3696 usec)
    p50 latency: 17527 usec
    p90 latency: 17863 usec
    p95 latency: 18009 usec
    p99 latency: 18171 usec
    Avg HTTP time: 15448 usec (send/recv 45 usec + response wait 15403 usec)
  Server:
    Inference count: 8240
    Execution count: 224
    Successful request count: 224
    Avg request latency: 15113 usec (overhead 33 usec + queue 6175 usec + compute input 212 usec + compute infer 6420 usec + compute output 2273 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 8, throughput: 8160 infer/sec, latency 15449 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 16 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 16
  Client:
    Request count: 833
    Throughput: 13328 infer/sec
    Avg latency: 18942 usec (standard deviation 4757 usec)
    p50 latency: 21924 usec
    p90 latency: 22781 usec
    p95 latency: 22973 usec
    p99 latency: 23346 usec
    Avg HTTP time: 18940 usec (send/recv 40 usec + response wait 18900 usec)
  Server:
    Inference count: 13488
    Execution count: 176
    Successful request count: 176
    Avg request latency: 18604 usec (overhead 49 usec + queue 7204 usec + compute input 439 usec + compute infer 6584 usec + compute output 4328 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 16, throughput: 13328 infer/sec, latency 18942 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 24 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 24
  Client:
    Request count: 1017
    Throughput: 16272 infer/sec
    Avg latency: 23105 usec (standard deviation 4840 usec)
    p50 latency: 25627 usec
    p90 latency: 26342 usec
    p95 latency: 26491 usec
    p99 latency: 26811 usec
    Avg HTTP time: 23110 usec (send/recv 48 usec + response wait 23062 usec)
  Server:
    Inference count: 16528
    Execution count: 152
    Successful request count: 152
    Avg request latency: 22755 usec (overhead 78 usec + queue 9280 usec + compute input 639 usec + compute infer 6683 usec + compute output 6075 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 24, throughput: 16272 infer/sec, latency 23105 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 32 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 32
  Client:
    Request count: 1155
    Throughput: 18480 infer/sec
    Avg latency: 27387 usec (standard deviation 5248 usec)
    p50 latency: 29959 usec
    p90 latency: 30637 usec
    p95 latency: 30788 usec
    p99 latency: 31202 usec
    Avg HTTP time: 27383 usec (send/recv 44 usec + response wait 27339 usec)
  Server:
    Inference count: 18592
    Execution count: 131
    Successful request count: 131
    Avg request latency: 27006 usec (overhead 96 usec + queue 11317 usec + compute input 777 usec + compute infer 6917 usec + compute output 7899 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 32, throughput: 18480 infer/sec, latency 27387 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 40 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 40
  Client:
    Request count: 1217
    Throughput: 19472 infer/sec
    Avg latency: 32052 usec (standard deviation 5517 usec)
    p50 latency: 34475 usec
    p90 latency: 35242 usec
    p95 latency: 35629 usec
    p99 latency: 36166 usec
    Avg HTTP time: 32022 usec (send/recv 48 usec + response wait 31974 usec)
  Server:
    Inference count: 19696
    Execution count: 114
    Successful request count: 114
    Avg request latency: 31301 usec (overhead 155 usec + queue 12083 usec + compute input 783 usec + compute infer 6997 usec + compute output 11283 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 40, throughput: 19472 infer/sec, latency 32052 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 48 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 48
  Client:
    Request count: 1284
    Throughput: 20544 infer/sec
    Avg latency: 36368 usec (standard deviation 5496 usec)
    p50 latency: 38660 usec
    p90 latency: 39298 usec
    p95 latency: 39542 usec
    p99 latency: 40521 usec
    Avg HTTP time: 36351 usec (send/recv 51 usec + response wait 36300 usec)
  Server:
    Inference count: 20896
    Execution count: 102
    Successful request count: 102
    Avg request latency: 35571 usec (overhead 165 usec + queue 15631 usec + compute input 822 usec + compute infer 7293 usec + compute output 11660 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 48, throughput: 20544 infer/sec, latency 36368 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 56 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 56
  Client:
    Request count: 1308
    Throughput: 20928 infer/sec
    Avg latency: 41271 usec (standard deviation 5965 usec)
    p50 latency: 43614 usec
    p90 latency: 44358 usec
    p95 latency: 44582 usec
    p99 latency: 45064 usec
    Avg HTTP time: 41283 usec (send/recv 49 usec + response wait 41234 usec)
  Server:
    Inference count: 21504
    Execution count: 90
    Successful request count: 90
    Avg request latency: 40199 usec (overhead 205 usec + queue 17203 usec + compute input 891 usec + compute infer 7466 usec + compute output 14434 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 56, throughput: 20928 infer/sec, latency 41271 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 64 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 64
  Client:
    Request count: 1371
    Throughput: 21936 infer/sec
    Avg latency: 45901 usec (standard deviation 5809 usec)
    p50 latency: 47998 usec
    p90 latency: 48148 usec
    p95 latency: 48176 usec
    p99 latency: 48609 usec
    Avg HTTP time: 45899 usec (send/recv 50 usec + response wait 45849 usec)
  Server:
    Inference count: 22512
    Execution count: 84
    Successful request count: 84
    Avg request latency: 44632 usec (overhead 235 usec + queue 20504 usec + compute input 818 usec + compute infer 7420 usec + compute output 15655 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 64, throughput: 21936 infer/sec, latency 45901 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 72 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 72
  Client:
    Request count: 1367
    Throughput: 21872 infer/sec
    Avg latency: 50685 usec (standard deviation 7097 usec)
    p50 latency: 52923 usec
    p90 latency: 54825 usec
    p95 latency: 55308 usec
    p99 latency: 56675 usec
    Avg HTTP time: 50678 usec (send/recv 67 usec + response wait 50611 usec)
  Server:
    Inference count: 22384
    Execution count: 74
    Successful request count: 74
    Avg request latency: 49610 usec (overhead 242 usec + queue 22420 usec + compute input 1291 usec + compute infer 7048 usec + compute output 18609 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 72, throughput: 21872 infer/sec, latency 50685 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 80 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 80
  Client:
    Request count: 1414
    Throughput: 22624 infer/sec
    Avg latency: 55253 usec (standard deviation 6873 usec)
    p50 latency: 57349 usec
    p90 latency: 58190 usec
    p95 latency: 58575 usec
    p99 latency: 59103 usec
    Avg HTTP time: 55267 usec (send/recv 61 usec + response wait 55206 usec)
  Server:
    Inference count: 23392
    Execution count: 70
    Successful request count: 70
    Avg request latency: 54572 usec (overhead 219 usec + queue 25330 usec + compute input 1523 usec + compute infer 7217 usec + compute output 20283 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 80, throughput: 22624 infer/sec, latency 55253 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 88 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 88
  Client:
    Request count: 1398
    Throughput: 22368 infer/sec
    Avg latency: 60192 usec (standard deviation 7101 usec)
    p50 latency: 62197 usec
    p90 latency: 63032 usec
    p95 latency: 63619 usec
    p99 latency: 64212 usec
    Avg HTTP time: 60203 usec (send/recv 62 usec + response wait 60141 usec)
  Server:
    Inference count: 23248
    Execution count: 64
    Successful request count: 64
    Avg request latency: 58680 usec (overhead 263 usec + queue 25227 usec + compute input 1643 usec + compute infer 7424 usec + compute output 24123 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 88, throughput: 22368 infer/sec, latency 60192 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 96 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 96
  Client:
    Request count: 1442
    Throughput: 23072 infer/sec
    Avg latency: 65193 usec (standard deviation 7861 usec)
    p50 latency: 67372 usec
    p90 latency: 68853 usec
    p95 latency: 70217 usec
    p99 latency: 70550 usec
    Avg HTTP time: 65166 usec (send/recv 68 usec + response wait 65098 usec)
  Server:
    Inference count: 23712
    Execution count: 59
    Successful request count: 59
    Avg request latency: 63788 usec (overhead 292 usec + queue 29505 usec + compute input 1534 usec + compute infer 7414 usec + compute output 25043 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 96, throughput: 23072 infer/sec, latency 65193 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 104 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 104
  Client:
    Request count: 1398
    Throughput: 22368 infer/sec
    Avg latency: 70060 usec (standard deviation 7811 usec)
    p50 latency: 72235 usec
    p90 latency: 73262 usec
    p95 latency: 73894 usec
    p99 latency: 75046 usec
    Avg HTTP time: 70079 usec (send/recv 68 usec + response wait 70011 usec)
  Server:
    Inference count: 23328
    Execution count: 54
    Successful request count: 54
    Avg request latency: 68423 usec (overhead 291 usec + queue 31392 usec + compute input 2095 usec + compute infer 7407 usec + compute output 27238 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 104, throughput: 22368 infer/sec, latency 70060 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 112 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 112
  Client:
    Request count: 1457
    Throughput: 23312 infer/sec
    Avg latency: 74963 usec (standard deviation 8502 usec)
    p50 latency: 77194 usec
    p90 latency: 78851 usec
    p95 latency: 79834 usec
    p99 latency: 82855 usec
    Avg HTTP time: 74951 usec (send/recv 69 usec + response wait 74882 usec)
  Server:
    Inference count: 24240
    Execution count: 52
    Successful request count: 52
    Avg request latency: 73515 usec (overhead 421 usec + queue 33247 usec + compute input 2212 usec + compute infer 7397 usec + compute output 30238 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 112, throughput: 23312 infer/sec, latency 74963 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 120 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 120
  Client:
    Request count: 1462
    Throughput: 23392 infer/sec
    Avg latency: 80705 usec (standard deviation 9173 usec)
    p50 latency: 83219 usec
    p90 latency: 85652 usec
    p95 latency: 86954 usec
    p99 latency: 91028 usec
    Avg HTTP time: 80709 usec (send/recv 61 usec + response wait 80648 usec)
  Server:
    Inference count: 23984
    Execution count: 48
    Successful request count: 48
    Avg request latency: 77853 usec (overhead 328 usec + queue 33885 usec + compute input 3171 usec + compute infer 8344 usec + compute output 32125 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 120, throughput: 23392 infer/sec, latency 80705 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 128 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 128
  Client:
    Request count: 1420
    Throughput: 22720 infer/sec
    Avg latency: 84802 usec (standard deviation 10672 usec)
    p50 latency: 86765 usec
    p90 latency: 92102 usec
    p95 latency: 96557 usec
    p99 latency: 101844 usec
    Avg HTTP time: 84702 usec (send/recv 70 usec + response wait 84632 usec)
  Server:
    Inference count: 24272
    Execution count: 46
    Successful request count: 46
    Avg request latency: 82961 usec (overhead 326 usec + queue 38326 usec + compute input 2991 usec + compute infer 7296 usec + compute output 34022 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 128, throughput: 22720 infer/sec, latency 84802 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 136 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 136
  Client:
    Request count: 1409
    Throughput: 22544 infer/sec
    Avg latency: 91121 usec (standard deviation 9819 usec)
    p50 latency: 93782 usec
    p90 latency: 97651 usec
    p95 latency: 100433 usec
    p99 latency: 111971 usec
    Avg HTTP time: 91063 usec (send/recv 139 usec + response wait 90924 usec)
  Server:
    Inference count: 23568
    Execution count: 42
    Successful request count: 42
    Avg request latency: 89654 usec (overhead 463 usec + queue 43963 usec + compute input 4372 usec + compute infer 6931 usec + compute output 33925 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 136, throughput: 22544 infer/sec, latency 91121 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 144 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 144
  Client:
    Request count: 1474
    Throughput: 23560.4 infer/sec
    Avg latency: 93830 usec (standard deviation 9618 usec)
    p50 latency: 96063 usec
    p90 latency: 97693 usec
    p95 latency: 98010 usec
    p99 latency: 98184 usec
    Avg HTTP time: 93834 usec (send/recv 91 usec + response wait 93743 usec)
  Server:
    Inference count: 24832
    Execution count: 42
    Successful request count: 42
    Avg request latency: 92186 usec (overhead 340 usec + queue 44060 usec + compute input 2720 usec + compute infer 7745 usec + compute output 37321 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 144, throughput: 23560.4 infer/sec, latency 93830 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 152 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 152
  Client:
    Request count: 1446
    Throughput: 23136 infer/sec
    Avg latency: 101401 usec (standard deviation 11389 usec)
    p50 latency: 103350 usec
    p90 latency: 109388 usec
    p95 latency: 119128 usec
    p99 latency: 123910 usec
    Avg HTTP time: 101277 usec (send/recv 107 usec + response wait 101170 usec)
  Server:
    Inference count: 24256
    Execution count: 39
    Successful request count: 39
    Avg request latency: 99040 usec (overhead 414 usec + queue 47927 usec + compute input 6416 usec + compute infer 6632 usec + compute output 37651 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 152, throughput: 23136 infer/sec, latency 101401 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 160 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 160
  Client:
    Request count: 1416
    Throughput: 22656 infer/sec
    Avg latency: 106295 usec (standard deviation 10478 usec)
    p50 latency: 109910 usec
    p90 latency: 111259 usec
    p95 latency: 111972 usec
    p99 latency: 123602 usec
    Avg HTTP time: 106517 usec (send/recv 209 usec + response wait 106308 usec)
  Server:
    Inference count: 23744
    Execution count: 36
    Successful request count: 36
    Avg request latency: 104553 usec (overhead 412 usec + queue 50933 usec + compute input 9257 usec + compute infer 6305 usec + compute output 37646 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 160, throughput: 22656 infer/sec, latency 106295 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 168 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 168
  Client:
    Request count: 1478
    Throughput: 23648 infer/sec
    Avg latency: 108439 usec (standard deviation 11509 usec)
    p50 latency: 110700 usec
    p90 latency: 114022 usec
    p95 latency: 119488 usec
    p99 latency: 123949 usec
    Avg HTTP time: 108638 usec (send/recv 311 usec + response wait 108327 usec)
  Server:
    Inference count: 24896
    Execution count: 36
    Successful request count: 36
    Avg request latency: 107027 usec (overhead 442 usec + queue 49421 usec + compute input 4821 usec + compute infer 7405 usec + compute output 44938 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 168, throughput: 23648 infer/sec, latency 108439 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 176 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 176
  Client:
    Request count: 1391
    Throughput: 22256 infer/sec
    Avg latency: 117948 usec (standard deviation 11207 usec)
    p50 latency: 120062 usec
    p90 latency: 125704 usec
    p95 latency: 135922 usec
    p99 latency: 143769 usec
    Avg HTTP time: 118047 usec (send/recv 292 usec + response wait 117755 usec)
  Server:
    Inference count: 23936
    Execution count: 33
    Successful request count: 33
    Avg request latency: 116131 usec (overhead 427 usec + queue 55773 usec + compute input 13321 usec + compute infer 5912 usec + compute output 40698 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 176, throughput: 22256 infer/sec, latency 117948 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 184 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 184
  Client:
    Request count: 1372
    Throughput: 21952 infer/sec
    Avg latency: 120181 usec (standard deviation 9802 usec)
    p50 latency: 122531 usec
    p90 latency: 129118 usec
    p95 latency: 130305 usec
    p99 latency: 137087 usec
    Avg HTTP time: 120089 usec (send/recv 240 usec + response wait 119849 usec)
  Server:
    Inference count: 23904
    Execution count: 32
    Successful request count: 32
    Avg request latency: 117514 usec (overhead 481 usec + queue 58398 usec + compute input 5949 usec + compute infer 6599 usec + compute output 46087 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 184, throughput: 21952 infer/sec, latency 120181 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 192 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 192
  Client:
    Request count: 1364
    Throughput: 21824 infer/sec
    Avg latency: 126896 usec (standard deviation 14246 usec)
    p50 latency: 130327 usec
    p90 latency: 139456 usec
    p95 latency: 148917 usec
    p99 latency: 151106 usec
    Avg HTTP time: 126910 usec (send/recv 383 usec + response wait 126527 usec)
  Server:
    Inference count: 23664
    Execution count: 30
    Successful request count: 30
    Avg request latency: 124851 usec (overhead 559 usec + queue 58537 usec + compute input 17882 usec + compute infer 5696 usec + compute output 42177 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 192, throughput: 21824 infer/sec, latency 126896 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 200 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 200
  Client:
    Request count: 1376
    Throughput: 22016 infer/sec
    Avg latency: 132325 usec (standard deviation 9743 usec)
    p50 latency: 135614 usec
    p90 latency: 137229 usec
    p95 latency: 137559 usec
    p99 latency: 137950 usec
    Avg HTTP time: 132708 usec (send/recv 280 usec + response wait 132428 usec)
  Server:
    Inference count: 24368
    Execution count: 30
    Successful request count: 30
    Avg request latency: 129866 usec (overhead 488 usec + queue 61257 usec + compute input 18609 usec + compute infer 5721 usec + compute output 43791 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 200, throughput: 22016 infer/sec, latency 132325 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 208 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 208
  Client:
    Request count: 1376
    Throughput: 22016 infer/sec
    Avg latency: 135261 usec (standard deviation 12401 usec)
    p50 latency: 139143 usec
    p90 latency: 141468 usec
    p95 latency: 152773 usec
    p99 latency: 160842 usec
    Avg HTTP time: 135959 usec (send/recv 508 usec + response wait 135451 usec)
  Server:
    Inference count: 24416
    Execution count: 29
    Successful request count: 29
    Avg request latency: 133866 usec (overhead 463 usec + queue 64641 usec + compute input 17109 usec + compute infer 6022 usec + compute output 45631 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 208, throughput: 22016 infer/sec, latency 135261 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 216 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 216
  Client:
    Request count: 1369
    Throughput: 21904 infer/sec
    Avg latency: 142090 usec (standard deviation 10948 usec)
    p50 latency: 143880 usec
    p90 latency: 149673 usec
    p95 latency: 160174 usec
    p99 latency: 165792 usec
    Avg HTTP time: 141793 usec (send/recv 442 usec + response wait 141351 usec)
  Server:
    Inference count: 24512
    Execution count: 28
    Successful request count: 28
    Avg request latency: 138987 usec (overhead 551 usec + queue 68049 usec + compute input 16702 usec + compute infer 5943 usec + compute output 47742 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 216, throughput: 21904 infer/sec, latency 142090 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 224 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 224
  Client:
    Request count: 1400
    Throughput: 22377.6 infer/sec
    Avg latency: 147125 usec (standard deviation 11057 usec)
    p50 latency: 149661 usec
    p90 latency: 151706 usec
    p95 latency: 155744 usec
    p99 latency: 169304 usec
    Avg HTTP time: 147132 usec (send/recv 536 usec + response wait 146596 usec)
  Server:
    Inference count: 23632
    Execution count: 26
    Successful request count: 26
    Avg request latency: 144654 usec (overhead 526 usec + queue 69960 usec + compute input 21470 usec + compute infer 5878 usec + compute output 46820 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 224, throughput: 22377.6 infer/sec, latency 147125 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 232 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 232
  Client:
    Request count: 1409
    Throughput: 22544 infer/sec
    Avg latency: 151663 usec (standard deviation 10106 usec)
    p50 latency: 153333 usec
    p90 latency: 159520 usec
    p95 latency: 162103 usec
    p99 latency: 164740 usec
    Avg HTTP time: 151700 usec (send/recv 727 usec + response wait 150973 usec)
  Server:
    Inference count: 24496
    Execution count: 26
    Successful request count: 26
    Avg request latency: 149319 usec (overhead 523 usec + queue 72340 usec + compute input 19149 usec + compute infer 6248 usec + compute output 51059 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 232, throughput: 22544 infer/sec, latency 151663 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 240 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 240
  Client:
    Request count: 1384
    Throughput: 22144 infer/sec
    Avg latency: 159603 usec (standard deviation 5573 usec)
    p50 latency: 160268 usec
    p90 latency: 162840 usec
    p95 latency: 164126 usec
    p99 latency: 164797 usec
    Avg HTTP time: 159532 usec (send/recv 593 usec + response wait 158939 usec)
  Server:
    Inference count: 23920
    Execution count: 31
    Successful request count: 31
    Avg request latency: 157229 usec (overhead 558 usec + queue 92610 usec + compute input 20292 usec + compute infer 5202 usec + compute output 38567 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 240, throughput: 22144 infer/sec, latency 159603 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 248 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 248
  Client:
    Request count: 1382
    Throughput: 22112 infer/sec
    Avg latency: 163815 usec (standard deviation 5760 usec)
    p50 latency: 164385 usec
    p90 latency: 167278 usec
    p95 latency: 168291 usec
    p99 latency: 169167 usec
    Avg HTTP time: 163830 usec (send/recv 545 usec + response wait 163285 usec)
  Server:
    Inference count: 23856
    Execution count: 30
    Successful request count: 30
    Avg request latency: 161023 usec (overhead 580 usec + queue 94225 usec + compute input 20911 usec + compute infer 5128 usec + compute output 40179 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 248, throughput: 22112 infer/sec, latency 163815 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 256 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 256
  Client:
    Request count: 1421
    Throughput: 22736 infer/sec
    Avg latency: 168366 usec (standard deviation 13386 usec)
    p50 latency: 169446 usec
    p90 latency: 175654 usec
    p95 latency: 185160 usec
    p99 latency: 204738 usec
    Avg HTTP time: 167916 usec (send/recv 715 usec + response wait 167201 usec)
  Server:
    Inference count: 24656
    Execution count: 31
    Successful request count: 31
    Avg request latency: 165245 usec (overhead 614 usec + queue 96889 usec + compute input 16707 usec + compute infer 6074 usec + compute output 44961 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 256, throughput: 22736 infer/sec, latency 168366 usec
  Batch    Concurrency    Inferences/Second    Client Send    Network+Server Send/Recv    Server Queue    Server Compute Input    Server Compute Infer    Server Compute Output    Client Recv    p50 latency    p90 latency    p95 latency    p99 latency    avg latency
-------  -------------  -------------------  -------------  --------------------------  --------------  ----------------------  ----------------------  -----------------------  -------------  -------------  -------------  -------------  -------------  -------------
     16              8               8160               45                         321            6175                     212                    6420                     2273              0          17527          17863          18009          18171          15446
     16             16              13328               40                         344            7204                     439                    6584                     4328              0          21924          22781          22973          23346          18939
     16             24              16272               48                         376            9280                     639                    6683                     6075              0          25627          26342          26491          26811          23101
     16             32              18480               44                         431           11317                     777                    6917                     7899              0          29959          30637          30788          31202          27385
     16             40              19472               48                         854           12083                     783                    6997                    11283              0          34475          35242          35629          36166          32048
     16             48              20544               51                         908           15631                     822                    7293                    11660              0          38660          39298          39542          40521          36365
     16             56              20928               49                        1225           17203                     891                    7466                    14434              0          43614          44358          44582          45064          41268
     16             64              21936               50                        1450           20504                     818                    7420                    15655              0          47998          48148          48176          48609          45897
     16             72              21872               67                        1246           22420                    1291                    7048                    18609              0          52923          54825          55308          56675          50681
     16             80              22624               61                         837           25330                    1523                    7217                    20283              0          57349          58190          58575          59103          55251
     16             88              22368               62                        1711           25227                    1643                    7424                    24123              0          62197          63032          63619          64212          60190
     16             96              23072               68                        1625           29505                    1534                    7414                    25043              0          67372          68853          70217          70550          65189
     16            104              22368               68                        1858           31392                    2095                    7407                    27238              0          72235          73262          73894          75046          70058
     16            112              23312               69                        1798           33247                    2212                    7397                    30238              0          77194          78851          79834          82855          74961
     16            120              23392               61                        3115           33885                    3171                    8344                    32125              0          83219          85652          86954          91028          80701
     16            128              22720               70                        2095           38326                    2991                    7296                    34022              0          86765          92102          96557         101844          84800
     16            136              22544              139                        1788           43963                    4372                    6931                    33925              0          93782          97651         100433         111971          91118
     16            144              23560.4             91                        1891           44060                    2720                    7745                    37321              0          96063          97693          98010          98184          93828
     16            152              23136              107                        2665           47927                    6416                    6632                    37651              0         103350         109388         119128         123910         101398
     16            160              22656              209                        1942           50933                    9257                    6305                    37646              0         109910         111259         111972         123602         106292
     16            168              23648              311                        1540           49421                    4821                    7405                    44938              0         110700         114022         119488         123949         108436
     16            176              22256              292                        1948           55773                   13321                    5912                    40698              0         120062         125704         135922         143769         117944
     16            184              21952              240                        2906           58398                    5949                    6599                    46087              0         122531         129118         130305         137087         120179
     16            192              21824              383                        2219           58537                   17882                    5696                    42177              0         130327         139456         148917         151106         126894
     16            200              22016              280                        2665           61257                   18609                    5721                    43791              0         135614         137229         137559         137950         132323
     16            208              22016              508                        1348           64641                   17109                    6022                    45631              0         139143         141468         152773         160842         135259
     16            216              21904              442                        3208           68049                   16702                    5943                    47742              0         143880         149673         160174         165792         142086
     16            224              22377.6            536                        2457           69960                   21470                    5878                    46820              0         149661         151706         155744         169304         147121
     16            232              22544              727                        2137           72340                   19149                    6248                    51059              0         153333         159520         162103         164740         151660
     16            240              22144              593                        2335           92610                   20292                    5202                    38567              0         160268         162840         164126         164797         159599
     16            248              22112              545                        2825           94225                   20911                    5128                    40179              0         164385         167278         168291         169167         163813
     16            256              22736              715                        3016           96889                   16707                    6074                    44961              0         169446         175654         185160         204738         168362
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ [[ ts-trace == \t\s\-\t\r\a\c\e ]]
+ export FORMAT_SUFFIX=pt
+ FORMAT_SUFFIX=pt
+ python3 triton/export_model.py --input-path triton/model.py --input-type pyt --output-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.pt --output-type ts-trace --ignore-unknown-parameters --onnx-opset 13 --checkpoint /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/traffic_bin/ --precision fp32 --dataloader triton/dataloader.py --dataset datasets/traffic_bin --batch-size 1
Got additional args ['--onnx-opset', '13']
2023-03-04 04:46:19,550 INFO export_model args:
2023-03-04 04:46:19,550 INFO export_model     input_path = triton/model.py
2023-03-04 04:46:19,550 INFO export_model     input_type = pyt
2023-03-04 04:46:19,550 INFO export_model     output_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.pt
2023-03-04 04:46:19,550 INFO export_model     output_type = ts-trace
2023-03-04 04:46:19,550 INFO export_model     dataloader = triton/dataloader.py
2023-03-04 04:46:19,551 INFO export_model     verbose = False
2023-03-04 04:46:19,551 INFO export_model     ignore_unknown_parameters = True
2023-03-04 04:46:19,551 INFO export_model     precision = fp32
2023-03-04 04:46:19,551 INFO export_model     dataset = datasets/traffic_bin
2023-03-04 04:46:19,551 INFO export_model     checkpoint = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/traffic_bin/
2023-03-04 04:46:19,551 INFO export_model     batch_size = 1
2023-03-04 04:46:19,551 INFO triton.deployment_toolkit.args Initializing get_dataloader_fn({'dataset': 'datasets/traffic_bin', 'checkpoint': '/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/traffic_bin/', 'batch_size': '1'})
2023-03-04 04:46:21,203 INFO triton.deployment_toolkit.args Initializing PyTorchModelLoader({'checkpoint': '/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/traffic_bin/', 'precision': 'fp32'})
triton/model.py:33: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['s_cat'] = s_cat if s_cat.shape[1] != 1 else None
triton/model.py:34: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['s_cont'] = s_cont if s_cont.shape[1] != 1 else None
triton/model.py:35: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['k_cat'] = k_cat if k_cat.shape[1] != 1 else None
triton/model.py:36: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['k_cont'] = k_cont if k_cont.shape[1] != 1 else None
triton/model.py:37: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['o_cat'] = o_cat if o_cat.shape[1] != 1 else None
triton/model.py:38: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  wrapped_input['o_cont'] = o_cont if o_cont.shape[1] != 1 else None
2023-03-04 04:46:23,420 INFO export_model inputs: {'s_cat__0': TensorSpec(name='s_cat__0', shape=(-1, 192, 1), dtype=dtype('int64')), 's_cont__1': TensorSpec(name='s_cont__1', shape=(-1, 1), dtype=dtype('float32')), 'k_cat__2': TensorSpec(name='k_cat__2', shape=(-1, 1), dtype=dtype('float32')), 'k_cont__3': TensorSpec(name='k_cont__3', shape=(-1, 192, 3), dtype=dtype('float32')), 'o_cat__4': TensorSpec(name='o_cat__4', shape=(-1, 1), dtype=dtype('float32')), 'o_cont__5': TensorSpec(name='o_cont__5', shape=(-1, 1), dtype=dtype('float32')), 'target__6': TensorSpec(name='target__6', shape=(-1, 192, 1), dtype=dtype('float32')), 'id__7': TensorSpec(name='id__7', shape=(-1, 192, 1), dtype=dtype('int64'))}
2023-03-04 04:46:23,421 INFO export_model outputs: {'target__0': TensorSpec(name='target__0', shape=(-1, 24, 3), dtype=dtype('float32'))}
2023-03-04 04:46:23,423 INFO triton.deployment_toolkit.args Initializing TorchScriptSaver({})
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ [[ ts-trace == \t\s\-\t\r\a\c\e ]]
+ export FORMAT_SUFFIX=pt
+ FORMAT_SUFFIX=pt
+ model-navigator convert --model-name TFT --model-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.pt --output-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model --target-formats ts-trace --target-precisions fp16 --launch-mode local --override-workspace --verbose --onnx-opsets 13 --max-batch-size 1024 --container-version 21.08 --max-workspace-size 10000000000 --atol target__0=100 --rtol target__0=100
2023-03-04 04:46:25 - DEBUG - model_navigator.log: initialized logger verbose=True
2023-03-04 04:46:25 - DEBUG - convert: Running 'model-navigator convert' with config_path: None
2023-03-04 04:46:25 - INFO - model_navigator.log: Environment state
2023-03-04 04:46:25 - INFO - model_navigator.log: 	docker_container_id = 98c4b3fc07
2023-03-04 04:46:25 - INFO - model_navigator.log: 	docker_container_ip_address = None
2023-03-04 04:46:25 - INFO - model_navigator.log: 	docker_container_mounts = [{'host_path': PosixPath('/var/run/docker.sock'), 'container_path': PosixPath('/var/run/docker.sock'), 'mount_type': 'bind'}, {'host_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/dataset/processed'), 'container_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/datasets'), 'mount_type': 'bind'}, {'host_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT'), 'container_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT'), 'mount_type': 'bind'}]
2023-03-04 04:46:25 - DEBUG - model_navigator.validators: Running command validator: <model_navigator.validators.ModelNavigatorDockerContainerShouldHaveMountedWorkspaceDir object at 0x7f14ec75d190>
2023-03-04 04:46:25 - DEBUG - model_navigator.validators: ModelNavigatorDockerContainerShouldHaveMountedWorkspaceDir is_running_in_container=True has_mounted_workspace_path=True is_running_converter_in_docker=False
2023-03-04 04:46:25 - DEBUG - model_navigator.utils.workspace: Workspace path /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:46:25 - INFO - model_navigator.log: convert args:
2023-03-04 04:46:25 - INFO - model_navigator.log: 	model_name = TFT
2023-03-04 04:46:25 - INFO - model_navigator.log: 	model_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.pt
2023-03-04 04:46:25 - INFO - model_navigator.log: 	model_format = None
2023-03-04 04:46:25 - INFO - model_navigator.log: 	model_version = 1
2023-03-04 04:46:25 - INFO - model_navigator.log: 	target_formats = [<Format.TORCHSCRIPT: 'torchscript'>]
2023-03-04 04:46:25 - INFO - model_navigator.log: 	onnx_opsets = [13]
2023-03-04 04:46:25 - INFO - model_navigator.log: 	tensorrt_precisions = [<TensorRTPrecision.FP16: 'fp16'>]
2023-03-04 04:46:25 - INFO - model_navigator.log: 	tensorrt_precisions_mode = TensorRTPrecisionMode.HIERARCHY
2023-03-04 04:46:25 - INFO - model_navigator.log: 	tensorrt_explicit_precision = False
2023-03-04 04:46:25 - INFO - model_navigator.log: 	tensorrt_strict_types = False
2023-03-04 04:46:25 - INFO - model_navigator.log: 	tensorrt_sparse_weights = False
2023-03-04 04:46:25 - INFO - model_navigator.log: 	tensorrt_max_workspace_size = 10000000000
2023-03-04 04:46:25 - INFO - model_navigator.log: 	atol = {'target__0': 100.0}
2023-03-04 04:46:25 - INFO - model_navigator.log: 	rtol = {'target__0': 100.0}
2023-03-04 04:46:25 - INFO - model_navigator.log: 	max_batch_size = 1024
2023-03-04 04:46:25 - INFO - model_navigator.log: 	inputs = None
2023-03-04 04:46:25 - INFO - model_navigator.log: 	outputs = None
2023-03-04 04:46:25 - INFO - model_navigator.log: 	min_shapes = None
2023-03-04 04:46:25 - INFO - model_navigator.log: 	opt_shapes = None
2023-03-04 04:46:25 - INFO - model_navigator.log: 	max_shapes = None
2023-03-04 04:46:25 - INFO - model_navigator.log: 	value_ranges = None
2023-03-04 04:46:25 - INFO - model_navigator.log: 	dtypes = None
2023-03-04 04:46:25 - INFO - model_navigator.log: 	workspace_path = navigator_workspace
2023-03-04 04:46:25 - INFO - model_navigator.log: 	override_workspace = True
2023-03-04 04:46:25 - INFO - model_navigator.log: 	output_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model
2023-03-04 04:46:25 - INFO - model_navigator.log: 	container_version = 21.08
2023-03-04 04:46:25 - INFO - model_navigator.log: 	framework_docker_image = nvcr.io/nvidia/pytorch:21.08-py3
2023-03-04 04:46:25 - INFO - model_navigator.log: 	gpus = ('all',)
2023-03-04 04:46:25 - DEBUG - model_navigator.utils.workspace: Cleaning workspace dir /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:46:25 - DEBUG - model_navigator.converter.convert: Converter created; workspace=/home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:46:25 - DEBUG - model_navigator.converter.convert: Convert Commands Executor created; workspace=/home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:46:25 - DEBUG - model_navigator.results: Saving results of convert_model stage into /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/convert_model_results.yaml
2023-03-04 04:46:25 - DEBUG - model_navigator.results: ConversionResult(status=Status(state=<State.SUCCEEDED: 'succeeded'>, message='Source model', log_path=None), source_model_config=ModelConfig(model_name='TFT', model_path=PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.pt'), model_format=None, model_version='1'), conversion_config=ConversionConfig(target_format=<Format.TORCHSCRIPT: 'torchscript'>, onnx_opset=None, tensorrt_precision=None, tensorrt_precision_mode=<TensorRTPrecisionMode.HIERARCHY: 'hierarchy'>, tensorrt_explicit_precision=False, tensorrt_strict_types=False, tensorrt_sparse_weights=False), tensorrt_common_config=None, model_signature_config=None, comparator_config=None, dataset_profile=None, output_model=Model(name='TFT', path=PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.pt'), format=<Format.TORCHSCRIPT: 'torchscript'>, signature=ModelSignatureConfig(inputs={'s_cat__0': TensorSpec(name='s_cat__0', shape=(-1, 192, 1), dtype=dtype('int64')), 's_cont__1': TensorSpec(name='s_cont__1', shape=(-1, 1), dtype=dtype('float32')), 'k_cat__2': TensorSpec(name='k_cat__2', shape=(-1, 1), dtype=dtype('float32')), 'k_cont__3': TensorSpec(name='k_cont__3', shape=(-1, 192, 3), dtype=dtype('float32')), 'o_cat__4': TensorSpec(name='o_cat__4', shape=(-1, 1), dtype=dtype('float32')), 'o_cont__5': TensorSpec(name='o_cont__5', shape=(-1, 1), dtype=dtype('float32')), 'target__6': TensorSpec(name='target__6', shape=(-1, 192, 1), dtype=dtype('float32')), 'id__7': TensorSpec(name='id__7', shape=(-1, 192, 1), dtype=dtype('int64'))}, outputs={'target__0': TensorSpec(name='target__0', shape=(-1, 24, 3), dtype=dtype('float32'))}), properties=TorchScriptProperties(), num_required_gpus=None), framework_docker_image=None)
2023-03-04 04:46:25 - DEBUG - convert: Copy /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.pt to /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model
2023-03-04 04:46:25 - DEBUG - convert: Copy /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/exported_model.pt.yaml to /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model.yaml
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ [[ ts-trace == \t\s\-\t\r\a\c\e ]]
+ export CONFIG_FORMAT=torchscript
+ CONFIG_FORMAT=torchscript
+ model-navigator triton-config-model --model-repository /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models --model-name TFT --model-version 1 --model-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model --model-format torchscript --model-control-mode explicit --load-model --load-model-timeout-s 100 --verbose --backend-accelerator none --tensorrt-precision fp16 --tensorrt-capture-cuda-graph --tensorrt-max-workspace-size 10000000000 --max-batch-size 1024 --batching dynamic --preferred-batch-sizes 512 1024 --max-queue-delay-us 1 --engine-count-per-device gpu=2
2023-03-04 04:46:25 - DEBUG - model_navigator.log: initialized logger verbose=True
2023-03-04 04:46:25 - DEBUG - triton_config_model: Running 'model-navigator triton-config-model' with config_path: None
2023-03-04 04:46:25 - INFO - model_navigator.log: Environment state
2023-03-04 04:46:25 - INFO - model_navigator.log: 	docker_container_id = 98c4b3fc07
2023-03-04 04:46:25 - INFO - model_navigator.log: 	docker_container_ip_address = None
2023-03-04 04:46:25 - INFO - model_navigator.log: 	docker_container_mounts = [{'host_path': PosixPath('/var/run/docker.sock'), 'container_path': PosixPath('/var/run/docker.sock'), 'mount_type': 'bind'}, {'host_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/dataset/processed'), 'container_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/datasets'), 'mount_type': 'bind'}, {'host_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT'), 'container_path': PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT'), 'mount_type': 'bind'}]
2023-03-04 04:46:25 - DEBUG - model_navigator.validators: Running command validator: <model_navigator.validators.ModelNavigatorBatchingConfiguration object at 0x7f018d2d7a90>
2023-03-04 04:46:25 - DEBUG - model_navigator.validators: ModelNavigatorBatchingConfiguration 
batching=Batching.DYNAMIC 
max_batch_size=1024 
2023-03-04 04:46:25 - INFO - model_navigator.log: triton-config-model args:
2023-03-04 04:46:25 - INFO - model_navigator.log: 	model_name = TFT
2023-03-04 04:46:25 - INFO - model_navigator.log: 	model_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model
2023-03-04 04:46:25 - INFO - model_navigator.log: 	model_format = Format.TORCHSCRIPT
2023-03-04 04:46:25 - INFO - model_navigator.log: 	model_version = 1
2023-03-04 04:46:25 - INFO - model_navigator.log: 	model_repository = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models
2023-03-04 04:46:25 - INFO - model_navigator.log: 	load_model = True
2023-03-04 04:46:25 - INFO - model_navigator.log: 	load_model_timeout_s = 100
2023-03-04 04:46:25 - INFO - model_navigator.log: 	model_control_mode = ModelControlMode.EXPLICIT
2023-03-04 04:46:25 - INFO - model_navigator.log: 	inputs = None
2023-03-04 04:46:25 - INFO - model_navigator.log: 	outputs = None
2023-03-04 04:46:25 - INFO - model_navigator.log: 	max_batch_size = 1024
2023-03-04 04:46:25 - INFO - model_navigator.log: 	batching = Batching.DYNAMIC
2023-03-04 04:46:25 - INFO - model_navigator.log: 	backend_accelerator = BackendAccelerator.NONE
2023-03-04 04:46:25 - INFO - model_navigator.log: 	tensorrt_precision = TensorRTOptPrecision.FP16
2023-03-04 04:46:25 - INFO - model_navigator.log: 	tensorrt_capture_cuda_graph = True
2023-03-04 04:46:25 - INFO - model_navigator.log: 	tensorrt_max_workspace_size = 10000000000
2023-03-04 04:46:25 - INFO - model_navigator.log: 	preferred_batch_sizes = [512, 1024]
2023-03-04 04:46:25 - INFO - model_navigator.log: 	max_queue_delay_us = 1
2023-03-04 04:46:25 - INFO - model_navigator.log: 	engine_count_per_device = {<DeviceKind.GPU: 'gpu'>: 2}
2023-03-04 04:46:25 - INFO - model_navigator.log: 	triton_backend_parameters = {}
2023-03-04 04:46:25 - INFO - model_navigator.log: 	server_url = grpc://localhost:8001
2023-03-04 04:46:25 - INFO - model_navigator.log: model:
2023-03-04 04:46:25 - INFO - model_navigator.log: 	name = TFT
2023-03-04 04:46:25 - INFO - model_navigator.log: 	path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model
2023-03-04 04:46:25 - INFO - model_navigator.log: 	format = Format.TORCHSCRIPT
2023-03-04 04:46:25 - INFO - model_navigator.log: 	signature = {'inputs': {'s_cat__0': {'name': 's_cat__0', 'shape': (-1, 192, 1), 'dtype': dtype('int64')}, 's_cont__1': {'name': 's_cont__1', 'shape': (-1, 1), 'dtype': dtype('float32')}, 'k_cat__2': {'name': 'k_cat__2', 'shape': (-1, 1), 'dtype': dtype('float32')}, 'k_cont__3': {'name': 'k_cont__3', 'shape': (-1, 192, 3), 'dtype': dtype('float32')}, 'o_cat__4': {'name': 'o_cat__4', 'shape': (-1, 1), 'dtype': dtype('float32')}, 'o_cont__5': {'name': 'o_cont__5', 'shape': (-1, 1), 'dtype': dtype('float32')}, 'target__6': {'name': 'target__6', 'shape': (-1, 192, 1), 'dtype': dtype('float32')}, 'id__7': {'name': 'id__7', 'shape': (-1, 192, 1), 'dtype': dtype('int64')}}, 'outputs': {'target__0': {'name': 'target__0', 'shape': (-1, 24, 3), 'dtype': dtype('float32')}}}
2023-03-04 04:46:25 - INFO - model_navigator.log: 	properties = {}
2023-03-04 04:46:25 - INFO - model_navigator.log: 	num_required_gpus = None
2023-03-04 04:46:25 - DEBUG - model_navigator.triton.model_store: Deploying model /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model in Triton Model Store /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models with: 
batching config: TritonBatchingConfig(max_batch_size=1024, batching=<Batching.DYNAMIC: 'dynamic'>)
optimization config: TritonModelOptimizationConfig(backend_accelerator=<BackendAccelerator.NONE: 'none'>, tensorrt_precision=<TensorRTOptPrecision.FP16: 'fp16'>, tensorrt_capture_cuda_graph=True)
dynamic batching config: TritonDynamicBatchingConfig(preferred_batch_sizes=[512, 1024], max_queue_delay_us=1)
2023-03-04 04:46:25 - DEBUG - model_navigator.triton.model_store: Copying /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model to /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models/TFT/1/model.pt
2023-03-04 04:46:25 - DEBUG - model_navigator.triton.model_config: Generated Triton config:
name: "TFT"
max_batch_size: 1024
input {
  name: "s_cat__0"
  data_type: TYPE_INT64
  dims: 192
  dims: 1
}
input {
  name: "s_cont__1"
  data_type: TYPE_FP32
  dims: 1
}
input {
  name: "k_cat__2"
  data_type: TYPE_FP32
  dims: 1
}
input {
  name: "k_cont__3"
  data_type: TYPE_FP32
  dims: 192
  dims: 3
}
input {
  name: "o_cat__4"
  data_type: TYPE_FP32
  dims: 1
}
input {
  name: "o_cont__5"
  data_type: TYPE_FP32
  dims: 1
}
input {
  name: "target__6"
  data_type: TYPE_FP32
  dims: 192
  dims: 1
}
input {
  name: "id__7"
  data_type: TYPE_INT64
  dims: 192
  dims: 1
}
output {
  name: "target__0"
  data_type: TYPE_FP32
  dims: 24
  dims: 3
}
instance_group {
  count: 2
  kind: KIND_GPU
}
dynamic_batching {
  preferred_batch_size: 512
  preferred_batch_size: 1024
  max_queue_delay_microseconds: 1
}
backend: "pytorch"

2023-03-04 04:46:25 - DEBUG - triton_config_model: Loading model TFT:1 model_control_mode: ModelControlMode.EXPLICIT
2023-03-04 04:46:25 - DEBUG - model_navigator.triton.client: Connecting to grpc://localhost:8001
2023-03-04 04:46:25 - DEBUG - triton_config_model: Waiting for server (timeout=5s)
is_server_ready, metadata ()

ready: true

is_server_live, metadata ()

live: true

2023-03-04 04:46:25 - DEBUG - triton_config_model: Sending load_model request
load_model, metadata ()
override files omitted:
model_name: "TFT"

Loaded model 'TFT'
2023-03-04 04:46:27 - DEBUG - triton_config_model: Polling for model availability (timeout=100s)
get_model_repository_index, metadata ()

models {
  name: "TFT"
  version: "1"
  state: "READY"
}

is_model_ready, metadata ()
name: "TFT"

ready: true

get_model_metadata, metadata ()
name: "TFT"

name: "TFT"
versions: "1"
platform: "pytorch_libtorch"
inputs {
  name: "s_cat__0"
  datatype: "INT64"
  shape: -1
  shape: 192
  shape: 1
}
inputs {
  name: "s_cont__1"
  datatype: "FP32"
  shape: -1
  shape: 1
}
inputs {
  name: "k_cat__2"
  datatype: "FP32"
  shape: -1
  shape: 1
}
inputs {
  name: "k_cont__3"
  datatype: "FP32"
  shape: -1
  shape: 192
  shape: 3
}
inputs {
  name: "o_cat__4"
  datatype: "FP32"
  shape: -1
  shape: 1
}
inputs {
  name: "o_cont__5"
  datatype: "FP32"
  shape: -1
  shape: 1
}
inputs {
  name: "target__6"
  datatype: "FP32"
  shape: -1
  shape: 192
  shape: 1
}
inputs {
  name: "id__7"
  datatype: "INT64"
  shape: -1
  shape: 192
  shape: 1
}
outputs {
  name: "target__0"
  datatype: "FP32"
  shape: -1
  shape: 24
  shape: 3
}

2023-03-04 04:46:27 - DEBUG - model_navigator.utils.workspace: Workspace path /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace
2023-03-04 04:46:27 - DEBUG - model_navigator.results: Saving results of triton_config_model stage into /home/cc/DLEx/PyTorch/Forecasting/TFT/navigator_workspace/triton_config_model_results.yaml
2023-03-04 04:46:27 - DEBUG - model_navigator.results: ConfigModelResult(status=Status(state=<State.SUCCEEDED: 'succeeded'>, message='Model configured and loaded correctly', log_path=None), model_config=ModelConfig(model_name='TFT', model_path=PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model'), model_format=<Format.TORCHSCRIPT: 'torchscript'>, model_version='1'), model_version='1', batching_config=TritonBatchingConfig(max_batch_size=1024, batching=<Batching.DYNAMIC: 'dynamic'>), optimization_config=TritonModelOptimizationConfig(backend_accelerator=<BackendAccelerator.NONE: 'none'>, tensorrt_precision=<TensorRTOptPrecision.FP16: 'fp16'>, tensorrt_capture_cuda_graph=True), dynamic_batching_config=TritonDynamicBatchingConfig(preferred_batch_sizes=[512, 1024], max_queue_delay_us=1), instances_config=TritonModelInstancesConfig(engine_count_per_device={<DeviceKind.GPU: 'gpu'>: 2}), tensorrt_common_config=TensorRTCommonConfig(tensorrt_max_workspace_size=10000000000), model_dir_in_model_store=PosixPath('/home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models/TFT'))
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ mkdir -p /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data
+ python triton/prepare_input_data.py --input-data-dir /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/ --dataset datasets/traffic_bin --checkpoint /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/checkpoints/traffic_bin/
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ python triton/run_performance_on_triton.py --model-repository /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models --model-name TFT --input-data /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/data.json --batch-sizes 1,2,4,8,16,32,64,128,256,512,1024 --number-of-triton-instances 1 --batching-mode static --evaluation-mode offline --measurement-request-count 500 --warmup --performance-tool perf_analyzer --result-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/triton_performance_offline.csv
2023-03-04 04:46:31,449 INFO run_performance_on_triton Running warmup before the main test
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 1
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 1000
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 1081
    Throughput: 270.182 infer/sec
    Avg latency: 3694 usec (standard deviation 25 usec)
    p50 latency: 3692 usec
    p90 latency: 3716 usec
    p95 latency: 3725 usec
    p99 latency: 3738 usec
    Avg HTTP time: 3692 usec (send/recv 13 usec + response wait 3679 usec)
  Server:
    Inference count: 1082
    Execution count: 1082
    Successful request count: 1082
    Avg request latency: 3612 usec (overhead 25 usec + queue 13 usec + compute input 47 usec + compute infer 3521 usec + compute output 6 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 270.182 infer/sec, latency 3694 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 1024
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 1000
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 1012
    Throughput: 24671.8 infer/sec
    Avg latency: 41477 usec (standard deviation 264 usec)
    p50 latency: 41439 usec
    p90 latency: 41840 usec
    p95 latency: 41978 usec
    p99 latency: 42168 usec
    Avg HTTP time: 41473 usec (send/recv 37 usec + response wait 41436 usec)
  Server:
    Inference count: 1037312
    Execution count: 1013
    Successful request count: 1013
    Avg request latency: 41158 usec (overhead 28 usec + queue 147 usec + compute input 468 usec + compute infer 4318 usec + compute output 36197 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 24671.8 infer/sec, latency 41477 usec
2023-03-04 04:48:56,532 INFO run_performance_on_triton Using Perf Analyzer for performance evaluation
2023-03-04 04:48:56,532 INFO run_performance_on_triton Selected configuration
2023-03-04 04:48:56,532 INFO run_performance_on_triton 	server_url = http://127.0.0.1:8000
2023-03-04 04:48:56,532 INFO run_performance_on_triton 	model_name = TFT
2023-03-04 04:48:56,532 INFO run_performance_on_triton 	input_data = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/data.json
2023-03-04 04:48:56,532 INFO run_performance_on_triton 	input_shapes = []
2023-03-04 04:48:56,532 INFO run_performance_on_triton 	batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
2023-03-04 04:48:56,532 INFO run_performance_on_triton 	number_of_triton_instances = 1
2023-03-04 04:48:56,532 INFO run_performance_on_triton 	number_of_model_instances = 1
2023-03-04 04:48:56,532 INFO run_performance_on_triton 	measurement_mode = MeasurementMode.COUNT_WINDOWS
2023-03-04 04:48:56,532 INFO run_performance_on_triton 	measurement_interval = 5000
2023-03-04 04:48:56,532 INFO run_performance_on_triton 	measurement_request_count = 500
2023-03-04 04:48:56,532 INFO run_performance_on_triton 	concurrency_steps = 32
2023-03-04 04:48:56,532 INFO run_performance_on_triton 	batching_mode = BatchingMode.STATIC
2023-03-04 04:48:56,532 INFO run_performance_on_triton 	evaluation_mode = EvaluationMode.OFFLINE
2023-03-04 04:48:56,532 INFO run_performance_on_triton 	offline_mode = OfflineMode.SYSTEM
2023-03-04 04:48:56,532 INFO run_performance_on_triton 	output_shared_memory_size = 100240
2023-03-04 04:48:56,532 INFO run_performance_on_triton 	result_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/triton_performance_offline.csv
2023-03-04 04:48:56,532 INFO run_performance_on_triton 	verbose = False
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 1
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 541
    Throughput: 270.5 infer/sec
    Avg latency: 3690 usec (standard deviation 16 usec)
    p50 latency: 3690 usec
    p90 latency: 3711 usec
    p95 latency: 3718 usec
    p99 latency: 3730 usec
    Avg HTTP time: 3689 usec (send/recv 14 usec + response wait 3675 usec)
  Server:
    Inference count: 541
    Execution count: 541
    Successful request count: 541
    Avg request latency: 3608 usec (overhead 25 usec + queue 12 usec + compute input 47 usec + compute infer 3518 usec + compute output 6 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 270.5 infer/sec, latency 3690 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 2
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 731
    Throughput: 487.333 infer/sec
    Avg latency: 4098 usec (standard deviation 26 usec)
    p50 latency: 4104 usec
    p90 latency: 4130 usec
    p95 latency: 4134 usec
    p99 latency: 4142 usec
    Avg HTTP time: 4097 usec (send/recv 14 usec + response wait 4083 usec)
  Server:
    Inference count: 1464
    Execution count: 732
    Successful request count: 732
    Avg request latency: 4016 usec (overhead 25 usec + queue 13 usec + compute input 48 usec + compute infer 3684 usec + compute output 246 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 487.333 infer/sec, latency 4098 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 4
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 719
    Throughput: 958.667 infer/sec
    Avg latency: 4163 usec (standard deviation 73 usec)
    p50 latency: 4152 usec
    p90 latency: 4166 usec
    p95 latency: 4174 usec
    p99 latency: 4657 usec
    Avg HTTP time: 4162 usec (send/recv 14 usec + response wait 4148 usec)
  Server:
    Inference count: 2880
    Execution count: 720
    Successful request count: 720
    Avg request latency: 4081 usec (overhead 25 usec + queue 14 usec + compute input 50 usec + compute infer 3737 usec + compute output 255 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 958.667 infer/sec, latency 4163 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 8
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 712
    Throughput: 1898.03 infer/sec
    Avg latency: 4205 usec (standard deviation 11 usec)
    p50 latency: 4204 usec
    p90 latency: 4220 usec
    p95 latency: 4224 usec
    p99 latency: 4232 usec
    Avg HTTP time: 4203 usec (send/recv 14 usec + response wait 4189 usec)
  Server:
    Inference count: 5696
    Execution count: 712
    Successful request count: 712
    Avg request latency: 4122 usec (overhead 24 usec + queue 13 usec + compute input 53 usec + compute infer 3770 usec + compute output 262 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 1898.03 infer/sec, latency 4205 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 686
    Throughput: 3658.67 infer/sec
    Avg latency: 4366 usec (standard deviation 95 usec)
    p50 latency: 4321 usec
    p90 latency: 4535 usec
    p95 latency: 4549 usec
    p99 latency: 4586 usec
    Avg HTTP time: 4365 usec (send/recv 14 usec + response wait 4351 usec)
  Server:
    Inference count: 10992
    Execution count: 687
    Successful request count: 687
    Avg request latency: 4284 usec (overhead 26 usec + queue 44 usec + compute input 61 usec + compute infer 3805 usec + compute output 348 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 3658.67 infer/sec, latency 4366 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 32
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 625
    Throughput: 6666.67 infer/sec
    Avg latency: 4794 usec (standard deviation 100 usec)
    p50 latency: 4852 usec
    p90 latency: 4891 usec
    p95 latency: 4901 usec
    p99 latency: 4943 usec
    Avg HTTP time: 4792 usec (send/recv 13 usec + response wait 4779 usec)
  Server:
    Inference count: 20000
    Execution count: 625
    Successful request count: 625
    Avg request latency: 4712 usec (overhead 27 usec + queue 104 usec + compute input 73 usec + compute infer 4083 usec + compute output 425 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 6666.67 infer/sec, latency 4794 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 64
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 505
    Throughput: 10773.3 infer/sec
    Avg latency: 5929 usec (standard deviation 69 usec)
    p50 latency: 5958 usec
    p90 latency: 5978 usec
    p95 latency: 5986 usec
    p99 latency: 6028 usec
    Avg HTTP time: 5928 usec (send/recv 14 usec + response wait 5914 usec)
  Server:
    Inference count: 32384
    Execution count: 506
    Successful request count: 506
    Avg request latency: 5846 usec (overhead 28 usec + queue 109 usec + compute input 86 usec + compute infer 4139 usec + compute output 1484 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 10773.3 infer/sec, latency 5929 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 128
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 603
    Throughput: 15433.7 infer/sec
    Avg latency: 8283 usec (standard deviation 44 usec)
    p50 latency: 8289 usec
    p90 latency: 8320 usec
    p95 latency: 8329 usec
    p99 latency: 8360 usec
    Avg HTTP time: 8282 usec (send/recv 14 usec + response wait 8268 usec)
  Server:
    Inference count: 77184
    Execution count: 603
    Successful request count: 603
    Avg request latency: 8201 usec (overhead 28 usec + queue 151 usec + compute input 112 usec + compute infer 4145 usec + compute output 3765 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 15433.7 infer/sec, latency 8283 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 256
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 540
    Throughput: 19748.6 infer/sec
    Avg latency: 12938 usec (standard deviation 79 usec)
    p50 latency: 12932 usec
    p90 latency: 13042 usec
    p95 latency: 13067 usec
    p99 latency: 13113 usec
    Avg HTTP time: 12934 usec (send/recv 37 usec + response wait 12897 usec)
  Server:
    Inference count: 138496
    Execution count: 541
    Successful request count: 541
    Avg request latency: 12710 usec (overhead 29 usec + queue 187 usec + compute input 167 usec + compute infer 4164 usec + compute output 8163 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 19748.6 infer/sec, latency 12938 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 512
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 534
    Throughput: 22782.1 infer/sec
    Avg latency: 22451 usec (standard deviation 135 usec)
    p50 latency: 22440 usec
    p90 latency: 22648 usec
    p95 latency: 22707 usec
    p99 latency: 22783 usec
    Avg HTTP time: 22447 usec (send/recv 35 usec + response wait 22412 usec)
  Server:
    Inference count: 273920
    Execution count: 535
    Successful request count: 535
    Avg request latency: 22284 usec (overhead 28 usec + queue 200 usec + compute input 271 usec + compute infer 4184 usec + compute output 17601 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 22782.1 infer/sec, latency 22451 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 1024
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 1
  Client:
    Request count: 506
    Throughput: 24672.3 infer/sec
    Avg latency: 41458 usec (standard deviation 254 usec)
    p50 latency: 41424 usec
    p90 latency: 41821 usec
    p95 latency: 41955 usec
    p99 latency: 42133 usec
    Avg HTTP time: 41454 usec (send/recv 35 usec + response wait 41419 usec)
  Server:
    Inference count: 518144
    Execution count: 506
    Successful request count: 506
    Avg request latency: 41142 usec (overhead 29 usec + queue 150 usec + compute input 469 usec + compute infer 4322 usec + compute output 36172 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, throughput: 24672.3 infer/sec, latency 41458 usec
  Batch    Concurrency    Inferences/Second    Client Send    Network+Server Send/Recv    Server Queue    Server Compute Input    Server Compute Infer    Server Compute Output    Client Recv    p50 latency    p90 latency    p95 latency    p99 latency    avg latency
-------  -------------  -------------------  -------------  --------------------------  --------------  ----------------------  ----------------------  -----------------------  -------------  -------------  -------------  -------------  -------------  -------------
      1              1              270.5               14                          91              12                      47                    3518                        6              0           3690           3711           3718           3730           3688
      2              1              487.333             14                          91              13                      48                    3684                      246              0           4104           4130           4134           4142           4096
      4              1              958.667             14                          92              14                      50                    3737                      255              0           4152           4166           4174           4657           4162
      8              1             1898.03              14                          91              13                      53                    3770                      262              0           4204           4220           4224           4232           4203
     16              1             3658.67              14                          92              44                      61                    3805                      348              0           4321           4535           4549           4586           4364
     32              1             6666.67              13                          93             104                      73                    4083                      425              0           4852           4891           4901           4943           4791
     64              1            10773.3               14                          94             109                      86                    4139                     1484              0           5958           5978           5986           6028           5926
    128              1            15433.7               14                          95             151                     112                    4145                     3765              0           8289           8320           8329           8360           8282
    256              1            19748.6               37                         217             187                     167                    4164                     8163              0          12932          13042          13067          13113          12935
    512              1            22782.1               35                         157             200                     271                    4184                    17601              0          22440          22648          22707          22783          22448
   1024              1            24672.3               35                         307             150                     469                    4322                    36172              0          41424          41821          41955          42133          41455
+ test -f /etc/shinit_v2
+ source /etc/shinit_v2
+++ sed -n 's/^NVRM.*Kernel Module *\([^() ]*\).*$/\1/p' /proc/driver/nvidia/version
+++ sed 's/^$/unknown/'
++ NV_DRIVER_VERS=530.30.02
++ export _CUDA_COMPAT_PATH=/usr/local/cuda/compat
++ _CUDA_COMPAT_PATH=/usr/local/cuda/compat
+++ hostname
++ _CUDA_COMPAT_CHECKFILE=/usr/local/cuda/compat/.530.30.02.a100.checked
++ _CUDA_COMPAT_REALLIB=/usr/local/cuda/compat/lib.real
++ _CUDA_COMPAT_SYMLINK=/usr/local/cuda/compat/lib
++ '[' -n 530.30.02 -a -e /dev/nvidiactl -a '!' -e /usr/local/cuda/compat/.530.30.02.a100.checked ']'
+++ cat /sys/module/mlx5_core/version
+++ true
++ DETECTED_MOFED=
++ dpkg --compare-versions 0 lt 4.9
++ export 'OMPI_MCA_pml=^ucx'
++ OMPI_MCA_pml='^ucx'
++ unset _CUDA_COMPAT_CHECKFILE
++ unset _CUDA_COMPAT_REALLIB
++ unset _CUDA_COMPAT_SYMLINK
+ '[' -z '' ']'
+ return
+ set -x
+ set -e
+ export PYTHONUNBUFFERED=1
+ PYTHONUNBUFFERED=1
++ pwd
+ export PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ PYTHONPATH=/home/cc/DLEx/PyTorch/Forecasting/TFT
+ python triton/run_performance_on_triton.py --model-repository /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models --model-name TFT --input-data /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/data.json --batch-sizes 1,2,4,8,16,32,64,128,256,512,1024 --number-of-triton-instances 1 --number-of-model-instances 2 --batching-mode dynamic --evaluation-mode online --measurement-request-count 500 --warmup --performance-tool perf_analyzer --result-path /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/triton_performance_online.csv
2023-03-04 04:52:40,991 INFO run_performance_on_triton Running warmup before the main test
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 1000
  Latency limit: 0 msec
  Concurrency limit: 128 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 128
  Client:
    Request count: 1410
    Throughput: 22560 infer/sec
    Avg latency: 83956 usec (standard deviation 12579 usec)
    p50 latency: 87082 usec
    p90 latency: 94254 usec
    p95 latency: 94612 usec
    p99 latency: 97074 usec
    Avg HTTP time: 83905 usec (send/recv 99 usec + response wait 83806 usec)
  Server:
    Inference count: 23936
    Execution count: 44
    Successful request count: 44
    Avg request latency: 82825 usec (overhead 367 usec + queue 36356 usec + compute input 3175 usec + compute infer 7305 usec + compute output 35622 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 128, throughput: 22560 infer/sec, latency 83956 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 1000
  Latency limit: 0 msec
  Concurrency limit: 256 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 256
  Client:
    Request count: 1358
    Throughput: 21728 infer/sec
    Avg latency: 163994 usec (standard deviation 21457 usec)
    p50 latency: 167033 usec
    p90 latency: 194256 usec
    p95 latency: 198869 usec
    p99 latency: 200305 usec
    Avg HTTP time: 164025 usec (send/recv 880 usec + response wait 163145 usec)
  Server:
    Inference count: 24784
    Execution count: 33
    Successful request count: 33
    Avg request latency: 161515 usec (overhead 594 usec + queue 97020 usec + compute input 6971 usec + compute infer 7932 usec + compute output 48998 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 256, throughput: 21728 infer/sec, latency 163994 usec
2023-03-04 04:52:52,268 INFO run_performance_on_triton Using Perf Analyzer for performance evaluation
2023-03-04 04:52:52,268 INFO run_performance_on_triton Selected configuration
2023-03-04 04:52:52,268 INFO run_performance_on_triton 	server_url = http://127.0.0.1:8000
2023-03-04 04:52:52,268 INFO run_performance_on_triton 	model_name = TFT
2023-03-04 04:52:52,268 INFO run_performance_on_triton 	input_data = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/input_data/data.json
2023-03-04 04:52:52,268 INFO run_performance_on_triton 	input_shapes = []
2023-03-04 04:52:52,268 INFO run_performance_on_triton 	batch_sizes = [16]
2023-03-04 04:52:52,268 INFO run_performance_on_triton 	number_of_triton_instances = 1
2023-03-04 04:52:52,268 INFO run_performance_on_triton 	number_of_model_instances = 2
2023-03-04 04:52:52,268 INFO run_performance_on_triton 	measurement_mode = MeasurementMode.COUNT_WINDOWS
2023-03-04 04:52:52,268 INFO run_performance_on_triton 	measurement_interval = 5000
2023-03-04 04:52:52,268 INFO run_performance_on_triton 	measurement_request_count = 500
2023-03-04 04:52:52,268 INFO run_performance_on_triton 	concurrency_steps = 32
2023-03-04 04:52:52,268 INFO run_performance_on_triton 	batching_mode = BatchingMode.DYNAMIC
2023-03-04 04:52:52,268 INFO run_performance_on_triton 	evaluation_mode = EvaluationMode.ONLINE
2023-03-04 04:52:52,268 INFO run_performance_on_triton 	offline_mode = OfflineMode.SYSTEM
2023-03-04 04:52:52,268 INFO run_performance_on_triton 	output_shared_memory_size = 100240
2023-03-04 04:52:52,268 INFO run_performance_on_triton 	result_path = /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/triton_performance_online.csv
2023-03-04 04:52:52,268 INFO run_performance_on_triton 	verbose = False
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 8 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 8
  Client:
    Request count: 502
    Throughput: 8032 infer/sec
    Avg latency: 15619 usec (standard deviation 3618 usec)
    p50 latency: 17559 usec
    p90 latency: 17796 usec
    p95 latency: 17898 usec
    p99 latency: 18109 usec
    Avg HTTP time: 15618 usec (send/recv 36 usec + response wait 15582 usec)
  Server:
    Inference count: 8112
    Execution count: 224
    Successful request count: 224
    Avg request latency: 15302 usec (overhead 32 usec + queue 6425 usec + compute input 200 usec + compute infer 6391 usec + compute output 2254 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 8, throughput: 8032 infer/sec, latency 15619 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 16 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 16
  Client:
    Request count: 857
    Throughput: 13712 infer/sec
    Avg latency: 18425 usec (standard deviation 4954 usec)
    p50 latency: 21859 usec
    p90 latency: 22966 usec
    p95 latency: 23189 usec
    p99 latency: 23735 usec
    Avg HTTP time: 18427 usec (send/recv 38 usec + response wait 18389 usec)
  Server:
    Inference count: 13888
    Execution count: 174
    Successful request count: 174
    Avg request latency: 18149 usec (overhead 53 usec + queue 6549 usec + compute input 449 usec + compute infer 6702 usec + compute output 4396 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 16, throughput: 13712 infer/sec, latency 18425 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 24 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 24
  Client:
    Request count: 1037
    Throughput: 16592 infer/sec
    Avg latency: 22739 usec (standard deviation 4944 usec)
    p50 latency: 25412 usec
    p90 latency: 26370 usec
    p95 latency: 26783 usec
    p99 latency: 27315 usec
    Avg HTTP time: 22740 usec (send/recv 40 usec + response wait 22700 usec)
  Server:
    Inference count: 16816
    Execution count: 152
    Successful request count: 152
    Avg request latency: 22391 usec (overhead 76 usec + queue 9215 usec + compute input 573 usec + compute infer 6908 usec + compute output 5619 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 24, throughput: 16592 infer/sec, latency 22739 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 32 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 32
  Client:
    Request count: 1152
    Throughput: 18432 infer/sec
    Avg latency: 27422 usec (standard deviation 5395 usec)
    p50 latency: 29990 usec
    p90 latency: 30837 usec
    p95 latency: 31121 usec
    p99 latency: 31937 usec
    Avg HTTP time: 27417 usec (send/recv 42 usec + response wait 27375 usec)
  Server:
    Inference count: 18496
    Execution count: 131
    Successful request count: 131
    Avg request latency: 27037 usec (overhead 117 usec + queue 10530 usec + compute input 692 usec + compute infer 6897 usec + compute output 8801 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 32, throughput: 18432 infer/sec, latency 27422 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 40 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 40
  Client:
    Request count: 1238
    Throughput: 19788.2 infer/sec
    Avg latency: 31593 usec (standard deviation 5547 usec)
    p50 latency: 34042 usec
    p90 latency: 34952 usec
    p95 latency: 35621 usec
    p99 latency: 37247 usec
    Avg HTTP time: 31560 usec (send/recv 43 usec + response wait 31517 usec)
  Server:
    Inference count: 19936
    Execution count: 114
    Successful request count: 114
    Avg request latency: 31157 usec (overhead 138 usec + queue 13256 usec + compute input 817 usec + compute infer 7171 usec + compute output 9775 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 40, throughput: 19788.2 infer/sec, latency 31593 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 48 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 48
  Client:
    Request count: 1308
    Throughput: 20928 infer/sec
    Avg latency: 36142 usec (standard deviation 5698 usec)
    p50 latency: 38446 usec
    p90 latency: 39285 usec
    p95 latency: 39695 usec
    p99 latency: 40942 usec
    Avg HTTP time: 36140 usec (send/recv 43 usec + response wait 36097 usec)
  Server:
    Inference count: 21040
    Execution count: 103
    Successful request count: 103
    Avg request latency: 35551 usec (overhead 162 usec + queue 15579 usec + compute input 809 usec + compute infer 7182 usec + compute output 11819 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 48, throughput: 20928 infer/sec, latency 36142 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 56 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 56
  Client:
    Request count: 1341
    Throughput: 21456 infer/sec
    Avg latency: 41096 usec (standard deviation 6151 usec)
    p50 latency: 43452 usec
    p90 latency: 44569 usec
    p95 latency: 44644 usec
    p99 latency: 45345 usec
    Avg HTTP time: 41094 usec (send/recv 46 usec + response wait 41048 usec)
  Server:
    Inference count: 21936
    Execution count: 91
    Successful request count: 91
    Avg request latency: 40386 usec (overhead 190 usec + queue 18421 usec + compute input 822 usec + compute infer 7551 usec + compute output 13402 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 56, throughput: 21456 infer/sec, latency 41096 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 64 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 64
  Client:
    Request count: 1377
    Throughput: 22032 infer/sec
    Avg latency: 45829 usec (standard deviation 7311 usec)
    p50 latency: 48254 usec
    p90 latency: 50770 usec
    p95 latency: 51562 usec
    p99 latency: 53098 usec
    Avg HTTP time: 45818 usec (send/recv 48 usec + response wait 45770 usec)
  Server:
    Inference count: 22560
    Execution count: 82
    Successful request count: 82
    Avg request latency: 45162 usec (overhead 196 usec + queue 19790 usec + compute input 1276 usec + compute infer 7543 usec + compute output 16357 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 64, throughput: 22032 infer/sec, latency 45829 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 72 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 72
  Client:
    Request count: 1401
    Throughput: 22416 infer/sec
    Avg latency: 50567 usec (standard deviation 6680 usec)
    p50 latency: 52716 usec
    p90 latency: 53863 usec
    p95 latency: 54258 usec
    p99 latency: 56714 usec
    Avg HTTP time: 50583 usec (send/recv 53 usec + response wait 50530 usec)
  Server:
    Inference count: 22704
    Execution count: 75
    Successful request count: 75
    Avg request latency: 49547 usec (overhead 216 usec + queue 22433 usec + compute input 1285 usec + compute infer 7227 usec + compute output 18386 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 72, throughput: 22416 infer/sec, latency 50567 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 80 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 80
  Client:
    Request count: 1418
    Throughput: 22688 infer/sec
    Avg latency: 55239 usec (standard deviation 7813 usec)
    p50 latency: 57646 usec
    p90 latency: 59200 usec
    p95 latency: 60067 usec
    p99 latency: 63927 usec
    Avg HTTP time: 55189 usec (send/recv 52 usec + response wait 55137 usec)
  Server:
    Inference count: 23312
    Execution count: 69
    Successful request count: 69
    Avg request latency: 54170 usec (overhead 243 usec + queue 24535 usec + compute input 1465 usec + compute infer 7383 usec + compute output 20544 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 80, throughput: 22688 infer/sec, latency 55239 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 88 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 88
  Client:
    Request count: 1438
    Throughput: 23008 infer/sec
    Avg latency: 60012 usec (standard deviation 7858 usec)
    p50 latency: 62580 usec
    p90 latency: 63693 usec
    p95 latency: 64849 usec
    p99 latency: 66514 usec
    Avg HTTP time: 59976 usec (send/recv 56 usec + response wait 59920 usec)
  Server:
    Inference count: 23680
    Execution count: 64
    Successful request count: 64
    Avg request latency: 58534 usec (overhead 276 usec + queue 26888 usec + compute input 1489 usec + compute infer 7335 usec + compute output 22546 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 88, throughput: 23008 infer/sec, latency 60012 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 96 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 96
  Client:
    Request count: 1413
    Throughput: 22608 infer/sec
    Avg latency: 64463 usec (standard deviation 8819 usec)
    p50 latency: 66877 usec
    p90 latency: 68919 usec
    p95 latency: 70597 usec
    p99 latency: 73322 usec
    Avg HTTP time: 64482 usec (send/recv 66 usec + response wait 64416 usec)
  Server:
    Inference count: 23504
    Execution count: 58
    Successful request count: 58
    Avg request latency: 63612 usec (overhead 249 usec + queue 29049 usec + compute input 1908 usec + compute infer 7478 usec + compute output 24928 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 96, throughput: 22608 infer/sec, latency 64463 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 104 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 104
  Client:
    Request count: 1419
    Throughput: 22704 infer/sec
    Avg latency: 69306 usec (standard deviation 9332 usec)
    p50 latency: 71685 usec
    p90 latency: 75659 usec
    p95 latency: 76755 usec
    p99 latency: 78092 usec
    Avg HTTP time: 69296 usec (send/recv 62 usec + response wait 69234 usec)
  Server:
    Inference count: 23504
    Execution count: 54
    Successful request count: 54
    Avg request latency: 68006 usec (overhead 319 usec + queue 31427 usec + compute input 1916 usec + compute infer 7615 usec + compute output 26729 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 104, throughput: 22704 infer/sec, latency 69306 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 112 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 112
  Client:
    Request count: 1453
    Throughput: 23248 infer/sec
    Avg latency: 74104 usec (standard deviation 8522 usec)
    p50 latency: 76217 usec
    p90 latency: 77979 usec
    p95 latency: 78763 usec
    p99 latency: 79126 usec
    Avg HTTP time: 74076 usec (send/recv 66 usec + response wait 74010 usec)
  Server:
    Inference count: 24064
    Execution count: 52
    Successful request count: 52
    Avg request latency: 72621 usec (overhead 283 usec + queue 34390 usec + compute input 2186 usec + compute infer 7621 usec + compute output 28141 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 112, throughput: 23248 infer/sec, latency 74104 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 120 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 120
  Client:
    Request count: 1456
    Throughput: 23296 infer/sec
    Avg latency: 79076 usec (standard deviation 9084 usec)
    p50 latency: 81437 usec
    p90 latency: 83323 usec
    p95 latency: 84116 usec
    p99 latency: 85255 usec
    Avg HTTP time: 78996 usec (send/recv 59 usec + response wait 78937 usec)
  Server:
    Inference count: 24560
    Execution count: 50
    Successful request count: 50
    Avg request latency: 77264 usec (overhead 360 usec + queue 35115 usec + compute input 2861 usec + compute infer 7067 usec + compute output 31861 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 120, throughput: 23296 infer/sec, latency 79076 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 128 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 128
  Client:
    Request count: 1449
    Throughput: 23184 infer/sec
    Avg latency: 83765 usec (standard deviation 9292 usec)
    p50 latency: 85972 usec
    p90 latency: 88433 usec
    p95 latency: 90881 usec
    p99 latency: 91219 usec
    Avg HTTP time: 83926 usec (send/recv 64 usec + response wait 83862 usec)
  Server:
    Inference count: 25104
    Execution count: 48
    Successful request count: 48
    Avg request latency: 81973 usec (overhead 353 usec + queue 36963 usec + compute input 2789 usec + compute infer 7407 usec + compute output 34461 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 128, throughput: 23184 infer/sec, latency 83765 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 136 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 136
  Client:
    Request count: 1455
    Throughput: 23280 infer/sec
    Avg latency: 89463 usec (standard deviation 9809 usec)
    p50 latency: 91123 usec
    p90 latency: 93832 usec
    p95 latency: 95022 usec
    p99 latency: 110123 usec
    Avg HTTP time: 89412 usec (send/recv 88 usec + response wait 89324 usec)
  Server:
    Inference count: 24576
    Execution count: 44
    Successful request count: 44
    Avg request latency: 87643 usec (overhead 386 usec + queue 41738 usec + compute input 3188 usec + compute infer 7067 usec + compute output 35264 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 136, throughput: 23280 infer/sec, latency 89463 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 144 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 144
  Client:
    Request count: 1492
    Throughput: 23872 infer/sec
    Avg latency: 93705 usec (standard deviation 10610 usec)
    p50 latency: 96549 usec
    p90 latency: 100539 usec
    p95 latency: 101344 usec
    p99 latency: 106161 usec
    Avg HTTP time: 93875 usec (send/recv 86 usec + response wait 93789 usec)
  Server:
    Inference count: 25152
    Execution count: 42
    Successful request count: 42
    Avg request latency: 92065 usec (overhead 390 usec + queue 43044 usec + compute input 4803 usec + compute infer 7042 usec + compute output 36786 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 144, throughput: 23872 infer/sec, latency 93705 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 152 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 152
  Client:
    Request count: 1478
    Throughput: 23648 infer/sec
    Avg latency: 98275 usec (standard deviation 10925 usec)
    p50 latency: 100474 usec
    p90 latency: 103058 usec
    p95 latency: 107687 usec
    p99 latency: 120481 usec
    Avg HTTP time: 98318 usec (send/recv 110 usec + response wait 98208 usec)
  Server:
    Inference count: 25040
    Execution count: 40
    Successful request count: 40
    Avg request latency: 96667 usec (overhead 372 usec + queue 45518 usec + compute input 3537 usec + compute infer 7498 usec + compute output 39742 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 152, throughput: 23648 infer/sec, latency 98275 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 160 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 160
  Client:
    Request count: 1472
    Throughput: 23552 infer/sec
    Avg latency: 102631 usec (standard deviation 9438 usec)
    p50 latency: 104550 usec
    p90 latency: 106974 usec
    p95 latency: 107435 usec
    p99 latency: 111520 usec
    Avg HTTP time: 102763 usec (send/recv 173 usec + response wait 102590 usec)
  Server:
    Inference count: 24848
    Execution count: 38
    Successful request count: 38
    Avg request latency: 101020 usec (overhead 381 usec + queue 48201 usec + compute input 3968 usec + compute infer 7457 usec + compute output 41013 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 160, throughput: 23552 infer/sec, latency 102631 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 168 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 168
  Client:
    Request count: 1482
    Throughput: 23712 infer/sec
    Avg latency: 110328 usec (standard deviation 13349 usec)
    p50 latency: 113252 usec
    p90 latency: 123890 usec
    p95 latency: 128509 usec
    p99 latency: 135120 usec
    Avg HTTP time: 110178 usec (send/recv 143 usec + response wait 110035 usec)
  Server:
    Inference count: 25008
    Execution count: 36
    Successful request count: 36
    Avg request latency: 107965 usec (overhead 454 usec + queue 51796 usec + compute input 6322 usec + compute infer 6514 usec + compute output 42879 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 168, throughput: 23712 infer/sec, latency 110328 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 176 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 176
  Client:
    Request count: 1423
    Throughput: 22745.3 infer/sec
    Avg latency: 115845 usec (standard deviation 12053 usec)
    p50 latency: 118176 usec
    p90 latency: 126756 usec
    p95 latency: 127995 usec
    p99 latency: 142276 usec
    Avg HTTP time: 115816 usec (send/recv 290 usec + response wait 115526 usec)
  Server:
    Inference count: 23920
    Execution count: 33
    Successful request count: 33
    Avg request latency: 113971 usec (overhead 437 usec + queue 54366 usec + compute input 12328 usec + compute infer 6009 usec + compute output 40831 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 176, throughput: 22745.3 infer/sec, latency 115845 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 184 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 184
  Client:
    Request count: 1432
    Throughput: 22912 infer/sec
    Avg latency: 119111 usec (standard deviation 14073 usec)
    p50 latency: 121175 usec
    p90 latency: 127971 usec
    p95 latency: 137033 usec
    p99 latency: 144618 usec
    Avg HTTP time: 118736 usec (send/recv 219 usec + response wait 118517 usec)
  Server:
    Inference count: 24512
    Execution count: 33
    Successful request count: 33
    Avg request latency: 116419 usec (overhead 436 usec + queue 54949 usec + compute input 7723 usec + compute infer 6676 usec + compute output 46635 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 184, throughput: 22912 infer/sec, latency 119111 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 192 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 192
  Client:
    Request count: 1453
    Throughput: 23248 infer/sec
    Avg latency: 126101 usec (standard deviation 11874 usec)
    p50 latency: 128567 usec
    p90 latency: 134699 usec
    p95 latency: 143467 usec
    p99 latency: 147397 usec
    Avg HTTP time: 126036 usec (send/recv 438 usec + response wait 125598 usec)
  Server:
    Inference count: 24976
    Execution count: 32
    Successful request count: 32
    Avg request latency: 124360 usec (overhead 475 usec + queue 60123 usec + compute input 10386 usec + compute infer 6420 usec + compute output 46956 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 192, throughput: 23248 infer/sec, latency 126101 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 200 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 200
  Client:
    Request count: 1431
    Throughput: 22896 infer/sec
    Avg latency: 131763 usec (standard deviation 9601 usec)
    p50 latency: 134348 usec
    p90 latency: 136712 usec
    p95 latency: 137132 usec
    p99 latency: 141570 usec
    Avg HTTP time: 131742 usec (send/recv 352 usec + response wait 131390 usec)
  Server:
    Inference count: 24576
    Execution count: 30
    Successful request count: 30
    Avg request latency: 129366 usec (overhead 497 usec + queue 60648 usec + compute input 20186 usec + compute infer 5646 usec + compute output 42389 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 200, throughput: 22896 infer/sec, latency 131763 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 208 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 208
  Client:
    Request count: 1432
    Throughput: 22912 infer/sec
    Avg latency: 135682 usec (standard deviation 12201 usec)
    p50 latency: 138999 usec
    p90 latency: 142738 usec
    p95 latency: 152132 usec
    p99 latency: 159904 usec
    Avg HTTP time: 135711 usec (send/recv 427 usec + response wait 135284 usec)
  Server:
    Inference count: 23856
    Execution count: 28
    Successful request count: 28
    Avg request latency: 133136 usec (overhead 524 usec + queue 62020 usec + compute input 18590 usec + compute infer 5910 usec + compute output 46092 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 208, throughput: 22912 infer/sec, latency 135682 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 216 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 216
  Client:
    Request count: 1380
    Throughput: 22080 infer/sec
    Avg latency: 142088 usec (standard deviation 12064 usec)
    p50 latency: 143294 usec
    p90 latency: 145109 usec
    p95 latency: 166873 usec
    p99 latency: 175533 usec
    Avg HTTP time: 142767 usec (send/recv 588 usec + response wait 142179 usec)
  Server:
    Inference count: 23888
    Execution count: 27
    Successful request count: 27
    Avg request latency: 140573 usec (overhead 513 usec + queue 67963 usec + compute input 18903 usec + compute infer 6339 usec + compute output 46855 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 216, throughput: 22080 infer/sec, latency 142088 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 224 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 224
  Client:
    Request count: 1375
    Throughput: 22000 infer/sec
    Avg latency: 146362 usec (standard deviation 10113 usec)
    p50 latency: 148600 usec
    p90 latency: 151524 usec
    p95 latency: 154475 usec
    p99 latency: 155196 usec
    Avg HTTP time: 146428 usec (send/recv 590 usec + response wait 145838 usec)
  Server:
    Inference count: 23792
    Execution count: 26
    Successful request count: 26
    Avg request latency: 143849 usec (overhead 524 usec + queue 68979 usec + compute input 22483 usec + compute infer 5745 usec + compute output 46118 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 224, throughput: 22000 infer/sec, latency 146362 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 232 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 232
  Client:
    Request count: 1403
    Throughput: 22448 infer/sec
    Avg latency: 150296 usec (standard deviation 6360 usec)
    p50 latency: 151413 usec
    p90 latency: 152314 usec
    p95 latency: 152563 usec
    p99 latency: 153388 usec
    Avg HTTP time: 150247 usec (send/recv 850 usec + response wait 149397 usec)
  Server:
    Inference count: 24448
    Execution count: 26
    Successful request count: 26
    Avg request latency: 147966 usec (overhead 724 usec + queue 72243 usec + compute input 25878 usec + compute infer 5437 usec + compute output 43684 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 232, throughput: 22448 infer/sec, latency 150296 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 240 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 240
  Client:
    Request count: 1400
    Throughput: 22400 infer/sec
    Avg latency: 156631 usec (standard deviation 11640 usec)
    p50 latency: 157779 usec
    p90 latency: 160505 usec
    p95 latency: 176462 usec
    p99 latency: 182450 usec
    Avg HTTP time: 156630 usec (send/recv 747 usec + response wait 155883 usec)
  Server:
    Inference count: 24320
    Execution count: 25
    Successful request count: 25
    Avg request latency: 154255 usec (overhead 550 usec + queue 75310 usec + compute input 22869 usec + compute infer 5948 usec + compute output 49578 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 240, throughput: 22400 infer/sec, latency 156631 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 248 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 248
  Client:
    Request count: 1436
    Throughput: 22976 infer/sec
    Avg latency: 160279 usec (standard deviation 14077 usec)
    p50 latency: 161674 usec
    p90 latency: 169076 usec
    p95 latency: 183116 usec
    p99 latency: 193942 usec
    Avg HTTP time: 160227 usec (send/recv 557 usec + response wait 159670 usec)
  Server:
    Inference count: 24992
    Execution count: 26
    Successful request count: 26
    Avg request latency: 157134 usec (overhead 562 usec + queue 79144 usec + compute input 22281 usec + compute infer 6037 usec + compute output 49110 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 248, throughput: 22976 infer/sec, latency 160279 usec
 Successfully read data for 1 stream/streams with 1 step/steps.
*** Measurement Settings ***
  Batch size: 16
  Using "count_windows" mode for stabilization
  Minimum number of samples in each window: 500
  Latency limit: 0 msec
  Concurrency limit: 256 concurrent requests
  Using synchronous calls for inference
  Stabilizing using average latency

Request concurrency: 256
  Client:
    Request count: 1416
    Throughput: 22656 infer/sec
    Avg latency: 163211 usec (standard deviation 15027 usec)
    p50 latency: 165646 usec
    p90 latency: 181947 usec
    p95 latency: 188546 usec
    p99 latency: 189524 usec
    Avg HTTP time: 163426 usec (send/recv 696 usec + response wait 162730 usec)
  Server:
    Inference count: 24736
    Execution count: 30
    Successful request count: 30
    Avg request latency: 160060 usec (overhead 583 usec + queue 92293 usec + compute input 16097 usec + compute infer 6348 usec + compute output 44739 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 256, throughput: 22656 infer/sec, latency 163211 usec
  Batch    Concurrency    Inferences/Second    Client Send    Network+Server Send/Recv    Server Queue    Server Compute Input    Server Compute Infer    Server Compute Output    Client Recv    p50 latency    p90 latency    p95 latency    p99 latency    avg latency
-------  -------------  -------------------  -------------  --------------------------  --------------  ----------------------  ----------------------  -----------------------  -------------  -------------  -------------  -------------  -------------  -------------
     16              8               8032               36                         311            6425                     200                    6391                     2254              0          17559          17796          17898          18109          15617
     16             16              13712               38                         288            6549                     449                    6702                     4396              0          21859          22966          23189          23735          18422
     16             24              16592               40                         379            9215                     573                    6908                     5619              0          25412          26370          26783          27315          22734
     16             32              18432               42                         457           10530                     692                    6897                     8801              0          29990          30837          31121          31937          27419
     16             40              19788.2             43                         528           13256                     817                    7171                     9775              0          34042          34952          35621          37247          31590
     16             48              20928               43                         708           15579                     809                    7182                    11819              0          38446          39285          39695          40942          36140
     16             56              21456               46                         851           18421                     822                    7551                    13402              0          43452          44569          44644          45345          41093
     16             64              22032               48                         812           19790                    1276                    7543                    16357              0          48254          50770          51562          53098          45826
     16             72              22416               53                        1182           22433                    1285                    7227                    18386              0          52716          53863          54258          56714          50566
     16             80              22688               52                        1258           24535                    1465                    7383                    20544              0          57646          59200          60067          63927          55237
     16             88              23008               56                        1693           26888                    1489                    7335                    22546              0          62580          63693          64849          66514          60007
     16             96              22608               66                        1032           29049                    1908                    7478                    24928              0          66877          68919          70597          73322          64461
     16            104              22704               62                        1555           31427                    1916                    7615                    26729              0          71685          75659          76755          78092          69304
     16            112              23248               66                        1696           34390                    2186                    7621                    28141              0          76217          77979          78763          79126          74100
     16            120              23296               59                        2110           35115                    2861                    7067                    31861              0          81437          83323          84116          85255          79073
     16            128              23184               64                        2078           36963                    2789                    7407                    34461              0          85972          88433          90881          91219          83762
     16            136              23280               88                        2116           41738                    3188                    7067                    35264              0          91123          93832          95022         110123          89461
     16            144              23872               86                        1942           43044                    4803                    7042                    36786              0          96549         100539         101344         106161          93703
     16            152              23648              110                        1868           45518                    3537                    7498                    39742              0         100474         103058         107687         120481          98273
     16            160              23552              173                        1817           48201                    3968                    7457                    41013              0         104550         106974         107435         111520         102629
     16            168              23712              143                        2671           51796                    6322                    6514                    42879              0         113252         123890         128509         135120         110325
     16            176              22745.3            290                        2017           54366                   12328                    6009                    40831              0         118176         126756         127995         142276         115841
     16            184              22912              219                        2907           54949                    7723                    6676                    46635              0         121175         127971         137033         144618         119109
     16            192              23248              438                        1775           60123                   10386                    6420                    46956              0         128567         134699         143467         147397         126098
     16            200              22896              352                        2538           60648                   20186                    5646                    42389              0         134348         136712         137132         141570         131759
     16            208              22912              427                        2641           62020                   18590                    5910                    46092              0         138999         142738         152132         159904         135680
     16            216              22080              588                        1438           67963                   18903                    6339                    46855              0         143294         145109         166873         175533         142086
     16            224              22000              590                        2444           68979                   22483                    5745                    46118              0         148600         151524         154475         155196         146359
     16            232              22448              850                        2201           72243                   25878                    5437                    43684              0         151413         152314         152563         153388         150293
     16            240              22400              747                        2176           75310                   22869                    5948                    49578              0         157779         160505         176462         182450         156628
     16            248              22976              557                        3147           79144                   22281                    6037                    49110              0         161674         169076         183116         193942         160276
     16            256              22656              696                        3035           92293                   16097                    6348                    44739              0         165646         181947         188546         189524         163208
