- batching_config:
    batching: dynamic
    max_batch_size: 1024
  dynamic_batching_config:
    max_queue_delay_us: 1
    preferred_batch_sizes:
    - 512
    - 1024
  instances_config:
    engine_count_per_device:
      gpu: 2
  model_config:
    model_format: torchscript
    model_name: TFT
    model_path: /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/shared/converted_model
    model_version: '1'
  model_dir_in_model_store: /home/cc/DLEx/PyTorch/Forecasting/TFT/runner_workspace/executor/triton_models/TFT
  model_version: '1'
  optimization_config:
    backend_accelerator: none
    tensorrt_capture_cuda_graph: true
    tensorrt_precision: fp16
  status:
    log_path: null
    message: Model configured and loaded correctly
    state: succeeded
  tensorrt_common_config:
    tensorrt_max_workspace_size: 10000000000
